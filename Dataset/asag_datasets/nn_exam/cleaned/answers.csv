	id	answer	score
0	1	ANN is huge parallel distributed processor ,    consist of simple processing units and     which has propensity of storing experintial knowlegde     and making it available for use.	1
1	1	Artificial neural network is a massively parrallal distributed processor made up of simple processing units which has a natural propensity to acquire knowledge from the environment and make it available for future use.    It resembels the human brain in following ways.    1. Both of them acquire knowledge from the environment.  2. The neurons are connected by synapses cahrecterized by their weights which can be adjusted.	2
2	1	YOUR ANSWER HERE    An artificial neural network is a massively distributed parallel processor made of simple processing units. It has natural propensity to store experential knowledge and it makes the knowledge available for further use.    An artificial neural network uses inter neuron connections called synaptic weights to store the knowledge acquired knowledge which is very similar to how human brain works.	2
3	1	ANN is a massively distributed processor, consisting of simple processing units called neurons. These neurons in terms of ANN are similar to neurons in human brain. Both neurons are characterized by synapses(connection links). They represent connections used for data flow between neurons. In both ANN and Human brain, the knowledge is represented by its very structure and activation state of neurons.  	2
4	1	Artificial neural network is massive parallel processor made up of simple processing units called neurons.     They are capable of storing experential knowledge and make it available for later use.     Similarity to human brain:   1. they learn from the envirnoment   2. they store knowledge as synaptic weight in the interneuron connection 	2
5	1	An artificial neural network is a highly distributed processor which consists of several simple processing units. It resembles the human brain, because the processing units are neurons, which are connected with weights. The human brain also consists of neurons.	1
6	1	Artificial neural network is a **massively parallel distributed processor** with synaptic links that can able to **store experimental knowledge** and make it available for use.    It resembles human brain in two ways,    * Knowledge is acquired by the neural network from its environment through learning process.  * Interneuron connection called synaptic links stores the acquired knowledge.	2
7	1	Artificial neural network are the network of the units that learn data from the environment and store them using synaptic weights.    The structure of the artificial neural network is similar to human brain. It has neurons, ie., the store units and the axoms called synapses which link the stored data.	1
8	1	Definition:  1. Artificial neural networks are massively distributed parallel processor.  2. It is made up of small units,   3. Which has the propensity for storing the experential knowledge.  4. And making it available for use.     It resembles the brain in 2 aspects.   1. Similar to the brain, artificial neural network does the process of learning from the environment.   2. It as a pair of inter neuron links known as the synaptic weights, which is used for storing the information. 	2
9	1	Artificial neural network is massive parallely distributed processor. It comprises of small processing units called neurons. It learns from experiencial knowledge which is then stored and can be used for making predictions. It resembles human brain in 2 ways:  * It learns from experiencial knowledge  * Knowledge is stored in synaptic interneuron connections.	2
10	1	It is a massively parallel distributed processor consisting of simple processing units, which can store experiential knowledge and make it available for use. it resembles the human brain in 2 ways: 1. knowledge is acquired from environment through a learning process; 2. interneuron connections are used to store the experiential knowledge.	2
11	1	It is a massive parallel distributed processor made up of smaller processing units, that aquire knowledge through the environmnet through a learning process and makes it available for use. It resembles the brain in two ways:    - Knowledge is aquired through a stimulating process in the environment  - The knowledge is embedded in the synaptic links (weights) of the neurons. 	2
12	1	ANN is a learning machine which is composed of neurons as units of computation. The ANN learns via interacting with its environment. The ANN has built-in capacity to dynamically adapt upon input stimulus.    The ANN is motivated from biological brain and resembles human brain in terms of its localized representation for the inputs. In terms of motor cortex, the sensory stimulus to diffrent body-parts activates local part of the brain, similar to ANN local representation of similar type of input.	2
13	1	A neural network is a massively parallel distributed prcoessor made up for simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects  * Knowledge is acquired by the network from its environment through the learning process  * Interneuron connection strengths known as synaptic weights, are used to stor the acquired knowledge.	2
14	1	Artificial neural network is a massively parallel distributed processor which consists of one or more processing unit called neuron. It resembles the human brain for that it acquires knowledge from the environment through learning process, and that the acquired knowledge is stored in the synapses.	2
15	1	A ANN is a massively distributed processor. It has the propensity to store experiental knowledge and make it available for use. The knowledge is gained throug a process of learning. The knowledge is stored in the weights between the neurons. This structure resembles the structure of the brain. Neurons are a the basic information unit in the ANN and act similiar to real neurons.	2
16	1	An artificial neural network is defined as a learning machine which is divided by layers and each layer is composed by neurons. The neurons from different layers can be connected between each other, and give an output or multiple outputs by a given input. This structure is very similar with the neurological structure of our brain, where neurons are interconnected by synapses. Also important to mention, if a feature is really important for a given task, this wil have more connections and neurons participating (like in the human brain, the important humasn functions have more synapses).	2
17	1	A massively distributed processor, consisting of single processing units that have a natural prospensity of storing experimental knowledge and making it available for use.	1
18	1	Artificial neural network is highly parralel processing. It has a mathematical model similar to human brain, which it was inspired from, as human brain does computation in an extremely parallel manner. Similarities also lay in terminology, ANN is using neurons that are smallest computing unit of a network, similarly to human brain.	1
19	1	An ANN is a layered graphical model containing neurons and weighted connections, resembling the excitatory properties of the human brain. Weights of the ANN are changed after presenting it training examples from an environment, where weights are changed based on the training procedure used. Artificial neurons also are biased, just like real ones, adding a constant level of activation before being activated by a (nonlinear) activation function. Depending on the training procedure, both weights, topology or even activation functions may be learned.	2
20	1	Artificial Neural Networks are large parallel processing units that have the natural ability to learn experiential knowledge. They are composed of interconnected neurons as basic units; which in turn cosists of weights, squashing functions and adder functions.    ANN resembles brain in the manner that like in human brain, it is composed of a network of neurons which help in learning by adjusting the synaptic weights of the connections between neurons. This enables it to learn experiential knowledge.	2
21	1	Artificial neural network consists of:        . Largely parallel distributed processor            . simple processing units            . that has ability to store the experential knowledge and making it available to use        It resembles to human brain in two ways:        . Knowledge is acquired from the environment by the network as learning process      . Synaptic strengths called weights are used to store the knowledge	2
22	1	An artificial neural network is a massive distributed processor. It consists of several information processing units which are able to acquire and store knowledge.	1
23	1	An artificial neural network is a massively parallel distributed processor with simple processing units that has the natural propensity to store experiential knowledge and make use of them.    An artificial neural network is similar to the human brain in two ways:    1. The ANN works by the process of learning from its environment.  2. Interneuron connections called synaptic weights are used to store the knowledge gained. 	2
24	1	An articial neural network consists of neurons. Each neuron can have several weighted inputs, an activation function and output. Usually several neurons are connected together. Often in layers. The network then calculates the output given an input to the network. The human brain works in a similar way. It also consits of neurons that are connected in several ways.	1
25	1	An ANN is a  - massivly parallel distibuted Processor  - made up of simple processing units  - which have the capability of storing experiantal knowlenge  - and is made up for use.    An ANN resembles the brain because:    1) it gets its knowlenge through a learning process from its environment.    2) it stores its knowlenge in its interneuron connections (synaptic weights)	2
26	1	An ANN is a massivly parrallel distributed learing machine made up of small computational units. Computational units are connected via synapses defined by a weight. It resembles the human brain in two aspectes:   	1
27	1	An artificial neural network is a highly parallel working machine which consists of simple processing units (neurons) wich are connected to each other in layers. they are function approximators   the brain is resembled in the architecure, the processing units and thge weights and how the learning process takes place and the properties of the brain: fault tolerance, parallel computing, ... 	1
28	1	An artificial neural network is a highly parallel computation model with learning and memory capacities. Similar to the brain it learns from the environment by strengthening the synapses between neurons. Once a task is learned it can be quickly used by reactivating those learned synapses.	1
29	1	Artificial neural network is massively parallel distributed processor made up of simple computing units called neurons which aquires knowledge from environment through learning. It resembles brainlike structure in two ways,     1. It aquires knowledge through learning and experience   2. It stores knowledge in interneuron connections called synapses. 	2
30	1	Artificial neural network is massively distributed paralled processor containing simple processing units and has natural propensity to store experiential knowledge and use it.It resembles the human brain in two aspects, it gains knowledge from the environment and adapts the synaptic weight to store the knowledge.	2
31	1	ANN is a learning machine which can perform complex parallel computation. It has the ability to learn through the interactions withthe environment and store the learned knowedge.     It resembles the human brain in performing complex learning tasks, acquiring information, apadpting to the environment, and exploiting the acquired information.	2
32	1	An artificial neural network is a graph of small and identical processing units that these small units called neurons and they are connected to each other in different architectures and the whole network adapt and itself to the environment inputs by trying to decrease the error or the cost function and increase its preciseness by manipulating the free variables of the network which are the synaptic weights.    It is similar to human brain because similar to the human brain we have many small processing units that are connected together and they react to the environment and learn from the environment.	2
33	1	An Artificial Neural Network is a massively parallel distributed processor which interacts with its surrounding environment, with a propensity to store knowledge and make it available to use.    It resembles the brain in two aspects:    1. It has the ability to learn from its environment    2. The knowledge is stored in synaptic weights	2
34	1	An artificial neural network is a massively distributed parallel processor made up of simple processing units that have the natural propensity for storing experiental knowledge and making it available for future use.    It resembles the brain in the following ways:    1. Artificial neural networks have the ability to acquire knowledge from the environment in which they are are embedded.  2. Inter-neuron connection strenghts called synaptic links activate each neuron during the learning process.	2
35	1	An artificial neural network consists of neurons, which are small computation devices,and synapses, the connections between the neurons. This resembles the brain because it also has neurons and synapses. Also a artificial neural network has weights, which are used to store learned features from the environment. Like the brain a neural network learns from the environment. An artificial neural network also has an activation function, which creates the output.	2
36	2	1) Neuron is consist of sysnaptic  links which measured in terms of weights. neuron is given with inputs.\    2)it has adder funtion or combiner which adds all the inputs mulitplied by the weights and bias is extra input to the neuron as well.    3) it has a activation link which limit the amplitude of the output of the neuron. 	1
37	2	Neuron is the basic information processing unit which is the main component of a neural network. A neuron is charecterized by its input ($x_i$), synaptic weight ($w_i$) and activation function $\phi(v)$. Mathematically it can be modelled as $\phi(w_ix_i)$. Activation function bounds the input to a certain level.	2
38	2	YOUR ANSWER HERE    A neuron has three components    * Synaptic weight: w  * Adder function: it multiplies input x with the weight  * Activation function: It squashes the output of the adder function. Sigmoid, hyperbolic tangent, Rectified linear unit etc.	2
39	2	A neuron consist of set of inputs that takes data from environment. Each neuron contains synapses(connection links) that are characterized by weights. All inputs are connected to the summing (adder) function, that computes weighted sum of all input values. This weighted sum is called local field of neuron. The value of this local field (V) is limited(squased) by an activation funtion $\theta(V)$. The result from this squasing funtion is output of a neuron ($y = \theta(V)$).  Additionally, a bias term $(b)$ is added to the input, and its value is always 0, but its associated weight is being changed over training period. Finally, output of neuron is $y = \theta(V)$, where $V = \sum W_j * X_j + b$	2
40	2	The neurons are the basic processing units in neural network    output of the neuron = $  \phi (\sum w_{i} x_{i})$       they consist of three parts    Synaptic weight: the connections between the neurons. characterised by weights    Adder function: calculates the weighted sum of the inputs of the neuron    Activation function: limits the amplitude of the output of the neuron. ($\phi$)	2
41	2	The model of a neuron consists of synaptic weights which are applied to the input signals. The weighted inputs are then summed which gives the local field. This local field is put into an activation function whose output will be the output of the neuron.	2
42	2	A neuron is a basic information processing unit that have a adder function to compute **weighted sum of inputs plus bias** and apply activation function on the result.    $$ \phi(v) = \sum\limits_{i=1}^n \omega(i)x(i) + bias $$	2
43	2	Each neuron has a set of inputs and their respective weights.    The local field is,    $v = \sum(w_{ij} * x_i)$    The local field is passed through a activation function.    So the output of the neuron is,    $y = \phi(v)$    $y = \phi(\sum(w_{ij} * x_i))$	2
44	2	YOUR ANSWER HERE  A Mathematical model of a neurons consits of a     1. A set of synaptic links which are classified based on weights(w1, w2, w3...w_n)  2. It consits of a adder function, which performs the weighted sum of the inputs and the bias.  $\Sigma_{i=1....n} w_n.x + b$    3. It consists of an activation function, used to minimize the amplitude of the neuron output.  $\Phi(\Sigma_{i=1....n} w_n.x + b)$	2
45	2	A mathematical model of neuron comprises of 2 main units:  * Adder functions: it sums up all the product of all synaptic connections and inputs of neuron  * Synaptic weights: these are interneuron connections in which the knowledge is stored  * Activation function: it is used for introducing non-linearity 	2
46	2	$v_k = \sum_{j=1}^{m} w_{kj} x_j + b_k$, $y_k = \phi(v_k)$, $w_{kj}$ is the synaptic weight  connecting neuron k and input data j, $x_j$ is input data, $b_k$ is bias, $v_k$ is induced local field, $y_k$ is output of neuron.	2
47	2	The neuron is the basic processing unit of a neural network and is made of three main component:  - Weights: $w_1, w_2, ...,w_n$  - Adder function: it is the linear combination of the input and weights plus bias. (induced local field) $v = \sum w_i x_i + b$  - Squashing function: it is the activation function applied to the local field used to limit the output of the neuron. $\phi(v)$ 	2
48	2	A neuron is a computational unit composed of  + synapses which are stored in the form of weights $w$. These are the variables that can are dynamical.  + summing function that computes the weighted sum of inputs: $v = \sum_i (w_ix_i)$  + activation function $\phi$: gives nonlinear nature to network, determines and normalizes the output produced by neuron. e.g. sigmoid function  + bias: another synaptic tunable variable with input 1. Therefore the net output of neuron: $ y = \sum_i (w_ix_i) +b$.	2
49	2	The following equations describe a nonlinear model of a neuron, labeled k.    1)u_k = sum from j=1 to m w_{kj} x_{j}    2)y_k = phi(u_{k} + b_{k})    where x_{j} are the input signals; w_{kj} are the weights of the neurons; u_{k} is the linear combiner output due to the input signals; b_{k} is the bias; phi() is the activation function; and y_k is the output signal of te neuron.	2
50	2	A neuron is a processing unit that contains three main components: a set of synaptic weights that connect the neuron with other neurons; an adder that computes the induced local field, or the weighted sum of the signals flowing through the neuron; an activation function that constrains the magnitude of the output signal from the neuron.	2
51	2	The neuron consists of synapses/connecting link each characterised by a weight. A linear combiner sums up the weighted sum of inputs to a local field. The local field is then passed through an activation function. The result of the activation function is the output.	2
52	2	A neuron is defined by the following elements:  - A number of input values x  - A number of weights w  - A bias b  - An activation function $/phi$.    The inputs x are multiplied with the weights, and the result is summed with the bias (also, the bias can be used just as a weight value b and a single connetion with an stable input equal to 1, for mathematical simplicity). The resulting value, known as local field (v), will be the input to the activation function.    The mathematical model can be summarized in the formula:    $v = \sum^{n}_{i = 0} x(i)*w(i) + b$    $y = \phi(v)$	2
53	2	$y = \sum_{i=0} \Phi(w*x_i)$     A neuron consists of inputs $x$, synpatic weights $w$, an extra input $w_0$ which is fixed to 1 for the bias, an Adder function, that creates the local field $v$ and a squashing function $\Phi$.	2
54	2	Neuron is a simplest computation unit of a neural network that consists of input variables, weights, bias, summation term (combiner), activation function and output variables.	1
55	2	Input vector $x$    Weight matrix $w$    Net input $net=\sum x^Tw$    Net output $o=\phi(net)$	1
56	2	A neuron consists of three basic components:     - *Synaptic Weights*: The synaptic weights are connections between neurons and are adjusted through training.     - *Squashing/Activation Functions*: The squashing functions may be non linear or linear functions that that are applied to the signals from the neurons     - *Adder Functions*: The adder functions help in combining outputs from several neurons.         	2
57	2	Mathematical model of a neuron is given as :       y = $\phi(V)$ , where activation function is applied to local field(V)          V = $\summation (w_{i}x_{i} + b)$  . Local field is weighted(w) sum of inputs(x) plus bias(b)          	2
58	2	YOUR ANSWER HERE A neuron is an information processing unit. It consits of: inputs associated with weights, sum of inputs and an acitvation function   	1
59	2	A neuron is the simplest processing unit of a neural network which has:    1. synaptic weights to store the knowledge gained.  2. Adder function (linear combiner) which adds the weighted values of the input signals to produce the local field.  3. An activation function which squashes the local field to a range of values.    $ \phi(\sum_{i=0}^{N} w_i \cdot x_i) $	2
60	2	$N$ number inputs, $x_i$ input i, $v_j$ local field, $\varphi(v_j)$ activiation function, $y_j$ output, $w_{ji}$ weight from node i to j    $y_j = \varphi(v_j)$    $v_j = \sum_{i=0}^{N}w_{ji}x_i$	2
61	2	A neuron is a simple processing unit of an ANN, that is made up of    - the synaptik links which are defines by a weights $(w_1,...,w_n)$  - a adder function that combines the weighted input $(w_i*x_i)$ plus some bias $(b)$ to the local field  $(\sum{w_i*w_i}) +b=v$  - a activation function phi that squaches the local field to the output $(phi(v)=y)$ 	2
62	2	A neuron consists of input nodes x_1 to x_n and weights w_1 to w_n, a linear combiner v= SUM( $ x_i * w_i $) + b, where b is some bias. The result v is called local field and is used as input for an acivation function $ phi(v) $	2
63	2	A neuron consist of input connection links with a synaptic weight, a bias, an adder which adds the input singnals and the bias and produces the local field. The local field is processed by the activation function and produces the output of the neuron. 	2
64	2	A neuron consists of one or multiple inputs which are gathered by a summation function. The hereby induced local field of the neuron is processed by a squashing function and generates the output of the neuron.	1
65	2	Neuron is consists of three units.     1. Synaptic links characterizex by weights which linearly ways the input.  2. Adder which adds weighted inputs to generate local field  3. Activation function which is nonlinear function sqashing the output of the neuron	2
66	2	The mathematical model of neuron has three parts:  - a set of synapses or connencting links characterized by weight ,w .  - an adder function that calculates the weighted sum of inputs plus some bias  - an activation function (squashing function) to minimize the amplitude 	2
67	2	A mathematical model of neuron consists of 3 important parts.  A neuron is the smallest computaional node with:  1) Input vectors : set of vectors of a certan dimension to train the model  2) Weights (and biases): each of the input vectors are weighted using weight vectors in accordance withthe output that is required. Bias is added when necessary.  3) Activation function : The linear combination of weights and inputs are passed through the activation function which produces an output.	2
68	2	A neuron consists of a set of inputs and a bias which these inputs and predefined bias will be multiplied by a weight and then we have sum the results of all the inputs and bias multiplied by the weights which called induced field and after that we send this to an activation function which can be a linear or non-linear function and the output of this function is the final output of our neuron. 	2
69	2	Let $x_1$, $x_2$, ... , $x_N$ be the inputs to the neuron, $w_i$ be the corresponding weights of connections, $b$ be the bias and $\varphi(.)$ be the activation function.   Then, the induced field $v$ is given by -    $$v = \sum_{i = 1}^{N} w_i .x_i + b$$    The output $y$ is given by -    $$y = \varphi(v)$$	2
70	2	The neuron is the fundamental processing unit of an aritificial neural network that is characterised by the followig features:    1. A neuron has a set of non-linear synaptic links, an externally applied bias, and possibly one or more linear activation links. The bias is represented by a synaptic link from an input fixed at +1.  2. The synaptic links of the neuron weight the respective inputs.  3. An adder function (linear combiner) computes the weighted sum of the inputs to the neurons.  4. An activation function (squashing function) limits the amplitude of the neuron's output.	2
71	2	$y = \sum f(wx + b)$, where w are the weights, which change the input according to the learned weights, x is the input from the environment, b is the bias, which shifts the learned decision plane, and f() is the activation function, which limits the output to a desired region of values.	2
72	3	for a binary classifier we can use threshold activation funtion.    1) randomly initilize the weights     2) you calculte the output of the neruon     3) find out the error by subtracting expected output and current output.    4) modify the weights related to that input with respect to the error.    5)repeat the process 2-4 till the you get minimal error. 	2
73	3	YOUR ANSWER HERE	0
74	3	YOUR ANSWER HERE    continue_process = true    w = randomly_initialize()    while continue_process      for x in list of points          y = w.x            diff = d-y  // d is  the desired output            if(diff >= 0)              w = w + x              else              w = w - x                    if all points are classified without error          continue_process = false	2
75	3	n<- learinig rate    Repeat until the MSE is small enough:    t=t+1        for each point in training set do:            compute local vield of percepron: V = W*X            apply linear activation function: $y =  \theta(V) = V$            compute current error: e = (d-y)                              apply delta rule: W(t+1) = W(t) + n*e*X         end	2
76	3	w = [random number betrween -1 and 1]      for every data in training set      {   In the first layer:       calculate the weighted sum  using adder function      calculate the output of the activation function      In the ouput layer      calculate the output y        calculate the error e = d - y ; d- desired output         change the weights using the formula $ \delta w = \eta x_j e_j$    }     continue till the error converges 	2
77	3	    Initialize as many random weights as the dimension of the data points        For each data point:          if the output matches the desired output              do nothing          else:              change the weights in the direction of the datapoint so that the datapoint is classified correctly          end if      end for        if some weight was changed:          start again with the for loop      end if	2
78	3	* Randomly assign values to initial weights  * Run the perceptron network and calculate the error (e = y-d) where, e is error, y is output and d is desired response.  * Update the weights based on the error.  * If error is positive, add the error with the input and update weight.  * If error is negative, subtract the error with the input and update weight.  * If there is no error, don't update the weights.  * Repeat the above process until the calulated error is approximately equal to zero.	2
79	3	YOUR ANSWER HERE	0
80	3	We use threshold function as activation function.       if w.x + b >= 1       label class 1.      else label class 0.	0
81	3	e(n) = current error  <br>  eps = convergence criteria  <br>  n = learning rate  <br>  while (change in e(n) not less then eps) {<br>      calculate error e(n) <br>      w(n+1) = w(n) + n e(n) x(n)  (Widrow Hoffmen rule) <br>      }	2
82	3	apply input data to input layer and initialize small values weights    minimize error according to difference between desired signal and output signal    assign the test vector the class that has smallest error	2
83	3	Initialize the weight vector $\hat{w} = 0$  - do  -    for every training sample x,d             $v = \sum w_i x_i + b$           $y = \phi(v)$                      if d is not equal to y then               $e = d - y$               $w = w + \eta e(i) x_i$  -  until convergence     	2
84	3	**Pseudo code**  + initiate weights and bias randomly.  + compute output for the given input data $ y' = \sum_i (w_ix_i) +b$.  + compute error between computed $y'$ and desired output $y$.  + update weights: $w(n+1) = w(n) + \eta (y-y') x$  + stop when the error is below some specified threshold or becomes zero in case of data that is perfectly linearly separable.	2
85	3	YOUR ANSWER HERE	0
86	3	Initialize the perceptron with each weight equal to 0: $w(0) = 0$.    Present the labeled examples $(x_i, d_i)$ to the perceptron.  > for each example $(x_i, d_i)$  >> Compute actual output $y_i$ and error signal    >> Update weight based on the dlelta rule: $w(n+1) = w(n) + \eta (d(n) - y(n)) x(n)$	1
87	3	weights # a weight vector  phi = activation function  eta = learning rate  for each datapoint (x_i,y_i) do:      weights[i] = weights[i] + eta * (x_i[i]-y_i)*weights[i]	1
88	3	1: w, b = init_weights_bias() // the weights can be initialized to 0 or random initialized    2: n = 0    3: WHILE !stop_criteria() DO // iteration until stop criteria is fulfilled    4: y = w(n) * x(n) + b // calculate output    5: IF x is in C1: e = 1 // if the x belongs to class C1, error i 1, otherwise is -1    6: ELSE IF x is in C2: e = -1    7: w = w + e * x // update weights using the calculated error    8: n = n + 1    9: END    The stop criteria can be, if the number of misclassified input data is 0, then stop.	2
89	3	1. Initialize the weights at random or as 0.  2. Activate the Perceptron by giving an example.  3. Compute the actual output of the neuron.  4. Adjust the parameters of there perceptron.  5. Continue until convergence is achieved.    w = rand    y = sum($\Phi$(w*x))    for w_i in w:        w_i = w_i+$\eta$*e*y 	1
90	3	Define a bias in order to be able to trigger to which class data points will be classefied to. Assign initial randomly chosen weights, use a squashing function for example McCullon Pits, start training proccess and stop when error of output and desired output has reached desired percentage.	0
91	3	initialize weights with zero or small values;    sample data point, feed into network;    compute net output, use the step activation function;    compute error $e=(d-o)$, where d is the true label, o is the predicted label;    correct weights based on $w(t+1)=w(t)+\alpha(d-o)x$, where alpha is the training rate and x is the input pattern;    repeat for each pattern until convergence is reached;	2
92	3	For this case, the parameters that need to be learned are the slope of the line and the intercept. These are the parameters for the weight vector.      1. Initialize random small values for weight vector.  2. For input_data $x_i$ in Training Data:       - Apply the input to the weight vector.       - e = the difference between the local field and the desired output $(d_i-y_i)$       - Update weight: w(n+1) = w(n) + $\eta e x_i$     	1
93	3	1. Initialization: n(time step or iteration) = 1 and weights are small but randomly initialized  2. Activation of perceptron:  Apply training pattern to activate the perceptron  3. Compute Output: Apply Activation function to the local field(weighted sum of inputs plus bias)  4. Adjust Weights: Adjust weight if current output(y) != desired output(d)  5. Continuation: We continue by increasing n during each iteration and repeat from step 2 untill all input pattern are applied to network and also error is minimized 	2
94	3	YOUR ANSWER HERE:   y denotes the actual result, d denotes the desired result  positive train error: y = 0, d=1 $w_{new} = w_{old} + x $  negative train error: y = 1, d = 0 $w_{new} = w_{old} - x$	1
95	3	1. Label one class a positive with label +1 and the other class as negative with -1.  2. Augment the data with an additional value for the bias term.  3. Invert the sign of the data in the negative class.  4. Randomly initialize weights.  5. If $w^T \cdot x <= 0$, update weight by $ w(n+1) = w(n) + \eta x(n) $, else leave the weight unchanged.  6. Continue step 5.  7. terminate when there is no longer a change in any weight.	2
96	3	$\varphi(v) = \tanh(v)$, single node network, $\mu$ learning rate    repeat as long as error is too high:    1. present sample to network and collect output.  2. compare actual output with desired output (d).  3. If not equal adapt weights: $w_i(n + 1) = w_i(n) + \mu(d-y)x_i$	2
97	3	given $k$ date points $(x_i,y_i)$ and $y_i\in\{1,-1\}$    given a learning rate    for each point i        add a bias 1 so that point i == (1,x_i,y_i) ;        for each point i there y_i == -1        point = -1 * point;        w= Nullvector;    b = 0;    convergance = false;    while(convergence == false)        convergance = true;            for each point i in the training set:            if(w*x<=0) do                        w = w+learningrate*point_i;               convergance = false;      	2
98	3	training_set := set of labeled linear seperable data points    w := weight vector with dimension of input data    v := local field    phi(v): activation_function (threshold function)    y:= output    e := error (y - d) where d is the desired output from labeled training data    n := learning rate (0.1)        assign random values for w        for x in training_set:        v = sum(x_i * w_i)        y = phi(v)         e = y - d        w = w + n*x*e // delta rule    end      	2
99	3	pick random decision boundary  while one of data points is in wrong class      turn decision boundary by using vector of wrong data point      (negative rule or positive)	1
100	3	for n iterations        for each datapoint d             error = desired - output                    if error > 0                        weights = weights - error                        if error < 0                        weights = weights + error	2
101	3	initialize weights w and bias b    set learning rate n    set error_threshold (upper bound on error)    while error < error_threshold :        for every datapoint x in tarining dataset :          y = [w, b] . [x, 1]       (bias is represented as weight of fixed input 1)          if y is positive then x belongs to C1 otherwise to C2          store above predicted class.          find error in predicted output with respect to the labels          store error e      e_sum = sum of all errors e for every data point       w = w + n * e_sum          	2
102	3	YOUR ANSWER HERE	0
103	3	The linear binary classifiable data consists of input vector $X$ which when multipled with weights and added bias, fall into class+ or class- depending on the linear combination output of $WX + b$ being above 0(+) or below 0(class -).    Algo:  Para,meters : X,Y(desired output), W, b    1) weight vector W is initialized with small random values.    2) Input vector is chosen with a probabiity and output is computed using $WX + b$ . If the class Y of vector X is + and output is $<0$, or if the class of X is - and output if >0, then the weights are updated accordingly.  Otherwise weights are left unchanged.    3) iterated over other input vectors until convergence of output.	2
104	3	The learning process consists of three main steps:    1- Positive error:      - calculate the error of all the data sets in the learning set       - change the w(weight): w(n+1) = w(n)+positive error      - seperate the data points based on the new w  2- Negative error:      - calculate the error of all the data sets in the learning set       - change the w(weight): w(n+1) = w(n)+negative error      - seperate the data points based on the new w  3- No error:       - when we have no error this is the end of the training	1
105	3	1. Inputs X: $x_1$, $x_2$, ... , $x_N$    2. Desired outputs y: $y_1$, $y_2$, ... , $y_N$    3. Initialize weight vector $w$ to random small values    4. For each data point $x_n$ in X:         Calculate $\hat{y}_n$ from $w$ and $x_n$         Calculate error $e_n = y_n - \hat{y}_n$         Update $w$ according to delta rule         end      	2
106	3	1. Initialization : At time step  n(0), initialize weight vectors with random values $w_j(0)$  2. Activation : Apply the input example $(x_i(n),d_i(n))$ to activate the perceptron with heavyside step function as the activation function.   3. If output of the perceptron $y(n) \neq d(n) $, adjust the weight vector using the rule : $w(n+1) = w(n) + \eta x(n)(d(n) - y(n))$  4. Go to Activation and repeat until no more change in weight vector is observed	2
107	3	Initialize the weights randomly.    $y = \sum f(wx + b)$, compute the output of the perceptron using the input x, the weight w, the bias b and the activation function f().    $e = d - y$, calculate the error by substracting the actual output from the desired output.    $w_{new} = w_{old} + learning\_rate \cdot x \cdot e$, update the weights with this formula. The learning rate is a parameter which changes how fast the perceptron learns.	2
108	4	YOUR ANSWER HERE:  - Classification is a problem of assigning labels or classes to the input. The output is a discrete variable.  - Regression is a problem of assigning a continuous variable to the input.	1
109	4	classification is type of problem where algorithm needs to saperate the one data class from the another data class.   If there is 2 classes C1 , C2 . algorithm classify the given data into these two classes.   it is discreet process.    Regression is the pridicting the next point depending on the previous points.  it is continuous process.	1
110	4	Classification is the problem where the input data has to be put in two or more classes distinctively different from each other. For example in case of binary classification on class can be -1 and the other +1    Regression on the other hand is data fitting. THe main aim is to find a hyperplane which can fit a given input pattern.	1
111	4	YOUR ANSWER HERE    Classification: Is a task to partition the given input into one of several classes. The calsses are descrete values.    Regression: Regression is the tasks of predicting output in a continuous range. The prediction can be any value within a range.	1
112	4	In classification task the aim to separate data in  different classes, such that output of NN gives value of class index for each input point. E.g in the task is to classify binary data, then the output of the NN will 0 or 1, and each value, represent on class.    In case of regression task, the aim is to fit data, namely a function that perform input-ouput mapping. Output of NN in this case, will be error value, such that we know how close is out function fitted to data points.	1
113	4	Regression:     Tries to fit a line are curve among the given points    The have continuous output    the output is a function    Classification:     Tries to classify the given points into two or more calsses    They have a discrete output    the output is a value representing the class	1
114	4	Classifaction problem is used to classify set of data points into specific groups.    Regression is used to predict time series data.    Classification works on discreate set of values and regression works on continuous values.	1
115	4	Classification: Classification is done between the classes. The machine determines to what class the data belongs to.    Regression: Regression is a expecting output for an input. The machine learns from the given data and models a function and when new input is given it expects the output.    Difference: Classification is discrete output where as Regression is a continuous output.	1
116	4	Classification:   We need to predict the output data discretely. That is the output space is a discrete space.     Regression:  We need to predict the output data continuously. That is the output space is continuous space    The main difference is the discreteness and contionousness.	1
117	4	Classification is a problem of assigning a particular class to each data point in a given dataset. <br>  Regression is a problem of fitting the given dataset on a particular hyperplane which can be used for representing the given data. It finds the hyperplane which minimises the mean square error. 	1
118	4	classification: assign a test data to a class that is prescribed    regression: approximating an unknown function with minimization errors for input-output mapping. 	1
119	4	Classification if to assign a class or category to the data, while regression is when you fit the data to a function.	1
120	4	+ Regression: learns model/function that can predict other unseen data well. Target/output is real spaced.  + Classification: learns a model that classifies/maps input to a discrete target label. Targetlabel/output is binary/discrete.	1
121	4	Classification describes the application, in which a sample is assigned to one specific pattern of the problem. In comparison to regression is the output deterministic an not continiously. In regression the output is continuous describing 	1
122	4	Classification is the task of classifying the input signals into a finite number of groups, so the output is a number that indicates a certain class. Regression is the task of approximating a function by estimating the values given the input signals, so the output can be any real number.	1
123	4	Both are learning tasks of a ANN.   In classification the goal is to assign a class label to new datapoints.    In regression the goal is to eastimate a unkown function.    The only difference between both is that classification uses discrete class labels, while in  regression a continuous output is used	1
124	4	The approach of classification is to classify sets of input data into their correct classes (for example, used in pattern recognition). The approach of regression is to approximize to a defined function f by calculating the error between this function and the result of an algorithm. THe difference is that, the classification approach is applied to a discret data (the samples are the different points of the input space), and regression is an analogic approach where the whole function must be approximize (for any input given).	1
125	4	In classification a binary pattern has to be partitioned into the two classes. In regression a line has to be fitted closest to some datapoints. The difference is, that in Classification mthe output is a single class label, while in regression the output is continuous	1
126	4	Classification is a problem of destinguishing to which discrete classes input variables are to be assigned to, regression is estimation of the output, by figuring out the continuous trend of the whole dataset.	1
127	4	In classification tasks, we assign discrete labels to data points of our training dataset, either being assigned a specific label or not (binary). For supervised learning, these datapoints are labeled with a label vector ground truth. In regression, we try to model a function which fits the data points of the training data, and thus model a function with continous values.	1
128	4	_Classification_:      - It refers to classifying given data into discrete classes.     - The output is discrete values.     - Use for activity like pattern recognition, etc.    _Regression_:      - It refers to estimating the value of some continuous function given an input.     - The output is continuous value.     - used for activities like motor control, etc.	1
129	4	In classification, output values are always discrete.      In regression, output values are continuous	1
130	4	YOUR ANSWER HERE  A hyerplane is given by y = w*x + b . Regression wants to determine w    Classification wants to assign a class to a set of observations.    Regression wants to determine separating hyerplane, classification wants to label data points with a class	1
131	4	Classification:    In classification, the output produced by the NN is a discrete value which indicates which class the input belongs to.    Regression:    In regression, the output produced by the NN is a continuous variable. This could be used for instance, to approximate a continuous function.	1
132	4	In classification we try to assign classes to input data. Regression we want the network to behave like a given system/formala. This can also be a time series of input and output data.	1
133	4	In classification the goal is to saperates points into different classes. The outcome is a class lable.     Regression trys to fit a hyperplante to a point cloud best, so that future data is representet by that hyperplane best (LMS). It trys to minimize the distance to all data points. The outcome is a countinius variable.	1
134	4	Classification means mapping inptut data a class label, for example 1 and -1. IN regression on the other hand a continuous function is learned in way that f(x) - F(x) is minimized, where f(x) is the function learned by a learning machine and F(x) is the original function.  	1
135	4	In classification the task is to give an discrete output value to an input. It assignes one of all defined classes to the current input. Regression try to approximate a function while minimizing error and produces a continous output value. 	1
136	4	Classification tries to label discrete data points with distinct classes, while regression tries to approximate a continuous function from discrete data points. Results of these methods are respectively a labeled data set or a continuous function.	1
137	4	Classification is supervised learning where underlying function  representing the trining data is learn from training data to predict classes of datapoints or patterns drawn from similar distribution as of tarining data. Weights of the neural network are learned to minimize the error in classification.     Regression is supervised learning algorithm where underlying function  representing the trining data is learn from training data to predict the value of label or output of some system  for new datapoint or pattern of similar type. Weights of the neural network are learned to minimize the error in prediction of function.      Differences. :  1. Output of classification is discrete ( Class 1,2,3 ) whereas output of regression is continuous   2. Error in classification is number of wrong classifications whereas Error in classification regression is distance between lable value and predicted value 	2
138	4	Classification is about classifying the given data into different classes, where as regresssion is about finding the local/global minima.We use perceptrons to classify the data and we use unconstrained optimization techniques like newton's method to find regression.	1
139	4	  Classification is a problem of catergorization into discrete classes where as regression is a problem in a continuous space where the goal is to ether minimize or maximize a cost function.    Classification is the process of dividing a set of discrete inputs into classes corresponding to similar patterns such as clustering.  Regression could be finding a pattern of the distribution of the data such as ftting a line.	1
140	4	- Classification: In classification problems we have different groups of data that have some common properties and after training we want that our model can detect the class of the new sample correctly  - Regression: In regression we have a series of values and we want to use the previuos values in this series and predict the next value	1
141	4	Classification is separating the data into classes and the output is a discontinuous variable. Regression is fitting a model and the output is a continuous variable.	1
142	4	Classification in machine learning is used to find a decision surface in the form of a hyperplane that can separate a set of input examples (or set of patterns) into their respective classes. Regression on the other hand is used to find the parameters (i.e, the weight vector $w$ and the bias b) for the function thatcan best fit the given data points $\{x_i,d_i\}$ . Thus classification deals with predicting the class label for discrete data points whereas regression deals with fitting a continuous real valued function.	1
143	4	In classification the input data is split in 2 or more classes. The goal of the neural network is to learn the input data and then be able to classify new input data into the classes. Based on the learned information the network then maps input data into one of the classes, which is discrete space.    In regression the input data is learned aswell. But here the network tries to predict feature values, which are in continuous space. The network tries to predict close as possible to new input data only using the learned model.	1
144	5	randomly inilize the weights    draw sample of inputs     Increase the weights of the local neihburhood of winning neuron    repeat the process above process till there is only one winning neuron	1
145	5	YOUR ANSWER HERE	0
146	5	YOUR ANSWER HERE    for i in num_of_epochs      for p in input_points          find the winning neuron      find the neighbours of the winning neuron within distance sigma      update winning neuron and neighbours weight      update sigma and learning_rate so that both reduces over time	2
147	5	YOUR ANSWER HERE	0
148	5	  {  take a rondom point from the training data    competitive phase: find the winning neuron - the neuron similar feature, using the eucledian distance formula    cooperative phase: find the neighbors of the winning neuron based on the neighbor function (eg: gaussian function)    adaption phase: change the weights of the all the neighboring neuron of the winning node using the formula $ \del w = \eta x_j - w_j $    }   	2
149	5	    input: distance function d(x, y), learning rate mu, neighborhood distance n            Initialize the map layer with random weights      for each input:          find the weight which is closest to the input (minimum d(x, y))          change the weight in the direction of the input depending on the learning rate          change all weights which are within the neighborhood distance n depending on their distance and the learning rate          reduce learning rate and neighborhood distance	2
150	5	* Initialize the neuron weights randomly in a way that all neurons have different weights.  * Generate random samples x from the input space.  * Iterate the samples and **Compare distance between current input and all neurons** in the weight space.  * **Find a winning neuron** with shortest distance from current input.  * Distance is calculated using **euclidean or manhatten distance**.  * Find the neurons in the neighborhood boundary of winning neuron.  * **Update the weights** of neighborhood neurons using delta rule.  * Adapt the size of neighborhood $(\lambda)$ and learning rate $(\eta)$ at each iteration  * Repeat the process until there is no neurons in the neighborhood boundary or all the inputs moved to some neuron.	2
151	5	Step1: It selects a datapoint in random through sampling.    Step2: Finds the nearest neuron through competitive learning.    Step3: Updates the weight of the winner neuron and updates the weight of neighbouring neurons by a fraction.    Step4: Continues steps 1, 2, 3 until there is no change in the weights or some stopping criteria is met.	2
152	5	Begin   n = range of data set      Initialise the weights. #We give a small random weights.       for the range of n:          Select a input signal,           Find the winning neuron based on the similarity between the weights.           Update the weights of the neighboring neuron          Repeat until the convergence.                1. initalising   2. Sampling  3 Similarity matching.  4. Updating the weights  5. continuation. 	2
153	5	intialise weights <br>  while( significant change is observed in topographic pattern) {<br>  take a random input (sampling) <br>  find the winning output neuron     (competition)  <br>  adjust the weights of the winning neuron and its neighbourhood neurons (cooperation) <br>  continue <br>  }	2
154	5	Initialization: set random small values to weights wj is different for each neuron    Sampling: draw n-th sample x from input space    Competition: identify winning neuron i using arg min $||x-w_i||$ which means weight vector of i is most similar to input     Cooperation: identify neighbors of winning neuron i using neighborhood function $h_{j,i(x)} (n)$ which shrinks with time    Weight adaptation: adjustments made to synaptic weights of winning neuron and its neighbors    go to sampling until no large changes in the feature map.	2
155	5	  - Find the winning neuron  - Find the neighbors of the winning neuron.	0
156	5	**Pseudo code**  + 1. Initialize map neurons, based on topology it could be a lattice, on a circle, etc.  + 2. competition: find map neuron that is closest to an input neuron by computing distances $d$.  + 3. update the position of closest map neuron with update rule.  + 4. Do 2 and 3 until all input neurons are assigned a map neuron.  + Do 2,3 and 4 until specified iterations or the net cumulative distance goes below some specified value or becomes zero.	2
157	5	Produce train_SOM;    begin:        randomize weights for all neurons;            for (i=1 to iteration_number) do:                begin:                        take random input pattern;                            find the winning neuron;                            find neighbors of the winner;                            modify synaptic weights of these neurons;                            reduce learning rate and lambda;                        end;            end;	2
158	5	Initialize the network with small and random weights.    Sample the data set by  picking an input randomly.    > Determine the winning neuron based on the output value.    > Determine the coopertaing neurons based using the neighborhood function.    > Update the weights of the cooperating neurons.    > Adjust the learning rate.    > Stop if the network converges.	2
159	5	YOUR ANSWER HERE	0
160	5	1: w = init_weights() // equal to zero or random initialized    2: n = 0    3: WHILE !stop_criteria()     4: winner_neuron, y = (x, w) // find on the map layer which neuron is closer to the input (euclidean distance)    5: neighborhood = define_neighboor(winner_neuron, n) // define the neighborhood size (first iterations big, and being reduced)    6: eta = define_learning_rate(n) // define the learning rate (large value at the first iterations and being reduced)    7: diff_w = adapt_weights(neighborhood, eta) // adapt the weights just for the winner neuron and its neighborhood    8: w = w + diff_w // update the weights    9: stop_critera = must_stop(y, x) // look if the distance between input and the winner neuron is 0 (or really close to 0)    9: END	2
161	5	1. Initialize small random weights.  2. Draw the nth sample from the input space.  3. Similarity matching: Determine the winning neuron.  4. Update the weights of the neuron an the topological neighborhood.  5. Repeat steps 2-4     w = random    n = example.draw()    w_max = get_min(w_i*n)    h_n = get_neighborhood(w_max)    for w_i in h_n:        w_i = w_i+$\eta$*h*y 	2
162	5	Has three parts in it - Competition, Cooperation, Adaptation    get input variable and choose amount of neurons to be more than amount of variables    then run competition, where from the input neurons will be compiting to each other on choosing which fits the most  after finding winning neuron change weight of neighbouring neuron only    in cooperation weights of neighbouring neurons are adjusted to clusters    in adaptation neurons are pulled to input variables to establish the classification	0
163	5	initialize weights with small values (such that all of the weight vectors are different);    sample a datapoint, feed into network;    determine the winning neuron on the lattice, picking the neuron with the least euclidean distance of its weight vector to the input vector;    determine the neighbourhood of the winning neuron through the neighbourhood function;    change weights of the neurons, namely spatially 'pulling' the weight vectors of the neighbourhood neurons towards the input vector;    depending on the timestep, reduce learning rate and neighbourhood size based on wether we are in the organizing or finetuning step;    repeat until maximum number of steps;	2
164	5	1. Randomly initialize weights.  2. Randomly select an input from the training data.  3. Find the nearest neighbour of this input in the weights. This is done by finding the euclidean distance of the input from each weight. And selecting the weight with least distance.  4. Update the weights of all the neurons within the neighbourhood $h(n)$ (which is gaussian function, with an exponentially decaying $\sigma(n)$) of the winning neuron with some learning rate $\eta(n)$.      $$\Delta w_{ij}=\eta(n)h(n)(||x_i-x_j||)$$    where,    $$\eta(n)= \eta_0e^{-n/T_1}$$ and $$\sigma(n)= \sigma_0e^{-n/T_1}$$	2
165	5	i. First we initialize random weights for neurons    ii.  Then we choose random input from input space    iii. We compute distance between input vector and each weight vector.     iv. Neuron that have minimium euclidean distance with input vector is considered as winner neuron    v. Then, we find the neighborhood neurons of the winning neuron    vi. We adjust the weights of all neighborhood neurons    vii. Reduce the learning parameter and neighborhood size    viii. Continue until it converges.	1
166	5	YOUR ANSWER HERE  w denote weights  t denotes threshold  h denotes the neighborhood function, which decreases with distance d from winning neuron    h(x, x_{win} //neighborhood function   return (exp(-2/||x-x_{win}||)    w = rand(); //initialize weights with random value  while (w_{delta} > t){ //proceed until there are no notieable changes      x_{win} = arg min ||x-w||^{2}//determine x which is closest to w (competetive learing)       // Update weights of winning neuron      // weights of losing neurons are not updated      w_{new} = w_{old} + x*h(x, x_{win})*(x-w)//update weights of neuron which a are in neighborhood of winning neuron              }        	2
167	5	1. Arrange the weights in the required topology according to the problem.  2. Initialize the weights randomly such that all the weights are different.  3. Sample the input from the input space.  4. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron.  5. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and reduce the learning rate and make sure learning rate is above zero.  6. If ordering and convergence is complete, stop. Else continue to step 3.	2
168	5	In SOM we start with randomized weights, $\mu$ learning reate, $d_{ji}$ distance between j and i, $h$ neighbour function    repeat as long as error is too high/max iterations are not reached:    1. take input sample  2. find closest node/weight  3. find all it neighbours  4. move the weight and its neighbours closer to the given input, use the neighbour function (e.g. gaussian) to reduce effect to far distance neighbours  5. (optional) adapt learning rate and neighbour function	2
169	5	given a neigbourhood function $h_{ij}(n)$ and a lerning rate over time    randomly assing different weights from the input layer to the neurons in the second layer    for each training point x_i do:    - find the winner-takes-all neuron $k$ with $min ||x_i-w_i||$  - find the neighbours of $k$ with the neigbourhood function  - compute the new weights for those neurons using the neighbourhood function and the learning rate  - update (decrease) the neighbourhood function and the learning rate  end	2
170	5	YOUR ANSWER HERE	0
171	5	given a map layer   set random small values for weights from input to map layer  repeat until not converged:      find best match of input value and weight of the neurons (competitive process)      adapt (increase) weight of winning neuron and neighboorhood (with gauss function and neighboorhood size) (cooperating process and weight adjustment)      decrease neighboorhood size 	2
172	5	for n iterations        winner = competition_between_neurons()            neighbourhood = cooperation_with_neighbourhood_function(winner)            update_weights(neighbourhood)	1
173	5	Initialize weight vectors of hidden neurons with same dimmension as of data.    Number of hidden neuron should be signifiacntly greater than number of data points.    initialize learning rate n and neghbouring function h    while (rate of change in weights is significant):        for every datapoint:            calculate distance of each neuron from data.          select winner neuron w with minimum distance (maximum similarity)          error = distnce of winner form datapoint          adjust weights of neurons with the rule w = w + n*h*error          	2
174	5	SOM is refered to as Self organized maps which is an unsupervised training algorithm for finding spatial pattern in data without using any external help.The process in SOM is explained below:  -Initialization: Initialize random weights w_j for input patterns  - Sampling: Take n_th random sample from the input (say x)  - similarity matching :for the input x, find the best match in the weight vector.    $i(x) = argmin(x - w)$  - update: the next step is to update the weights  $w(n+1) = w(n) + eta*h_ji(x)*i(x)$  - continuation : continue from sampling until there is no significant change in the feature map	2
175	5	Parameters: $X$ data vectors, $W$ weights vectors in lattice , $\eta(n)$ -learning rate, $\sigma(n)$ - neighbourhood width, $h_{ji(x)}$- neighbourhood function     algo:    1) Initialize the weights to a small, random , non-repeatible values.    2) Sample a data vector with a probabiity     3) Compute the euclidean distance to weight vectors from the data points and find the winning neuron with minimum distance .    4) Update the weights of the winning neuron and its neighbourhood towards the input direction using neighbourhood function.    5) reduce the learning rate and the neighbour hood width and iterate from step 2 until no significant changes between weight vectors and inputs are seen. 	2
176	5	- randomly define some values for the synapitc connections in the network  - send the first input to the network  - in the output layer(map layer) select the neuron that has lowest error(competition phase)  - based on a predefined method define the neighborhood of the selected neuron(cooparation phase)  - change the weights of the selected neuron and the neurons located in its neighborhood(adaptation phase)  - if the stop condition satisfied stop the process	2
177	5	1. Initailization: Initialize the weights of each neuron to small random values such that weight of each neuron is different.  2. Sampling: Sample an input from the input set  3. Similarity matching: Determine the neuron nearest to the sampled input based on its distance  4. Weight updation: Update the weights of the neighbouring neurons chosen by the neighbourhood function $h_{ij}(n)$  5. Continuation: Continue from sampling until there is no more change in the weights	2
178	5	1. Initialization : Initialize the weight vectors with random values such that the $w_j(0)$ is different for all weights.  2. Sampling : Draw sample example $x$ from input space.  3. Similarity matching : Find the best matching weight vector for the input vector : $W_i = argmin_i  (x - W_i(n))$  4. Adjust the weight vectors of neurons in the neighbourhood of the winning neuron   5. Go to Sampling step and repeat until no more changes are observed in the local neighbourhood of the winning neuron.	2
179	5	Initialize the weights randomly. Create a term T1 and T2, which decrease the learning\_rate and neighbourhood function respectively.    Calculate $i(x) = argmin | w - x |$, the weight which is closest to the input data received.    i(x) is the neuron, which wins the competetive process, this neuron and its neighbours weights are updated using $w_{new} = w_{old} - learning\_rate \cdot h(x) \cdot (w - x)$. h(x) is the neighbourhood function which determines, which neurons are updated and how strong they are changed by the update. It is defined using the distance between the neurons.    The learning\_rate is updated using $learning\_rate / T1$, also is the neighbourhood function updated in the same way using T2. The learning\_rate cannot get lower than 0.01, while the neighbourhood function can get as low as only the winning neuron. So in the beginning almost every neuron is updated and at the end only a small neighbourhood or the neuron itself is updated.	2
180	6	YOUR ANSWER HERE: SVM is a linear machine whose goal is to construct a optimal hyperplane such that the marginal separation is the maximum between the decision boundaries. The decision boundaries are drawn parallel to the hyperplane which just push the datapoints closest to the hyperplane. The datapoints closer to the hyperplane are called support vectors.	1
181	6	support vector machines are the finding classfiers, draw  the dision boundary which push against the support vectors.	1
182	6	The basic idea of Support Vector Machine (SVM) is to find the width of a line or hyperplane which which divides the input data into two classes. The points lying on the edge of the defined width are called support vectors.  	1
183	6	YOUR ANSWER HERE    SVM is a classifier that classifies a set of points in a way that maximizes the margin between the points of two classes. The classification can be linear or non linear.	1
184	6	Given a training set for classification, The basic idea of SVM is to construct a hyperplane as decision boundary such a way that the margin between the positive and negative points is maximum     Support vector is a small subset of the of the training data against which the boundary is pushed	2
185	6	A support vector machine classifies given data using a decision boundary. The width of this decision boundary (margin) is maximized to ensure good results, because a maximized width is as robust as possible. The margin width is $\frac{2}{\sqrt{w * w}}$. To maximize it, quadratic programming is used. In order to handle noisy data, slack variables are introduced. To eliminate them, duality is used.	2
186	6	SVM tries to find a **best hyperplane with widest margin with the help of support vectors** such that all the data points are classified correctly.	2
187	6	SVM is used for linearly separable data. A hyperplane is used to separate the data, but there could be so many hyperplanes that separate the data. The best hyperplane is choosen which separates data with a bigger margin. So in SVM we find the hyperplane which has a bigger margin between the hyperplane and both the positive and negative data lines.	1
188	6	Given a dataset, support vector machines builds a hyperplane in a such a way that positive and negative samples are seperated to the maximum distance. Width of the margin should be maximum    The vectors to which the margins(margin for positive and negative sample) are pushed on to it are called support vectors. 	1
189	6	SVM stands for Support Vector Machine. It creates a hyperplane such that margin of separation between positive and negative classes is maximised.  	1
190	6	basic idea of SVM is to construct a hyperplane as the decision surface in such a way that the margin of separation between negative examples and positive examples is maximized.	1
191	6	Support vector machines are a type of neural network that build a desicion boundary around classes such that the margin of separation between classes is maximized. 	0
192	6	SVMs are binary classifier. They learn the classification by memorizing the marginal data points (called support vectors) that make up the decision boundaries (2 : positive and negative).	1
193	6	The abbreviation SVM stands for Suppor Vector Machine. SVMs represent a feedforward category of NN. SVMs are binary learning machines whose functionality can be summarized for classification problem as follows:  Given a training sample, the SVM constructs a hyperplane as the decision surface in such a way that the margin of seperation between positive and negative examples is maximized.    One key innovation associated with SVMs is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. It allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge efficiently. Besides, the kernel function k often admits an implementation that is significantly more computatinal efficient than naively constructing two vectors and explicetly taking their dot product.	2
194	6	An SVM, or support vector machine, is a feedforward network with a hidden layer to learn a task in a supervised learning manner. The network tries to construct a hyperplane that separates the data points of two different classes by maximizing the margin of separation, which is the distance from the hyperplane to the closest data points called support vectors. 	1
195	6	The SVM is a maximum margin classifier. It is used the binary classify datapoints in a dichotomy. The idea is to find a line wich linearly seperates both classes. There perfect position of this line is in right in the middle of these classes. To find this line(descision boundary) we define a positive and a negative boundary which are parallel to this line. The boundarys define the margin between both classes. The idea of SVM is that the datapoints which are next to the boundary can be used to define the margin. They are called support vectors.   Addittionaly not every problem is linearly seperable so the idea was to transform the input into many higher dimensions using some kernel functions. We discussed the kernel function of polynomial terms and found out that it easy to compute. 	2
196	6	Support Vector Machines are a type of learning machines that try to classify different classes of an input space. For linear separable classes, the SVMs try to calculate the line that separates this two classes with maximum margin. The support vectors will be the points closer to this margin. When the input data is noisy, we have an optimization problem of two aspects (maximum margin, proper classification). So, a trade-off (C) will be defined. The trade-off will be calculated by the sum of the distance of misclassified points.    For non-linear separable classes, a kernel will be defined that will transform the input data into a higher dimensional space.	2
197	6	An SVM is a linear classifier that divides a binary pattern, by a line that maximizes the margin between its line and the respective support vectors.	2
198	6	Support vector machines are classifiers that are using support vectors, which are variables of the dataset. These variable are chosen during learning algorithm. Main advantage of SVMs is that it will not be overfitting by choosing correct margin. Activation functions can be both linear and nonlinear. Output of SVM is always TRUE or FALSE for given variable.	1
199	6	An SVM is a binary, linear classifier spanning a seperating hyperplane between two classes of datapoints. The hyperplane is spanned between both the positive and negative decision boundaries, and supported by a number of support vectors. Support vectors are the outermost datapoints which span the hyperplane. During training, the distance of falsely classified data points to their correct side of the hyperplane is minimized, utilizing a quadratic programming formulation. 	2
200	6	A SVM is a binary classifier with a maximum width boundary separating the two classes. This uses support vectors (vectors that pushes against the boundaries).    The equations of the lines in an SVM are:     - $wx+b>=1$:for class 1     - $wx+b<=-1$:for class -1     - M is the width between these boundaries.	1
201	6	Basic idea of SVM is to best segregate the data into two classes with the help of decision boundary. This decision boundary is margin, we always try to maximize the margin to make sure data is classified correctly	1
202	6	YOUR ANSWER HERE Support Vector Machines goal is to maximize margin between closest data points of separating hyperplane. Separating hyperplane is given by: 0 = w(n)*x(n) + b. By maximizing margin probability of classification errors is reduced. 	1
203	6	A support vector machine is a maximum margin classifier in which the width of the boundary of separation is maximized. A margin is defined as the width of the boundary before hitting a point.     This maximum margin intuitively feels safe, and is experimentally good.	1
204	6	Because SVMs are binary classifiers we can use a border to sperate the data. The border is typically placed where it has the largest possible distance to both classes. The vectors the border touches on both sides with its margin are the support vectors.	1
205	6	A SVM is an ANN for supervised learning, whicht is able to saperate two classes of data-points by using a hyperlane found by quadratic programming, by finding the biggest margin. The goal is to classify future data in there two classes.	1
206	6	SVMs are used to linearly seperate Data points. The decision boundary is line or hyperplane in higher dimensions that defines the lable of a data point. The decion boundary is choosen in a way that the margin is maximized. Data points on the decion boundary are called support vectors and define the hyperplane. In 2 dimensions if the data is liner seperable the margin is equal to 2/sqrt(w.w) where w is the weight vector. If the data is not linear seperable, the input can be projected into higher dimension space. This increases the chance of linear seperablity.	2
207	6	A SVM uses a few of the data points as support vectors to bild the maximum margin classifier. It searches for the seperating line, which has the maximum margin to the datapoints. In cases of Noise, the seperating line is searched, which minimizes the distance to the points in the wrong category. The data is cast to a higher dimensional space to use covers theorem while using kernels. The data is more likely linearly seperable in the higher dimensional feature space. Using structural risk minimization the dimensionality is reduced. 	2
208	6	An SVM is a learning machine that tries to learn the support vectors of a two class data set to get the maximum margin, the optimal seperating hyperplane, between the two classes.	2
209	6	Support vector machine is classifier which maximizes the margin between boundries learned from two classes.  Margin is minimum distance by boundries can be increased before hitting datapoints.  Support vectors are the datapoints against which magin pushes up the boundary.	2
210	6	SVM refers to support vector machines.In terms of a linear classification problem svm can be defined as creating a hyper plane which is a decision surface and to maximize the width of decision boundary.In cases where the problem is complex svm can be used as it classifies the data by projecting the data in higher dimension.If the data is to be separated in 3 classes , they can use 3 svm's for three different classes.	1
211	6	The idea of SVM is to fit a supervised model onto the training data allowing maximum generalization ability.     This is done by computing maximum margin between different classes of data using the support vectors.  The magrin can be computed using differeent kernels for a higher dimensional data.	2
212	6	In linear SVM we have a linear border line classifier that seperate two different classes(positive and negative planes) and we calculate the distance of the data points from this border line classifier. Also a margin will be defined and this margin will be maximized until it touches some data points in the plane. The data points that the margin pushed agains them will be our support vectors. The error for the wrongly classified datapoints will be calculated by calculating the distance of the data point from its correct plane. The SVM tries to learn the classifier and the margin from the training data. 	2
213	6	The basic idea of SVM is to determine the best decision boundary i.e. the one which provides maximum margin so that the boundary can be widened most before it touches any datapoint. It is done using Support Vectors which are the the datapoints the margin pushes against.	2
214	6	A SVM is a linear machine which is used in pattern classifcation problems to find a decision surface in the form of a hyperplane for linearly separable classes such that the margin of separation between the classes is as large as possible. SVM's are an approximate implementation of the induction principle of structural risk minimization which is based on the fact that the error rate in testing is bounded by a term that is dependent upon the sum of training error rate and the VC dimension of h. 	1
215	6	A SVM learns a decision boundary from the input data. Additionaly it learns two margins, which are parallel to the decision boundary and lie as close as possible at the data points, the support vectors. The decision boundary is chosen so that the margins are maximized. Using kernel functions higher dimensional data and non linearly separable data can be learned aswell.	2
216	7	Steepest decent while move in the direction of the max improvement ( in terms of decrasing) in the cost funtion or error.    if the learning rate is large then the it follows the zizag motion.    if the learning rate is too low then it takes time for converging .	1
217	7	The method of steepest descent is responsible for weight adjustments in the network. The weights are adjusted in the direction of the steepest descent that is equal to the negative grad of the error. It ensures that the weights are decreased in every iteration step.	2
218	7	YOUR ANSWER HERE    Steepest decent method helps in making the adjustments of the weights in a neural network in a way that minimizes the average squared error.    In each step it gives the direction towards which the maximum decrease of the average squared error can be achieved.	1
219	7	The method of steepest decent is used for finding minimum of a cost(error) function. The steepest decent iterates over possible values of weight vector to optimize the function. It is used for deriving error function in ADALINE (adaptive linear element) algorithm, and it is used also in backpropagation method in training of Multi-layer NNs.	2
220	7	The steepest descent finds the direction of the error function and tries to reduce it by adding in the opposite direction    $ del w = - \eta g(n)$    g(n)- gradient of the cost function	2
221	7	The steepest descent is used to find the right direction in which the weights should be changed while learning a network. The derivate of the error is used and weights are changed in that direction which makes the error smaller as fast as possible.	2
222	7	Steepest decent helps to **minimize the value of error function $E$** by finding the **right direction **to move the weight vector to reach global minima.    The direction is always **opposite to the direction of actual gradient vector**. 	2
223	7	Method of steepest descent is used to reduce the error. In backpropogation during backward pass we need to know how by how much amount the weights should be changed, this can be known if we use steepest descent, find the gradient of error and use it to reduce the error.	2
224	7	In steepest descent the adjustments done on the weight vector are in the direction of the steepest descent which is in the direction opposite to that of a gradient descent.     In a learning problem, it basically used to reduce the cost based on the weight. The main goal is to find an optimal weight.	2
225	7	Method of steepest decent is an unconstrained optimization technique used for learning in a network. It is used in iterative manner to minimize the error in supervised learning. It finds the direction of maximum gradient. So we go in the opposite direction hoping to find the minima. Convergence of the algorithm depends on the learning rate and also the condition that it doesn't get stuck in local minima.	2
226	7	steepest decent method is based on minimization of error cost function $\xi(w) = 0.5 e^2_k(n)$, so synaptic weight of network is updated in a direction opposite to gradient vector of $\xi(w)$, that is $W_k(n+1) = W_k(n) - \eta \nabla \xi(w) = W_k(n) - \eta e_k(n) x(n)$, $\eta $is learning rate.$e_k(n)$ is neuron k error signal, $x_j(n)$ is input data.	2
227	7	The steepest descent is an unconstrained optimization method that seeks to minimize an error function. This function is iteratively changed in direction oposite to the gradient vector.	1
228	7	Method of steepest descent updates the weights in the direction where the error is minimum.	1
229	7	The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. This property is used to determine the optimal weights of the NN.	2
230	7	Steepest descent is used to update the synaptic weights of a network based on a cost function expressed by the errors of the output. The weights are adjusted in the direction opposite to the gradient of the cost function.	2
231	7	The idea of learning a network is to minimize a certain costfunction. We can use steepest descent to minimize this cost function. While there are other optimization techniques which can be used for optimization, steepest decent is a widly used optimization technique. To optimize a network we calulate the partial derivatives(gradient) and use it to update our weights. It is also used in BP.	2
232	7	The approach of the method of steepest descent is to find the direction for the minimization of the error in an approximation problem. The cost function e, dependent of the weights w, will be derivated (partial derivative) for all defined weights. This gradient will be used for updating the weights for the next iteration. The direction of the minimization of the error is the oposite direction of the gradien: - g.	2
233	7	The steepest descent can be used to optimize the weights of a network. In steepest descent the error function is a function of the weights. So we determine the direction of the steepest descent on the error surface and go into that direction to minimize the error of the weights on optimize them. 	1
234	7	Steepest descent is a method of weight adaptation. It is using first order derivative to approximate the function. Therefore is rather slow.	1
235	7	When learning weights with a SD method, we try to reduce the error based on following the gradient of an error function in the opposite direction, effectively trailing the error surface towards the minimum. Here, the error function (typically some form of mean squared error) is differentiated w.r.t. the individual weights, expressing how much a weight contributes to the network error and must thus be corrected. Due to the gradient pointing in the direction of steepest ascent, we must thus step in the negative direction.	2
236	7	- Steepeset descent is used for error minimization when updating weights.  - According to this, we update the weights along a direction which minimizes the error; which is calculated by finiding the slope at the point.	1
237	7	Steepest descent is method of optimizing the algorithm by minimizing the error. Weights are adjusted in the direction of steeping descent, opposite to the direction of the gradient.	1
238	7	YOUR ANSWER HERE Steepest descent moves the error within error surface a small step into the opposite direction of gradient. By help of steepest descent we want to minimize error. Steepest descent stops when gradient = 0.	2
239	7	In steepest descent, the gradient of the cost function is found by partially differentiating it with respect to the weights. The weights are then updated in the opposite direction if the gradient. This ensures that the weight moves in the steepest direction are reduced. It can also be proven that the weights always reduce. Hence, steepest descent can be used to minimize the cost function.	2
240	7	Speepest decent is used to minimize the training error of a network given sample inputs and desired outputs. It uses the gradient of the error function to move the weights closer to an optimal weight with lowest output error. Using a learning rate we can influence the speed and stability of this algorithm.	2
241	7	The steepest decent is the direction the error function falls the most. We want to change the weights in the direction of the steepest decent (the opposide direction of the gradient) to have a smaller error in the next iteration and to optimize the ANN.	2
242	7	In error correction learning the weights of a network are learned in a way that e(x) is minimized, where e(x) is some error function. In order to minimize the error function the method of steepest desend is used. The negative gradient of e(x) points in the direction of steepest decend. Doing steepest descend in a single layer feed forward network leads to the delta rule.	2
243	7	the error function is computet. to adapt the weights (learn the network) the error function is followed in small steps in direction of steepest descent to decrease the error. using iterations the error is decreased in each step and end in a (local) minimum  used in back-propagation	1
244	7	Steepest descent is the basic learning algorithm others are derived from. The goal when learning a network is to minimize the error. This is achieved by starting at a random position and going in the opposite direction of the gradient vector, the steepest descent.	2
245	7	Steepest descent adjust the parameters (weights and bias) of the NN to minimize the error. It does so by adjustinmg the weights in the direction of steepest descent of the error function. 	1
246	7	The method of steepest descent moves in the direction opposite to the gradient to minimize the cost funcion .	2
247	7	Steepest descent involves weight updation in the direction of maximum steep or maximum derease in the cost function ot in the direction opposite to the gradient funcion. The weight update is $\Delta w(n) = - \eta g(n)$ where $\eta$ is the learning rate which defines the magnitude of learning using the gradient g(n) which is the gradient of the cost function of errorsin the nth iteration. Higher $\eta$ will result in rapid learning but with oscilations in responses.	2
248	7	When the inputs are being send into a network and we calculate the error we need a mechanism to learn and manipopulate the free parameters of the network and the learning uses the error but we must know in which direction in the search(optimization) space we should move so that we can reach the global minima of the error for this we use steepest decent. This method tells us in which direction we need to move by getting the gradient from the error.	2
249	7	When learning a network the steepest descent algorithm updates the weights in such a way that the error decreases in every iteration.	1
250	7	The method of steepest descent is used to find the direction in which the error function viewed as a function of weights is decreasing most rapidly and then take a small step in that direction. When learning a network, steepest descent enables to iteratively adjust the weight vectors until the optimal weight vector that minimises the cost function (i.e, the error function where error is computed as the difference between the desired and actual response of the network) is found.	2
251	7	The method of steepest descent is used to minimize the error function. The error function is the gradient of the error $\Delta e = d - y$, where d is the desired output and y is the actual output of the neuron.	2
252	8	YOUR ANSWER HERE: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$ if the hypothesis can clearly distinguish the positive examples from the negative examples in A.	1
253	8	Ginven a data set A if it is possible to find a hypotheis H which separates the data set into binary form without any error, we can say that hypothesis $h \in H$ shatters dataaet $A \subseteq X \Leftrightarrow \ldots$.	2
254	8	YOUR ANSWER HERE    It means that for all the points in A with input output pair (x,y), for any combination of ($x_i$,$y_i$) there exist parameter $\alpha$ of h that enables h to classify the points with zero error	2
255	8	We say that a hypothesis h shatters a dataset A, iff the h produces a zero training error for certain data set A. In other words, we say that a hypothesis h shatters a dataset A, when h separates data A in two classes without erorr.	0
256	8	H is the vc dimension of a learning maching that can shatter h points.     Vc dimension of a learning machine is the maximum number of points that can be arranged so that the learning machine can shatter them    Shattering:    The learning machine is said to shatter points $(x_1 ... x_r)$ if and only if all the possible training set of $((x_1,y_1) ... (x_r,y_r))$ can be classified with zero training error	2
257	8	A hypothesis shatters a dataset if it can correctly classify all combinations of labellings of the points in the dataset.	2
258	8	A hypothesis space H shatters a dataset, if and only if there is a **possbile $\alpha$ (weight vector)** on hypothesis space that **seperates all the positvie data from negative data**.	1
259	8	a hypothesis $h \in H$ shatters $A \subseteq X$ if and only if there exists a value of $\alpha$ for which the training error is zero	1
260	8	Given a dataset $A \subseteq X $ where X is the instent data space, for a given problem with the dataset A, if a learning machine is able to successfully split the positive and the negative data, then we say that A is shattered by the learning machine. 	2
261	8	Suppose X is a training dataset and A is the subset of training dataset then hypothesis h is said to shatter if can correctly classify all the points in A i.e zero training error.	2
262	8	For all possible binary labeling of dataset A, we can find a hypothesis h that can separate the positive examples from negative examples, the H shatters A.	2
263	8	A machine F can shatter a set of points $x_1, x_2, x3_,..., x_n$ if and only if for every training set, there is a weight vector $\alpha$ that produces zero training error.	2
264	8	A hypothesis $h \in H$ shatters a dataset $A \subset X \Leftrightarrow$ for each assignable configuration of $(x_i, y_i)\in A$, $h$ perfectly classifies all elements of the set $A$.	2
265	8	YOUR ANSWER HERE	0
266	8	A hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow$     at least on possible combination of dataset $A$ can be classified by the hypothesis $h \in H$ with zero training error.	0
267	8	there exists an arrangement of these points in A sucht that for each possible combination of labels to these points  the hypothesis h has zero training error	0
268	8	An hypthesis *h* shatters a dataset A, if for a given data set, h is able to distinguish (or separate) the different classes of this data set.	2
269	8	, if there exists a configuration of $X$, so that $h$ gets zero training error on any dichotomy of the datapoints.	0
270	8	H shatters A when for example in given dataset (X_1,X_2...X_r) output are in a form (X_1, Y_1),(X_2,Y_2)...(X_r,Y_r) there has been found a 0 error.	2
271	8	... when our learned machine achieves zero training error on every classification problem of the dataset A. Since we got a selection of $n$ points in the dataset A, the number of problems in binary classification is 2 to the power of $n$ (I didnt find the 'Dach' symbol on the english keyboard :) )	2
272	8	Considereing a dataset $A \subseteq X$ ,where X is the instance space and A contains N elements. Now there are $2^N$ binary maps or learning problems when we wnat to separate two classes.    If any of these problems can be separated completely by hypothesis $h \in H$ then h is said to shatter A.    i.e., a hypothesis shatters a dataset, if it can completely separate the classes with zero error for all possible combination of labels in the dataset. 	2
273	8		0
274	8	YOUR ANSWER HERE A a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$ if there exists a an $\alpha for every training set with zero training error	1
275	8	A dataset $A \subseteq X $ with N datapoints has $2^N$ binary maps. If for any of these binary maps, a hypothesis $h \in H$ splits the positive data from the negative data such that there is no training error, then it is said that h shatters the dataset A.	2
276	8	when every possible combination of input and desired output can be classified using $h$	1
277	8	A hypothesis $h \in H$ shatters a dataset $A \subseteq X$, then for every point $x_i \in A$ there is a label $y_i \in \{1,-1\}$ and the $H$ can saperate these two classes using $h$ with no training error.	2
278	8	h shatters A when and only when for all possibilities of (a_1, y_1), (a_2, y2), ... ,(a_n, y_n), where y is the class lable (1 or -1) there exists some $ alpha  $ for a learning machine f that produces 0 training error.	2
279	8	when all combinations of position and labeling of the data can be separated in the given classes by the hypothesis	2
280	8	for all possible classified subsets of dataset A the hypothesis h can seperate it	2
281	8	If there exist atleast one configuration of A for which training error of h is zero. i.e. it successfully classifies all oints in A. 	0
282	8		0
283	8	A hypothesis h is model that separates a dataset consisting of {(x_i , y_i)} samples into positive and negative samples. h is said to shatter a given subset of a dataset if it can successfully separate at least one configuration of the subset of dataset.	0
284	8	"""h"" shatters A if for any set of input data points in A there exist at least one training error of zero."	2
285	8	for each of the $2^N$ (where N is the size of A) combinations of input output mappings of the form $(X_i, y_i)$, h is able to classify the data correctly that is with zero error. 	2
286	8	A hypothesis $h \in H$ shatters a dataset $A \subseteq X $ if there exists an $\alpha$ for which there is zero training error	2
287	8	there exist w weights, which produce a perfect classification.	0
288	9	YOUR ANSWER HERE: Widrow- Hoff rule:  - $\Delta w$ = $\eta e(n) x(n)$  - Widrow-Hoff rule states that when an input x(n) produces an error e(n), then the change in the weight is directly proportional to the error signal and the input signal.	1
289	9	This the basicly the calulating mean squared  error (MSE) from the expected output and real output.    Modifiying the weights for Minimizing MSE it . 	0
290	9	YOUR ANSWER HERE    $\Delta W_{ji}$ = $\eta e_jx_i$    Adjustment made to the weight of a neuron is proportional to the product of the error in that neuron and input applied to the neuron.	2
291	9	Widrow-Hoff learning rule is derived from LMS error method, and it is defined as: $W{t+1} = W{t} + \mu \cdot \Delta W$, where $\mu$ represent learning rate, and  $\Delta W = -(gradient \ of \ instantaneus \ erorr) = -(d - y)X $, Here $d$ represent desired signal, while $y$ represent output signal of a neuron. $X$ represent input of a neuron	2
292	9	delta $ w_{kj} =  \eta e_k . x_j $    Widrow hoff rules states that the change in synaptic weight is proportional to the product of the error signal and the input signal 	1
293	9	$\Delta w(n) = \mu * x(n) * e(n)$    $\mu = $ learning rate    $x(n) = $ input at timestep n    $e(n) = d(n) - y(n)$    $d(n) = $ desired signal at timestep n    $y(n) = $ output of the network at timestep n    The Widroff-Hoff (or delta rule) changes the weights depending on the input and the error, which is the difference between the output of the network and the desired output. This weight change can be scaled by a learning rate.	2
294	9	$$\bigtriangleup \omega_{ji} = e_j * x_i$$  $$\omega(n+1) = \omega(n) + \eta \bigtriangleup \omega_{ji}$$    Widrow Hoff learning rule says that, the synaptic weight update is directly proportional to the product of error and the input.	2
295	9	Widrow-Hoff learning rule: The rules states that the weight update is directly proportional to the product of the input to the neuron and the error.    $\Delta w_{ij} = \eta e(n) \sum x_i(n)$	1
296	9	Widrow hoff's learning rule states that the adjustment of the weight of a synapses are propotional to the product of the error function and the input which is given by the synapses based on the problem.  	1
297	9	Widrow Hoff rule is based minimising the mean square error using gradient descent alogirthm. Weights are adjusted in following manner:<br>   w(n+1) = w(n) - n (gradient of mean square error) <br>  It takes the gradient of the mean square error $0.5 e^{2}(n) = e(n) \frac{\partial e(n)}{\partial w} = e(n) x(n)$	2
298	9	it is based on minimization of error cost function $\xi(w) = 0.5 e^2_k(n)$, so synaptic weight from neuron k to input j is updated in a direction opposite to gradient vector of $\xi(w)$, that is $w_{kj}(n+1) = w_{kj}(n) - \eta \nabla \xi(w) = w_{kj}(n) - \eta e_k(n) x_j(n)$, $\eta $ is learning rate.$e_k(n)$ is neuron k error signal, $x_j(n)$ is input data.	2
299	9	The Widrow-Holf or delta rule is a gradient descent learning rule used to adapt weight in a perceptron.     $\Delta w(n) = - \eta(d(n) - y(n))x(n) $    $\Delta w(n) = - \eta e(n)x(n) $	1
300	9	The widrow-Hoff (delta) learning rule is given by  $$ w(n+1) = w(n) - \eta x(n) e(n)$$  where $e(n)$ is the error vector, $\eta$ is learning parameter, $x(n)$ is input vector.	1
301	9	The Widrow-Hoff Learning rule is also referred to as Delta, or Least Mean Square (LMS) Rule. It is used to minimize the cost function and is defined as follows:    Delta w_ji(n) = eta (partial x_i(n) / (partial w_ji(n))    where eta is the learning rate paramter, x_i(n) is the total instantaneous error energy and w are the weights.	1
302	9	The Widrow-Hoff learning rule, also called delta rule, is used for learning a network by adjusting the synaptic weights of the network with the error signals:    $$ w(n+1) = w(n) + \eta (d(n) - y(n)) x(n) $$    where $n$ is the number of iteration, $\eta$ is the learning rate, $d(n)$ is the desired output signal, $y(n)$ is the actual output signal, and $x(n)$ is the input signal. $(d(n) - y(n))$ is the error signal.	2
303	9	$$ \Delta w(n) = \eta * e(n)*w(n) $$  $$ e(n) = (y-d) $$  The widrow hoff learning rule is error correction learning. It is used to train a network in a supervised manner. The widrow hoff learning rule can be derived from gradient decent. The rule consists of the error e(n) the neuron has and is muliplied with the weight so that the impact of the weight to the error is incorporated into the update.  A learning rule is use as a adjustment in how much we trust the weight change. The error is calculate by the difference between the current and expected output.	2
304	9	The Widrow-Hoff learning rule is defined as: $w(n + 1) = w(n) + \eta * x(n) * e(n)$    The Widrow-Hoff learning rule is a rule for adjusting the weights of a NN for a error correction learning task. This learning rule is derived from the steepest descent method, where the direction for the minimization of the error is the defined as the oposite direction of the cost function's gradient. This gradient can be simplified as $x(n) * e(n)$, where e(n) is defined as the difference between the desired response and the actual response of the learning machine (NN): $e(n) = d(n) - y(n)$.    $\eta$ defines the learning rate used.	2
305	9	The Widrow-Hoff rule is used in error-correction learning and uses the current error and output of the system to determine the new weights.    $w(n+1) = w(n)+\eta \cdot e(n) \cdot y(n) $	1
306	9	Windrow-Hoff rule is   $$W_{new}=x_{input}*W_{old}*(d_{output}-y_{output})*eta*a $$    where   $W_{new}=new weight,W_{old}=old weight,d_{output}=desired output,y_{output}=actual output,x_{input}=input, eta=learning rate, a=learning constant$	0
307	9	For neurons with a linear activation function (ADALINE): $w(t+1)=w(t)+\alpha (d-y)x$, where x is the input pattern, d is the true value and y is the net output. Notice that the delta rule looks similiar to the perceptron learning rule, but was derived from SD, whereas the perceptron works with a step function which is not fully differentiable.	2
308	9	Widrow-Hoff learning rule is also known as error correction rule is used to update the weights as:  $\Delta w = \eta (d_i-y_i)x_i$ where, d is the desired output and y is the output the network generates and x is the input.	1
309	9	Weights adjusted are proportional to the product of error signal and the input vector    w(n + 1) = w(n) + $\eta(d-y)x(n)$    $\eta$ is learning rate, d is desired output, y is current output. x(n) in input vector. 	2
310	9	YOUR ANSWER HERE Adaption of weight is proportional to product of input and error:  $w_{new} = w_{old} + x*e$	1
311	9	$ \Delta w = \eta e(n)x(n) $, where $\eta$ is the learning rate.    Widrow-Hoff rule states that the change in weights is proportional to the product of the error and the input in the corresponding synapse.	2
312	9	$w(n+1) = w(n) + \mu (d(n) - y(n))x(n)$    The change of the weights is determined using the error ($d(n) - y(n)$) and the input that was given to the network. The learning rate can improve learing speed. The new weights are dependent on the old ones and the change calculated	1
313	9	$w_{ij}(n)= w_{ij}(n-1)+ learningrate*(d_j-y_j)*x_i$    we change the weights by computing the error $e_j= (d_j-y_j)$ for the input and multiply it by the learningrate and the $x_i$ and adding it to the old weight. This minimises the squared error function (our cost function) and is the online variant of the steepest decent method. 	2
314	9	Rule: w+1 = w + n * x * ( y - d)  where n is the learning rate, x is the input, y is the ouput of the network d is the desired output    The widrow-Hoff rule minimizes the error (y-d). The weight change is proportional the ibnput x and the error. It can be derived from steepest descend.	2
315	9	w_new = w_old + learning_parameter * error(n) * input(n)    while error is: desired_input - current_output    the new value for the synaptic weight is computed of the old value plus a learning rate times the current error and the input. The output error is decreased in each step until the change is to small or the generalization is sufficient 	2
316	9	weights(t) = weights(t-1) * learning_rate * (desired(t) - output(t))    The Widrow-Hoff rule, also the delta rule, is used to update the weights of neural networks in a learning algorithm. It uses the previous weights' result and compares it to the desired result. This discrepancy is then applied to update the weights based on a learning rate.	2
317	9	YOUR ANSWER HERE	0
318	9	Widrow -Hoff learning rule states that the adaptation made to the synaptic weights is proportional to the product of input and the error function.It basically states that if the error is high then the product of input and error will also be high , and thus the adjustment made to the weight would be more.  $w_j(n+1) = w_j(n) + eta*(error)*input$	2
319	9	Widrow Hoff learning rule is also called as error corresction learning rule. The error is defined as the difference between the desired and the actua output of the learning machine. Assuming the desired signa is available, the error is computed and weights of the neural network are upadted in the direction of reduction of errors. The error for each input sample for a neuron k is computed using $e_k(i) = d_k(i) - y_k(i)$. weight change $\Delta W = W*e$, that is the dot product of error and the weights is computed.	2
320	9	YOUR ANSWER HERE	0
321	9	The Widrow-Hoff learning rule is given by  $$w(n + 1) = w(n) + \eta e(n) x(n)$$    where   $w(n)$: Weight in iteration n    $e(n) = d(n) - y(n)$: Error    $d(n)$: Desired output    $y(n)$: Actual output    $x(n$: Input      $\eta$: Learning rate	1
322	9	Given a neuron k excited by an input signal $x_i$, if $w_{ki}$ is the synaptic weight of the neuron, then the Widrow-Hoff learning rule gives the weight adjustment $\Delta w_{ki}$ applied to the neuron k in mathematical terms  as follows: $\Delta w_{ki} = \eta x_i(n)e(n)$ where e(n) is the instantaneous value of the error signal. Thus the  Widrow-Hoff rule states that the synaptic adjustment applied to the weights of a neuron is proportional to the product of the input signal to the neuro and the instantaneous value of the error signal. This rule assumes that the neuron has an external supply of desired response so that the error can be computed. 	2
323	9	$\Delta w(n) = learning\_rate \cdot x(n) \cdot e(n)$, where x is the input data, $e = d - y$ is the error from the desired output and the actual output, and the learning_rate is a parameter chosen as necessery to change the speed of learning.    $w_{new} = w_{old} + learning\_rate \cdot x \cdot e$, this is the formula to update the weights and to learn the input data.	2
324	10	the back propagation algorithm  it consist of forward pass and backward pass    computes the output of the neuron     then it propagates in backward direction while recursively compute local gradient of the neuron     weights are adjusted accordingle.	1
325	10	Back Propagation is the process of learning in Multi Layer Perceptron in which the error from, the output of the network is fed back into the network to adjust the weights in the hidden layer. That is the error back prpagates into the network to enable the network to learn by adjusting the synaptic weights based on it.	1
326	10	YOUR ANSWER HERE    * Back propagation is a process to make adjustment to the weights of a neural network in a way that minimizes the average squared error of the training data.    * It uses steepest decent method. In each step it moves towards the direction that gives maximum decrease of the error.    * In back propagation the error is propaged backward from the last layer towards the earlier layers. The adjustments made to the weights is proportional to the partial derivative of the error with respect to the weight.    * The partial derivative is calculated using repeated application of the chain rule.	1
327	10	The idea of back propagation method is to propagate error from ouput (final) layer backward to hidden layers, and adjust the weighs of neurond in hidden layer, based of this error. This is required because we do not have error information for hidden layers, only for output neurons. The error from output layer is propagated to hidden layers using idea from steepest descent method. Namely, local gradients are computed for each neuron in backpropagation, and these local gradients define how error changes, in terms of weights. Local gradients are derived from chain rule for each layer. The fact that local gradient for each hidden layer is derived based on local gradient of a previos layer, defines that as we propagate more and more in hidden layers of NN, the gradient of a error function vanishes, which means that as we go deeply back in NN, the change in weights is becominng smaller and smaller. This is a drawback of back propagation method. 	2
328	10	The back propagation is a learning method in neural networks. Back propagation enables the feed forward netwowrk to represent XOR gate.    It has two phases:    forward pass: the initial weights are used to calculate the value of the output neuron    backward pass: starts from the output layer and travels backward. During this phase the weights are changed based on the local gradients of each neuron|	1
329	10	Back propagation is used to learn weights in a multi-layer feed forward network. It is divided into two steps: forward and backward. In the forward step one input is passed through the network to calculate the output of the network. This output is used to calculate the error of each output neuron given the desired output. After this forward step, in the backward step the weights are changed beginning in the end of the network. Each weight is changed by taking the derivative of the activation function of the neuron times either the error, if the following neuron is an output neuron, or all local gradients of connected neurons times the corresponding weights. The weight changes are the local fields.	2
330	10	* Backpropagation is a steepest decent method that calcualtes the error at the output neurons and backpropagates those errors backwards to update the weights of each neuron.  * The synaptic weight updated is directly proportional to **partial derivatives**  * Local gradient is calculated at ouput neurons and hidden neurons.  * Local gradient at output neurons are calculated using the observed error.  * But the error function is missing in the hidden neurons, so the local gradient of hidden neuron j is calculated recursively from the local gradients of all neurons which are connected directly to the hidden neuron j. 	2
331	10	Backpropogation has 2 steps.     Forward pass: In forward pass the data is run through the network and the error is calculated.    Backward pass: In Backward pass the weight is adjusted using local gradient of error such that the error is minimized.    There are many ways for weight adjustment like, steepest descent, Newtons method, Gauss newton method.	1
332	10	Back propogation usually occurs in a multi layer perceptron.     It uses a non linear activation function.     Basic elements:   1. Functional signals: These are the input signals, which passes through the network from left to right. As the name denotes it performs a usefull function at the output of the neuron and another reason for the name is that the functional signals are calculated based on the parameters and the activation function.     2. Error signals: Error signals propogate usually in the reverse direction which contains the error based on the desired output.     It consists of 2 phases:   1. Forward phase: In the forward phase the signals propogate from left to right. Weights are fixed and passes through all the layers of the network, that is undergo all the activation.     2. Reverse phase: In the reverse phase, the local gradients are calcualted and are propogated through in the backward direction. Here weights change.	1
333	10	Backpropogation is used for training multi layer networks. It constitutes of forward pass and backward pass. In forward pass network computes the output. Based on this the errors are calculated based on difference between network output and desired output. These errors are the backpropogated to network during backward pass and used for adjusting the synaptic weights. 	1
334	10	It contains forward pass and backward pass. In the forward pass, input is applied to the network and propagate it forward through the network, then compute the output of neurons in output layer and errors for output neurons. In the backward pass, compute local gradients and update the synaptic weights according to error correction rule for each neuron layer by layer in a backward direction.	2
335	10	Backpropagation is a learning algorithm in multi layer networks that consists of two phases, a forward pass and a backward pass. In the forward pass, the output is calculated by passing activations layer through layer starting from the input, then through hidden layer and finally output. Then the error is calculated in the output layer and propagated backward through the network. In the forward pass, the weight do not change. In the backward pass, the weights change in proportion to the local gradient.	1
336	10	Backpropagation is a neural network based learning algorithm where the network learns by propagating the error through the network. BP consists of two stages:  + Forward pass: where the error is computed by feeding the input to the network.  + Backward pass: where error is propagated through the network for doing the weight updates locally.  Since BP has vanishing gradient problem, it is useful to use activation functions which are infinitely differentiable such as sigmoid function.	1
337	10	The back propagation algorithm is used to calculate the error contribution of each neuron after a batch of data is processed. Required is a known desired output of each input value. Thus the back propagation algorithm is a supervised method. The algorithm can be subdivided into two phases:    1) Propagation:  * Propagation forward through the network to generate the output value(s).  * Calculation of the cost error term.  * Propagation of the output activation back through the network usin the training pattern target in order to generate the deltas (differences between desired and actual output) of all output and hidden neurons / by recursevliy computing the local gradient of each neuron.    2) Weight update:    For each weight the following steps need to be applied:  * The weight's output delta and input activation are multiplied to find the gradient of the weight.  * A ratio (percentage) of the weight's gradient is substracted from the weight. This ration is also referred to as the learning rate and influences the speed and quality of the learning.    Learning is repeated for every new batch until the network performs adequately.	2
338	10	Backpropagation is an algorithm for training a neural network, and it contains of two main stages. The first stage is to compute the actual output given the input; in this stage, the signal flows forward from the input layer to the output layer, and the synaptic weights are fixed. The second stage is to update the synaptic weights by propagating the error signals backward from the output layer in a layer-by-layer manner; for each neuron, the local gradient, the partial derivative of cost function to the local field, is computed. 	2
339	10	Backpropagation is a learning algorithm for Multilayer FF NN. It is supervised error correction learning.   The weights are initialised randomly    The algorithm has to steps:    In the forward pass the the output is calculated by using the current weights.    In the backward pass the weight update for the outputlayer is as like in single layer ff. The error is used to update the weights. BP allows us to also calculate the error of hidden layers. For each hidden layer we use a local gradient as the error. The local gradient is the sum of weighted error of the following layer, which is passed trough the derivate of the activation function. So it is possible to backpropagate the error from the output layer to to first layer.    A common Problem in BP is the vainshing gradient problem. Depending on the activation function used the local gradient gets smaller in each layer until it is eventually less than the floating point precision used. This limits the number of layers that can be stacked.	2
340	10	The back propagation algorithm is a learning algorithm for updating in the weights in a multi-layer neural network. For updating the weights of all the layers, the error of each neuron must be calculated. In the back propagation algorithm, two phases will be defined:  - Forward phase: the output of the neural network will be calculated and also the error of the neurons in the output layer.  - Backward phase: the gradient of each neuron will be calculated, by using the calculated error on the output layer and the defined connections between the hidden layer and the output layer. If multiple hidden layers are defined, the error will be iteratevely will be given backwards and the weights at each neuron will be updated.	1
341	10	Backpropagation is used in Multilayer Perceptrons to give a method of adapting the weights. First the forward phase is run like in a regular feedforward network. Then after the output and thus the error is determined the error is backpropagated from ouput layer through the network. Since we have multiple layers, there is only a desired output of the network for the last layer. To counteract this problem a gradient is calculated for every neuron during the backward pass. The gradient is giving a measure of the contribution of this neuron to the final error. The gradient is then used to update the neurons weights. If the neuron is not part of the output layer, the previous gradients are used to calculate the new gradient instead of using the error.	1
342	10	Back propagation consists of two steps:  1. forward pass - data is passed through the network and weights are atapted  2. backward pass - by using local field of each neuron error signal is propagated backward by using local field of each neuron from end to beginning and stacking them up. Local field is partial derivative of the output signal of a a neuron, for output neuron it is simplest to calculate as it has only desired output and actual output to deal with.	1
343	10	Backpropagation is the general form of the delta rule, formulated for networks with multiple hidden layers. Here, we propagate the error of the network back to the input layer to determine the change of weights, using the error signal in the output layer and subsequently the local gradients in the hidden layers. In the forward pass, we compute the net output forwards. In the backward pass, we propagate the error backwards. The BP rule was derived from the error gradient w.r.t. the weights, and application of the chain rule.	2
344	10	Back propagation is propagation of error from the output layer to the hidden layer in network with multiple layers.     This is done by calculating the local gradient of each node and then using this (along with the weight) to determine how much of the error is to be propagated to the particular node	1
345	10	In back propagation, there are two phases:    1. Forward Phase: First we apply input to the network and compute the current output.   2. Backward Phase: We compute the error between current and desired output. Error is minimized by computing gradient of error with respect to weight. In return, weights are adjust.    After adjusting weights in backward phase, we again go to forward phase and compute the current output, check whether error is minimized or not. 	1
346	10	YOUR ANSWER HERE Back propagation wants to minimize the error function E. E is given by: \( $ \frac{1}{2}\sum e(n)^{2}$  \). THe error function can be minimized by calculating the gradient starting from the output. Term for calculating the gradient differs. It depends on whether the neuron for which the gradient to be calculated is an output neuron or  a hidden neuron.	1
347	10	In backpropagation, the gradient of the error produced at the output layer (by partially differentiating the cost function with respect to the weights) is propogated backwards one layer at a time back to the input layer. This propagated gradient is used to update the weights in the corresponding layer. Backpropagation is necessary because the desired output at every layer is not known and it is only possible to formulate the cost function at the output layer.	1
348	10	Back propagation is used in multi layer network. It consits of two phases: Forward and backward.  In the forward phase we give and input to the network and caculate its outputs. Also memorize the local field of each node. The local gradient (delta) is used to adapt the weights of the layers. It is different for output and the remaining layers.     For node i in an output layer: $\delta_i(v_i) = \varphi^\prime(v_i)(d_i - y_i)$    For node i in other layers: $\delta_i(v_i) = \varphi^\prime(v_i)\sum_{j\in C} w_ji \delta_j(v_j)$, where $C$ are all the nodes that use node i output as an input    repeat this process for all input data until error is small enough	2
349	10	The back propagation algorithim is there to train a mulilayer feedforward ANN. We change the weights by computing the local gradiant at each neuron by using the neurons in the layer befor. The local gradient of the output neurons can be computed easaly. The activation function has to be differantable for the backpropagation algorithm.    In the forwart pass we compute the output y at the output layer.    In the backard pass we use the output y and our desired output d to compute the local gradients at the output layer. Then we go back layer by layer and use the local gradients from before to compute the new local gradients.    By that we minimize the average squared error function.	2
350	10	In multi layer ff networks the error is only available in the last layer. Therefore the error is propagated back through the network using the backpropagtion algorithm. In order to do so the local gradient has to be calculated.   Update of the weight: w+1 = w + n * x * gradient where the iput x is the output of the previous layer.  The local gradient is calculated diffrently depending if the neuron is in the output layer or in the hidden layer.    Output layer: $ gradient = phi`(x) * (y -d) $    Hidden Layer: $ gradient = phi_j`(x) * SUM(w_i * local gradient_i) $    	2
351	10	back propagation is used in multilayer feedforward networks. first the forward pass is computed. The given error at the output nodes is used to compute the weight changes using widrow-hoff learning rule. then the error is given back layer by layer in the backward pass to compute the error and weight changing for each layer recursivly. The learning can be done in sequential (online) or batch mode (offline) 	1
352	10	Back propagation is a learning algorithm for multilayer neural networks. At first, the input is propagated through the network until the end is reached. Here the error is calculated with the desired result. Then the error is used to update the weights from the back to the front. For the output layer the weights can be updated directly with the calculated error. The following layers have to use the local gradient of the previous error, which is calculated with the derivative of the activation function and its error. This is then used to update the weights and repeated until the front is reached.	2
353	10	In steepest gradient weights are adjusted in decreasing direction of error function. But for hidden neurons there is no labels available to calculate the error. Hence final ouput error is backpropogated through the layers inside the hidden layers of NN. This is possible with continuous activation function and chain rule on its derivatives.   Final error is differentiated with respect to hidden weights. Chain rule is applied to find local error on hidden neurons. 	1
354	10	Backpropagation is a neural network which has two stages:  -Forward pass: In forward pass the error is calculated in the output layer with the help of the desired output and the given output. e = d - y  - Backward pass: It begins in the output layer , in this case the error is passed backwards with the calculation of gradients at each layer of the neural network  So in back propagation the adjustment to weights is made based on the local gradients which is calculated at each layer.	2
355	10	Back prop is a way of training a neural network by adapting the weights using error produced. It consists of two phases, forward and backward. Forward phase computes the output along the network using the function signal. In the backward phase, the error of thr outpur fromthe derired output is computed and a local gradient of the error is used to update the weights iof the network. The local gradient considers the credit or blame of the corresponding weights of neuron in producing the output.	2
356	10	Back propagation is a steepest decent method that uses the final produced error and the local gradient to define the amount of change needed for each synaptic weight.    In this method we have two phase:      - forward phase: in this phase we feed the input to the network and the network calculate the output      - backward phase: in this phase we first calculate the error and then use the local gradient to propagate the error to the network from the last layer to the first and manipulate the synaptic weights	2
357	10	Back propagation is moving the error backwards recursively through the network by calculating the local field of every neuron to update the weights. It is based on the chaining rule of derivatives.	1
358	10	The back propagation algorithm is based on the error correction learning rule and consists of two passes:  1. Forward pass : The input signal applied to the source nodes of the network is propagated forwards through the different layers of the network, and the output is computed at the output layer of the network.  2. Backward pass : The error signal computed at the output is propagated backwards, with a local gradient computed at each of the hidden layer neurons, in order to adjust the synaptic weightsof the neuron in the network.	2
359	10	Back propagation consists of two steps:  1. step - Forward pass: Here the input data is fed into the network and the output is calculated at the output nodes. The usual calculations of the induced local field are done by using this formula $v = \sum wx + b$. The output is then calculated using this formula $y = f(v)$, where f() is the activation function.    2. step - Backward pass: Here the error is backpropagated through the network from the output layer to the input layer. In the output layer the error is calculated using this formula $\delta = d - y$, using the desired output d and the actual output y. In the layers before the output layer the local gradient is used to calculate the error using the error from the output layer $\delta = w\delta x$. Additionally the weights are updated using $w_{new} = w_{old} - learning\_rate \cdot \delta x$	2
360	11	if the learning rate is large then the it follows the zizag motion.    if the learning rate is too low then it takes time for converging .    if the learning rate is very large or critcal then it becomes unstable.    while processing there is possiblity that it will get stuck in local minima.	2
361	11	When using steepest descent the learning rate($\eta$) determines the speed at which the weihts are adjusted in the NN. There can be two possible danger related to leraning rate depending on its magnitude:  1. Low learning rate(eg, $\eta = 0.01$)  results in smooth variation of the weights but makes the process becomes slow.  2. Hight learning rate (eg, $\eta = 0.01$) results in faster weight adjustment but it leads to an oscillatory nature in the learning which is unwanted.	2
362	11	YOUR ANSWER HERE    * With a small learning rate the network will converge very slowly towards the optimal weight of the network but it will give better perfomance in generalization.    * With a high learning rate there can be zigzag effect. because of the large rate the network may miss a local minima and jump to a higher point.    * With a very high learning rate the network may become unstable.	2
363	11	The learning rate defines the speed of steepest descent search for min of a error funtion. In other words, it defines how strong the change in weights will be, throughout optimization procedure. Higher learning rate, faster learning, but then learning is characterized by oscilations in searhc for min. This is dangerous because if learning rate, becomes bigger that a certain value, it can make search with steepest descent unstable. IN this case steepest descent will start to diverge, istead of converging to min.   In other case, when learing rate is small that lerning is slower but safer, and the learining path is not oscilatory.	2
364	11	Learning rate is used to decide how fast the network should converge during the training phase    If the learning rate is too high - the system oscillates and  becomes overdamped    too low - the system becomes underdamped and learns very slow	2
365	11	The learning rate tells how long one step in the method of steepest descent is. If the learning rate is too high, the learning will oscillate and may not converge. If the learning rate is too small the convergence will take many iterations.	2
366	11	* Learning rate tells the network that how much steps it should move towards direction opposite to the gradient vector.  * If the learning rate is too large, the weight updation will be high.   * So the danger is, learning may oscillate or the network overfit the data.	2
367	11	Learning rate is used to regulate the speed of learning. If the learning rate is small then the learning is slow and if the learning rate is high then it oscillates. If it exceeds the critical value then the algorithm is unstable.	2
368	11	The learning rate is $\eta$ So based on the learning rate, it undergoes various oscillation.   We could see zigzagging behaviours.   1. When the learning rate is large, the system is said to be under damped.   2. When the learning rate is small, the system is said to be over damped. Here we can see a zigzagging behaviour towards the convergence phase.   3. After the learning rate crosses a certain value it becomes unstable.     It may stuck in a local minima which is considered to be another danger 	2
369	11	Learning rate in steepest descent can directly affect the convergence of the algorithm. If the learning rate is very small then algorithm can take long time to converge i.e response is ovderdamped. But if the learning rate is amde very high then we may observe zig-zagging (oscillatory) behaviour and sometimes algorithm may fail to converge  (underdamped response).	2
370	11	learning rate controls the speed and convergence of steepest descent method. 1. if it is small, the trajectory of weight vector follows a smooth path in W plane; 2. if it is large, the trajectory of weight vector follows a zigzaging path; 3. if it exceeds a critical value, then the algorithm is unstable.	2
371	11	The learning rate tells us how confident we are of the error, and it affect the convergence rate. A low learning rate will slow the convergence, making the system overdamped. A high learning rate will speed the convergence but the value oscilates, making the system underdamped. The system can become unstable if the learning rate is above a threshold value.	2
372	11	+ If the learning rate is too small, then the system is overdamped and the algorithm takes a long time to converge.  + If the learning rate is too large, then the system is underdamped and the algorithm oscillates around and optimal solution or could potentially make the system unstable.	2
373	11	The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. The method of steepest descent starts at a point P_0 and as many times needed moves from P_i to P_(i+1) by minimizing along the line extending from P_i in the direction of gradient f(P_i) the local downhill gradient. The danger of the algorithm is, that it can get stuck in a local minima.	2
374	11	The learning rate determines the rate of learning: the smaller the learing rate is, the slower the learning process is, but the path of weight adjustment is smoother. The larger the value is, the faster the learing process is, but it can result in oscillation and instability. 	2
375	11	The learning reate is a factor of how much we trust the datapoint. Normally it is in the range of [0,1]. A high learning rate normally results in a faster convergence while a lower rate in a slower conversion. If the rate is choosen to high, it is possible that the cost function diverges. If the rate is to slow it is possible that the rate so conversion is so slow that we never reach a local minimum.	2
376	11	The learning rate is a parameter using on updating the weights in a given iteration. This parameter represents the importance that is given to the adaptation of the weights. So when setting the learning rate small, the learning machine will learn slower but also in a more stable way. On the other hand, when setting the learning rate with a large value, the learning machine will learn faster but in an unstable way. The danger here, is that depending on the learning rate's value, the algorithm may never come into the perfect value. If the learning rate is too small, it may land into a local minimum and never approach the global minimum of the function. If the learning rate is too big, the learning progression will have a zig-zagging behaviour and never approach the ideal value.	2
377	11	If we use steepest descent we use the learning rate to adjust the speed of the convergence to a minimum error. If the learning rate is too small, the learning is going on rather slow. If the rate is high, the error is zigzagging on the error surface towards the minimum. If the learning rate is to high, it might not converge but diverge.	2
378	11	If learning rate is to large, then proccess will oscillate a lot and might not converge.    If learning rate is to small, then convergance will happen very slowly	1
379	11	When training with SD, the learning rate determines the step size we take towards the negative gradient. When the learning rate is too small, the weights may be overdamped and reach the error function minimum slowly, eventually getting stuck in local minima. When step size is too big, the weights may be underdampened, bouncing between ridges of the error surface and never find the minimum (especially when the minimum is in a steep ravine of the error surface)	2
380	11	- Learning rate is used to control how much the wright update is affected by the error correction or so on.  - Learning rate too low: Learning is slow and takes more time  - Learning rate too high: Learning is fast, but causes zigzagging behaviour in convergence.  - If the learning rate is too high, it may result in situations where the zigzagging behaviour will cause it to overshoot, and may never finally converge.	2
381	11	If learning rate is very smaller, then transition are over-damping, trajectory of weight vector follows the smooth path.    If learning rate is large, then transition are under-damping, trajectory of weight vector exhibits the zigzagging(or oscillatory) behavior    If learning gets higher than some threshold, then learning algorithm gets unstable or diverges	1
382	11	YOUR ANSWER HERE Learning rate n determines stride of delta of weight. If learning rate is too large weights starts to ziggerate. 	1
383	11	Learning rate controls the speed of the descent. When learning rate is low, the weight updation is overdamped and convergence is slow. When the learning rate is high, the weight updation is underdamped and a zigzagging behaviour is exhibited in the weight space. When the learning rate is too large, learning becomes unstable.	2
384	11	The learning rate defines the speed of the weight change. A learning rate too high can lead to oscillation around the optimal weight such that its never reached. A learning rate to low results in very slow learning and slow convergence.	1
385	11	The learning rate is needet to make the algorithm more stable.     A high learning rate makes the weightchanges zickzacking and the algorithm might not converge    A low learning rate makes the path in the W-plane more smooth.    If the learning rate gets to a certan critical value the algorithm might not converge at all	2
386	11	A too small learing rate can lead to a very slow convergence or to no convergence at all if the time learn becomes too long. A high learning rate can lead to an oscillating behavior and prevent convergence.	2
387	11	The learning rate gives the speed of learning. It defines the stepwidth in direction of steepest descent. If the learning rate is small, the learning is more stable but slower. When it is high, the learning is more unstable but faster. The danger is to overcome a minimum and result in oscillating behaviour 	2
388	11	The learning rate defines the speed of the learning convergence. High values lead to faster learning und low values to slower learning. However, high values can lead to oscillations in the learning space and may overshoot the desired result and never reach it.	2
389	11	Learning rate is a scalar multiplied with adjustment term to adjsut the weights. It ensures the rate of learning. It is typical greater than 0 and less than equal to 1. It govers the rate of sliding alond the curve towards the minima.     1. Lower learning rate will result in slow learning but chances of finding optimal minima are greater.     2. Higher learning will result in hopping on either side of minima hence zigzag behaviour.    3. Very high learning may not converge. 	2
390	11	Learning rate has huge impact on convergence of the network. If the learning rate is low then the transient response of the algorithm is overdamped and the trajectory of w(n) is smooth. If the learning rate is high then the transient response of the algorithm is underdamped and trajectory of the w(n) is zigzag. If we choose the wrong learning rate then  the network might not converge.	2
391	11	The learning rate defines the efficiency of learning machine. If it is small, the system response may be overdamped, if large , the response may be underdamped and if it exceeds a critical value, the response may diverge.    The danger is the possibility of the system output to not converge. This should be ensured by scaling the learning rate using the largest eigen value of the correation matrix of the input.	2
392	11	- The learning rate defines the size of steps that the method moves in the search space.  - If the learning rate is too small the method needs to take huge number of steps and maybe it stuck in a local minima  - If the learning rate is too big the method will converge very fast toward the global minima but there is a probability that it oscilates around the global minima and never reachs it 	2
393	11	Learning rate $\eta$ has a profound impact on the learning in steepest descent.   1. If $\eta$ is too small, the system is underdamped and convergence is slow.   2. For larger $\eta$, the system is overdamped and tends to oscillate.    3. If $\eta$ exceeds a certain critical value, steepest descent may even diverge!	2
394	11	The value of the learning rate parameter $\eta$ controls the speed of descent and convergence towards the optimal weight vector. For small values of $\eta$, the transient response of the algorithm is overdamped and the weight trajectory follows a smooth path. On the other hand if the value of $\eta$  is large, the transient repsonse of the algorithm is underdamped, and the weight trajectory follows an oscillatory path in the W-plane.	2
395	11	The learning rate is a value between 0 and 1, which determines how fast the network learns. When using small values for the learning rate, the network converges slowly and needs alot of processing. When choosing big values the learning oscillates and becomes unstable. The goal is to choose the learning rate in a way that it does not learn to slow, which needs more input data for convergence, and that it does not become unstable.	2
396	12	YOUR ANSWER HERE	0
397	12	the main idea of the RBM is compute the Least mean square error of the difference between expected output and real output.	0
398	12	YOUR ANSWER HERE	0
399	12	YOUR ANSWER HERE    * It is a Recurrent neural netwokr  * It uses two groups of neurons, hidden and visible  * It process the training data by flipping the neurons	1
400	12	It has the structure of recurrent neural network.     It has two layers of neuron visible and hidden.    the neuron can store only binary values    they work based on flipping    theere are modes free running and clamped    the weights are changes based on the correlation of the neurons in the free running mode and clamped mode	1
401	12	In a Reduced Boltzman Machine there are one visible and at least one hidden layer. The visible layer is the input and acts as output at the same time. For each input the neurons of the visible layer will be assigned with a value. With their weights, hidden neurons may either be activated or not. Once the input has been passed through the hidden layers, the values are passed all the way back to the visible layer. For this, different weights are used since the values move in the opposite direction. 	1
402	12	Reduced boltzman machine work based on **flipping operation** and calculating the probability invariances of clamped state and freely running state.	1
403	12	RBMs run on boltzmann learning rule. The neurons have 2 modes of operation clipped and free running.    All the neurons are binary units. Their status can be changed by flipping. All the neurons that are in on position are clipped together.	1
404	12	YOUR ANSWER HERE	0
405	12	RBM is an unsupervised learning technique. It has visible neurons and hidden neurons. Neurons are in either +1 or -1 states. It uses the idea of simuilated annealing to flip the neuron states based on energy function and pseudo temperature. It operates in 2 states - clamped state and free flowing state. In clamped state only hidden neurons are flipped and in free flowing state both visible and hidden neurons are flipped. Weights are adjusted based on avergage correlation difference between all the neurons in clamped and free flowing state.	2
406	12	"The neurons operate in a binary states, ""on"" or ""off"". In clamped condition, all visible neurons are clamped into specific states by the environment; in free running condition, all neurons including visible and hidden neurons operate freely."	1
407	12	The Reduced boltzman machine works by flipping neurons. It can operate in clamped or free running state.  - If two connected neurons are activated at the same time, the weight is increased.  - If any of the two neurons are fired asynchronously, then the weight is reduced or removed.	1
408	12	+ Reduced Boltzman Machines (reduced because inputs do not share information via synapses) are one of the initial NNs which consists of input layer and hidden layer. The system adapts its internal weights and tries to reproduce the inputs.	1
409	12	A RBM is a shallow two layer network containing a visible and a hidden layer. Each noden in the visible layer is connected to each node of the hidden layer. It is considered as restricted, because no two nodes of one layer share a connection. A RBM is the mathematical equivalent of a two way translator. In the forward pass a RBM takes the inputs and translates them to a set of numbers that encode the inputs. In the backward pass it takes the set of numbers and translates them back to form the reconstructed inputs. A well trained RBM will be able to perform the backward translation with a higher degree of accuracy.     Three steps are repeated over and over through the training process:    1) Forward pass.    2) Backward pass.    3) Evaluate quality of reconstruction as visible layer (often solved with KL divergece)	1
410	12	YOUR ANSWER HERE	0
411	12	The structed of RBM is a bitpartied graph. It uses hebbian learning for training and the neurons used are binary stoachastic neurons, which have a binary state, which fire based on a probability. The training is achived by passing the information a many times between the hidden layer and the input layer. There weightsare updated on the pass into the hidden layer. Weigths between input and activations in the hidden layer are increased, weights between gernerated inputs of the rbm and the hidden layer are decreased.	2
412	12	The main idea of an RBM can be defined as follows:    - Two layers will be defined, where each neuron will be connected to every neuron of the other layer.  - The input will be passed from the first layer to the second one, and the state of each neuron of the second layer will be calculated.  - The neurons with active states will pass again its values to the input layer.  - The values given from the second layer will be compared with the input values, and with the two states, the weights will be adjusted.	1
413	12	In RBMs there are two states, the free running and the clamped state. During the clamped state, the input neurons are clamped to the output neurons. While the network is clamped the probabilities of the Hidden states to be in a certain state are calculated to determine a probability of the output to be correct.	1
414	12	They are neural network with only one hidden layer, neurons from input to hidden layer are fully connected, neurons from hidden layer to output layer are fully connected as well. 	0
415	12	RBM implement a combination of graphical and probabalistic ideas, using probabilites of activations inspired from energy based networks. We present a training input to the RBM, and determine the hidden activations based on a probability of net input and edge weights. Then, when unclamping the training data from the network, sample from the distribution of the hidden layer, where the RBM tries to rebuild the distribution of the input data. RBM may be used for data completion or denoising, where e.g. incomplete images are complted based on the learned probability distribution.	2
416	12	RBM has two layers and are interconnected (recurrent) operates by flipping the internal states (+/- 1)> Unlike the boltzmann machine, reduced boltzmann machine does not contain interconnections among the same layer. The weight update is done by the differnce in correleation in clamped and  free running mode.	2
417	12	It is a recurrent network. It opreates by flipping.     It has two groups of neurons: Visible neurons and hidden neurons. Visible neurons provides interaction between environment and network. Hidden neurons are running freely.     It has two modes of operation:       . Clamped State: states of the neurons are clamped.      . Free running state: Neurons are running in free condition	2
418	12	YOUR ANSWER HERE	0
419	12	The reduced blotzman machine works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.	2
420	12	It consists of only two layers: input and hidden layer. During training data is presented to the input. The hidden layer starts oscillating.	0
421	12	The Reduced Boltzman Machine is an stochastical recurrent ANN, that operates with two classes of neurons : hidden and visible. It operates by neuron-flipping with a probability impacted by the neurons arount. So it uses the hebbian rule. An Reduced Boltzman Machine can learn the classify data and can repoduce the learned patterns.	2
422	12	Neurons have to states e.g. on or off. Each neuron has a probability to flip from one state to another. 	0
423	12	the binary state of each neuron is flipped by a given probability. Stochastical learning 	0
424	12	Two fully connected layers, one input and one hidden layer are used. The input layer is the only connection to the environment. The RBM has a specified energy level which can not be changed. However the distribution of this energy to the nodes can be changed. Based on the data input every node has a chance to flip based on its input connections.	1
425	12	YOUR ANSWER HERE	0
426	12	Boltzmann machines is a neural network having recurrent structure.It is in two states either on which is +1 or off which is -1.The energy function is given by     $E = 1/(1+exp(-delta E/Temperature))$    The state of the input x is turned from +1 to -1 based on the change of the energy delta_E and the pseudo temeperature T.	1
427	12	RBMs work on the principle of binary states, free-running or clamped. The weight update is done based on the Botlzmann's formula using the pseudotemperature, which gives the proobability of error.	1
428	12	YOUR ANSWER HERE	0
429	12	A Reduced Boltzmann machine (RBM) consists of two layers of neurons: visible and hidden. The neurons may only have two states i.e. activated or not and they flip according to a certain probability based on the weights and states of other neurons. The RBM has two modes:  1. Clamped: The visible layer is clamped to a certain input while the hidden neurons are allowed to change state until the network settles. The correlation in this state is given by $\rho_{ij}^+$  2. Free-running: In this state, the network is allowed to flip all neurons until it settles. The correlation is $\rho_{ij}^-$    The weight update rule is given by   $$\Delta w_{ij} = \eta (\rho_{ij}^+ - \rho_{ij}^-)$$	2
430	12	The Reduced Boltzman Machines function by using two types of neurons : visible neurons that provide an interface between the environment and he network, an hidden neurons that operate freely. The learning can proceed under two conditions, namely:  1. Clamped state : where the visible neurons are clamped to a particular state of the environment  2. Free running state : where both visible and hidden neurons operate freely.     If $\rho^+_{ij}$ indicates the probability of correlation between the states of neurons i and j in clamped state, $\rho^{-}_{ij}$ indicates the probability of correlation between the states of neurons i and j in free running state, then the weight adjustment $\Delta w_{ij} = \eta (\rho^+_{ij} - \rho^{-}_{ij})$	2
431	12	The Reduced Boltzman Machine hast an input layer and a hidden layer. Each neuron has a state and a probability to turn on. If the neuron turns on the data passes trough it and the weights are updated. The probability of turning on is calculated by the network.	1
432	13	YOUR ANSWER HERE:  - Echo state networks are recurrent neural networks that have a large resorvoir of oscillator functions that are connected to the input layer.  - In FF NNs, consideredthe outputs at the hidden layers are also considered but in ESNs, the ouputs from the reservoir to the final output layer are only considered.	1
433	13	ESN are the RNN recurrent neural network which has at least one feedback cyle.     FF NN are normally forward moving networks where the input from one layer is fed into next layer and generated the output .     but IN ESN the out put is again fed back as input . ESN is tend to have Resvoir where its randomly connected.	0
434	13	YOUR ANSWER HERE    * ESN uses a large set of recurrent neurons called reservior.  * The weight of reservior neurons does not change after initialization.  * The network only lears the weight of reservior to output.  * It works very well for one dimentional time series data    The Feed forward networks works differently. The input is feed through the network layer by layer and error is propaged backward to make the adjustments till the first layer. In case of ESN the adjustment is made to the reservior to output weight only.	2
435	13	YOUR ANSWER HERE	0
436	13	The Echo state netwrork has a large number of recurrent neural network in them. this set of RNN is called the dynamic reservoir.     They can approximate any dynamic model     they train the model by changing only the weights of the connection of output of the dynamic reservoir and output of the network    FF:     they can approximate any continuous function    They train by adapting all the weights in the network	2
437	13	An echo state network contains of an input layer which is connected to a reservoir, which is a big recurrent network. The output layer is connected to the neurons of the reservoir. While learning in an ESN, only the weights between the reservoir and the output layer are changed, no changes within the reservoir.    Differences to feed forwared networks are, that the reservoir is recurrent and that during the training not all weights are changed, but only the ones between ouput layer and reservoir.	1
438	13	* Echo state networks are recurrent neural netwoks with **Dynamic Reservoirs.**  * Weights initialized in the dynamic reservoris will not be updated during training.  * Only the weights in output layer (readout states) is updated after each iteration.  * In FF NN, all neurons are connected with other neurons in next layer and all the weights are updated in each iteration.  * But in ESN, the **neurons are connected randomly** with other neurons and it is **recursive** and the **weights are not updated** in the dynamic reservoir.	2
439	13	ESN is a type of RNN. It has a dynamic reservoir. All the neuron are connected to each other. 	1
440	13	Echo state network provides structure and supervised learning for recurrent neural networks. It mainly 1) Directs the fixed, large reccurent neural netorks by providing an input stimuli and also fix a response signals to the neurons which are present inside the reservoir(Pool of neurons) 2) It can be directed to get the desired response by the trainable linear combiner of the response signals.     Unlike FF NNs, ESN's have memory. They can be also activated without an input stimuli, whereas in case of FF NN, they require a external stimuli so that they are activated. Also the neurons needs to connected in one full cycle. 	2
441	13	Echo State Newtors are type of RNN. It has dynamic reseivoir units which exhibits different dynamics. Weights of these reseivoir units are fixed and are not changed during the training phase. Only the reseivoir to output weights are changed to learn the inputs. These networks converge only if reseivoir units exhibitg echo state property i.e its ouput depends only on the previous inputs. this property is satisfied if spectral norm reseivoir weights is less then 1.	2
442	13	ESN is a kind of Recurrent NN, which has a large, random , fixed RNN called dynamic reservior and only the weights connecting the reservior and output layer are trained. So ESN combine the desired system function and input/output history echo function.	1
443	13	The ESN is a type of neural network model that uses a recurrent neural network as a large, random, fixed dynamic reservoir that remains unchanged during training and only changes the weight of the reservoir to output layer.  	1
444	13	ESNs are a form of recurrent neural networks with a least one recurrent input. The ESNs are reservior computers which have memory and can be activated without the inputs. In ESNs, instead of training we evolve the network state by feeding it input sequence.  ESNs are different from FF NNs because ESNs contains at least one recurrent connection (feedback).	1
445	13	The basic idea of ESNs is to use a large, random, fixed recurrent NN (referred to as dynamical reservoir) and to train only connections from the reservoir to the output.    The main difference to FF NN lies in the recurrent part of the network, where back passes are built in, giving feedback previous layers. It is not possible to maintain the reservoir beforhand so it suits the given problem. There is a lack of investigation of reservoir construction. 	2
446	13	An Echo State Network (ESN) is a modified version of a recurrent network. It has a reservoir, which is a large number of hidden neurons with sparsely-connected random and fixed weights. To train an ESN, only the weights connecting the reservoire and the output layer are adjusted; therefore, the efficiency is better than a normal recurrent network. 	1
447	13	A Echo State Network is a RNN, is has a dynamical reservoir of neurons which are connected with each other and itself. the DR typically consists of more that 100 neurons. The outputlayer consists of linear readouts of the DR. So a neuron in the output layer sums up the weighted behaviours of the DR neurons. The DR is randomly initialised and only the output layer is trained by supevised learning.     The main Diffrence is that ESN is a RNN. In contrast to FFNN it can resemble any dynamical system. Usually it is used for time series prediction.	2
448	13	Echo State Networks are a type of recurrent neural networks, where the input layer is interconnected to a reservoir (a random initialized group of neurons with also random interconnections), and this reservoir is connected to the output. The reservoir will not be adjusted, but the output weights. The output weights can also have recurrent connections with the reservoir. The states on the reservoir neurons will be calculated, and with these states and the output weights, the output will be extracted.     The main difference with the Feed Forward Neural Networks (FF NN) is that in the FF-NNs there's no recurrency, so the input values will be passed to the next layer.	2
449	13	In contrast to regular feedforward networks, ESN belongs to the group of Recurrent Neural Networks. It has a regular input layer like the FF, then comes a dynamic reservoir, which is a layer of neurons, where at least one full cycle of connections between the neurons is given. The connections inside this reservoir are not constrained and can thus be any possible connection. This reservoir is randomly initilaized and kept that way. Only the respective connections to the output layer are trained during the learning process. 	1
450	13	Echo State Networks are recurrent neural network type, meaning there are feedbacks in its structure. It is usually only 1% connected. Main difference is that it has a reservoir as a hidden layer where neurons are very randomly connected, with random weight etc. During learning phase only weight outputing neurons are changed. It is required more that 100 neurons to be in a reservoir.	2
451	13	ESN are different to FFNN in so far that they consist of a reservoir of hidden neurons, which may be connected recurrent, as opposed to having a feed forward architecture. Here, the inputs are connected to the recurrent dynamic reservoir, whereas the DR is connected to the linear output layer. The Output layer may be again connected to the DR, whereas during training only weights of the last layer are learned. Weights of the DR of the ESN are thus initialized and never learning, although since have been extended to minimal complexity architectures.	2
452	13	ESN are recurrent neural networls with a large reservoir (or echo chamber) with many nodes (recurrent). The weights are learnt only for the connection between this reservoir and the output layer. The weights are not learnt for the nodes inside the reservoir. The main idea is that during training, the input layer cuases the states inside the reservoir to behave in caertain way, and the weights in the output layer is adjusted to match this and the labelled output.    FFNN are feed forward networks, i.e., they do not have any recurrent connections, which is the main difference with respect to ESN   	2
453	13	Echo State network are recurrent neural network, which means these networks have feedback. While, in feedforward neural networks, there is no feedback. In feedfoward, training data or inputs are not dependent on each other. They do not have any system memory. In ESNs, training inputs are dependent on each other and they have system memory      In Echo state network, there are fixed, random generate reservoir weights. These weights are not trained. While, only output weights are trained	2
454	13	YOUR ANSWER HERE An ESN is a recurrent neural network with many layers and fixed weights. There are several differences. An ESN has a cycle that means witin the network there are backwarded connections. Withn a FF NN there are only feedforwad connections. Within a FF NN all weights are trained. Within an ESN only output weights are trained. An ESN can produce an output without any input. A FF NN needs an input to produce an output.	2
455	13	Echo State Network is a type of Recurrent Neural Network and has atleat one cyclic (feedback) connection. ESN consists of a dynamic reservoir and a output layer with neurons. The dynamic reservoir consists of randomly initialized neurons with random sturcture and connections (with atleast one feedback connection). The output layer combines the dynamic behaviours of the reservoir in a required fashion. Only the weights of the output neurons are updated while learning.    An ESN consists of feedback connections while a FF NN does not.    An ESN could have persisting activations even when there is no input which is not the case in FF NN.    An ESN can approximate dynamic systems while a FF NN cannot.	2
456	13	ESNs are a special class of recurrent neural networks. In contrast to ff they also allow backward node connections and thus are able to memorize data.  They are defined by: $x_i$ input i, $y_i$ output i, a dynamic resaviour, and weights connecting all the components.  The dynamic resaviour is generated randomly and fixed. Its topology including weights is never changed.  Only the weights between output layer and dynamic resaviour are changed during training. Because the dynamic resaviour allows all kinds of connections between its nodes it can contain memory that is able to remember data. It also has a spectral radius.	2
457	13	An ESN is a recurrent ANN with randome, sparse and fixed interneuron connections in the hidden layers. Just the output layer weights get trained, because the network itself is so complex, it can model very much. If the training was not successful we can just create a new randome ESN. Training an complete ESN would by very complex and would take very very very long.    A FF NN is not recurrent (no feedback) and all its weights get trained and most of the time the interneuron connections are not sparsly.	2
458	13	The core of an ESN is an arbitrary network with recurrence.	0
459	13	echo state networks have dynamical reservoir as hidden layer. The dynamical reservoir consists of recurrent non-linear neurons. Only the linear connections from dynamical reservoir to the output layer are trained. The difference to FF NN is, that the ESN is a recurrent network	1
460	13	An ESN is a recurrent neural network, that consists of an input layer, a dynamical reservoir and an output layer. In the dynamical reservoir feedback loops are possible in contrast to a feedforward network. However, this dynamical reservoir is only randomily initialzed and not learned. Only the connections to the output from the reservoir are learned. Normally, in FF NNs all connections are trained.	1
461	13	Echo state networks have dynamic reservoir with echo state property which is a randomely initialized RNN.   Hence it can maintain its own internal state. Which is not possible in FF NN. RNN have feedback connections which ecoes back the state of reservoir as well as previoulsly applied inputs. Hence it can model dynamic systems which not possible with FFNN. 	1
462	13	ESN refers to echo state networks. Echo state networks are the recurrent neural networks where the hidden to hidden layer weights are selected randomly and are fixed and hidden to output layer weights are changed by the learning process.Since ESN is recurrent neural network hence the output echoes throgh the network even when there is no input where as in ff nets there is no feedback so there is no output if there is no input.	1
463	13	ESN are another implementation of RNNs where training method is completely different. They comprise of a dynamic reservoir with fixed hidden to hidden connections which makes up an RNN with sparse connetivity. Only the output weights which connect the dynamic units and the output of the reservoir are trained using error, unlike RNNs, where the hidden weights are also trained. ESNs are less compuationaly expensive since they can be easiliy trained with experimentation .However, RNNs use much less hidden units compared to ESN for a similar task.	1
464	13	"- In the ESN we have a huge recurrent network which is called ""Dynamic Reservoir(DR)"" and we have an output layer connected to this DR and we will train the network by adapting and manipulating the connection weights just to the output layer  - Unlike a feedforward network in a ESN because of the DR we have at least one loops that returns the output of a neuron with some time delay therefore we have memory in our network but in FF NNs we don't have any memory"	2
465	13	ESNs are recurrent neural networks with at least one cyclic connection and are based on the concept of reservoirs. In contrast FF NNs do not have any cyclic connections. Additionally, in ESN the output weights are trained but the reservoir weights are not whereas in FF NNs all weights are trained. The ESN has memory while FF NNs do not have memory.	2
466	13	An Echo State Network (ESN) is a neural network that uses a reccurent neural network (RNN) as dynamic reservoir which is not changed during training, and trains only the connection from the dynamic reservoir to the output layer. An echo state network is different from FF NNs due to the presence of feedback connection with the dynamic reservoirs which enables it to maintain activation even without inputs. Each unit within the dynamic reservoir in ESNs are excited differently to different inputs.	2
467	13	ESN have an input layer connected to a reservoir, which is a recurrent neural network. The reservoir is connected to the output layer. On the connections to the output layer are weights, which are updated by the network. The weights of the reservoir are chosen randomly and not updated at all.	1
468	14	YOUR ANSWER HERE: Convolutional neural networks have 4 main layers where input layer is connected to convolutional and subsampling layers followed by another set of convolutional and subsampling layers connected to the output layer. They are designed to specifically recognize 2-d shapes are invariant to skewing, rotation and the actual location of the object.	1
469	14	CNN is has multiple layers and they dont use multiplication matrix. 	0
470	14	Convolutional Neural Network(CNN) has three main layers in them  1. Convolutional Layer  2. Pooling or Subsampling Layer  3. Output layer.	1
471	14	YOUR ANSWER HERE	0
472	14	THe CNN will have a     input layer    convolutional layer - Here the convolution and sub sampling of the feature maps take place    Feed Forward - Neural Network layer    Output layer	1
473	14	A convolutional neural network uses the steps of convolution and subsampling alternating in the beginning. Using different kernels during convolution, many feature maps are created. The subsampling step merges the maps to reduce their amount. After some of these steps, a classical feed forward network is in the end to transform the different feature maps to one output layer.	1
474	14	* Input layer  * Convoluton layer (Affine transformation)  * Filtering layer (Sampling)  * Learning layer  * Output layer	1
475	14	CNN has basically four types of layers. They are: convolutional layer, ReLU layer, Pooling layer and the fully connected layer.    We can arrange the convolutional layer and ReLU layer in different ways.    One of the ways is to have 1 convolutional layer, 1 Pooling layer, 1 ReLU layer and repreat this 3 layers again and then finally a fully connected layer.    Another way is to have 1 convolutional layer, 1 pooling layer again repeat the convolutional and pooling layer and then 1 ReLU layer and finally fully connected layer.    Convolutional layer is used to find the feature space.	2
476	14	A CNN   1. starts with a input, where we perform the convolution, which provides a piece of activation.   2. Next it is being sent through the activation layer otherwise known as the detection layer.   3. Then the final stage is the pooling. 	1
477	14	In CNN we have different Kernels which are used for extracting certain properties of the inputs. These are called feature maps. After this there is a detection phase which introduces non-linearity. Further there is pooling which introduces translational invariance. There can be many such layers of feature maps and pooling. Finally its reduced to single row input and trained using traditional methods like Back Propogation algorithms.	2
478	14	1. first stage, the layer performs several convolution parallel to produce a set of linear activation  2. detector stage, each linear activation is run through a non-linear activation  3. third stage, use a pooling function to modify the output of layer.	1
479	14	- The CNN has an input layer  - The input layer is connected to a convolution layer consisting of three phases:      - convolution stage      - Detector stage      - pooling stage  - The next layer (can be a traditional FFNN)	1
480	14	CNNs are feed forward neural networks which replaces matrix multiplication task with convolution operation which is much sparse. CNN contain followng stages:  + Convolution (learns local features)  + max pooling (coarse-graining to learn better abstraction of input image)	1
481	14	In comparison to other NN, in CNN matrix multiplication is replaced with convolution. Everything else remains the same.	0
482	14	An CNN (covolutional neural network) contains a set of hidden layers for feature extration (convolutional layers, pooling layers), and fully-connected layers that classifies the features. The covolutional layers are used to carry out the covolution between the incoming signals with a set of filters, resulting in a set of feature maps. The pooling layers are used to reduce the dimensionality of the feature maps, and make the features invariant of rotation or displacement.	1
483	14	A CNN typically consists of multiple CNN layers and a few fully connected FF Network Layers. I'll assume the fully connected part is not so relevant to this questions.    A CNN layer is typically a convolution layer and a pooling layer    In the convolution layer a kernel is convolved onto the input. If zero padding is used the result is in the same dimensionality. Depeding on the Kernel the convolution can be 1, 2 or 3D.     In the Pooling layer the result of the convolution is reduced to focus ont the importan features. It also helps on translational invariance.	2
484	14	A Convolutional Neural Networks has the following structure:   - The input is defined in a grid, so any image or video sequence will be used.   - A several number of convolutional layers, where also subsampling (pooling) can be used.   - In the convolutional steps a filter will be used for each layer.   - After applying multiple convolutional layers, a normal feed-forward networks can be applied, where for example a back propagation algorithm can be used for updating the weights in the numerous iterations.	1
485	14	A Concolutional neural network has alternating layers of convolution and pooling. The convolutional layer is applying a filter to the input, while the pooling layer sub-samples the input. In some networks this is replaced by strided convolution, which combines these two steps into one. The structure at the end of a CNN is equal to that of a regular feedforward network.	1
486	14	Convolutional neural networks are so that first layer is not fully connected but in a way that neuron connections overlap, leading to a grid type structure with overlapping circles. Another layer is connected only with nodes that are responsible for a particular feature (convolutions), then next layer is choosing wich of those convolutions from each ensemble is the most apropriate, after that next layer is fully connected to output neurons.	1
487	14	CNN learn on grid data (images, 3d volumes) using filters instead of matrix multiplication. Here, the filters are convoluted with the input in the Convolution layer per neuron, where we slide the filter (defined by fiter size $S\times S$) over the input with a stride (step size), and optional zero padding. Strictly speaking, since for RGB images we are working have three color channels, we work with volumes of filters (For example for RGB images of size $32\times 32\times 3$, a filter of window size $S=5$ has the dimensions $5\times5\times3$). Instead of learning a volume of weights for each convolution step, we share weights, considering that one feature detected in one part of the image may be of interest in another part. Then, we apply a nonlinearity, commonly the ReLu activation, as to introduce nonlinearity into our model. To reduce spatial size of our input, we can either use higher strided convolutional layers or pooling layers, for example the popular max pooling layer, where the maximum value over a subvolume is picked. These layers are then stacked, while in the last layers fully connected neurons are typically used to reduce data to for example a classification vector.	2
488	14	A CNN uses convolution instead of matrix multiplication. After this there is a non linearity which may be a function like ReLU. There is also a pooling stage which is used to pool the important features.    CNNs are translation invartiant.	1
489	14	In CNN, there are mainly three layers:        i. Convolution Layer: It is used to capture the low-level and high level features using kernal over the image.            ii. Pooling Layer: It is used for dimensionality reduction, and for translation invariance            iii. Fully Connected Layer: This layer is same as regular NNs, where all the nodes are fully connected with each other. There is mostly sigmoid activation function is used to compute the probabilities of each output/class.            Furthermore, In CNNs, we use Rectified linear unit(ReLU) activation function  	2
490	14	YOUR ANSWER HERE A Convolutional Neural Network has a kernel which is much smaller than the input. This is why it can operate much more efficient than a normal neural network. Normal Neural network O(n \times m), convolutional neural network O ( n $ \times $ k), k is much smaller than m. A convolutional Network operates no large images. The input is preproessed in many layers before it is given to a normal neural network. Preprocessing transforms input into a linear separable problem.	1
491	14	In a Convolutional Neural Network, the layer order is:    1. Convolutional layer (has kernels which convolve over the input image incase of first layer or feature maps otherwise).  2. Activation layer (ReLU activation).  3. Pooling layer (max or average pooling). These 3 layers can be repeated any number of times.  4. Finally one or more fully connected layers followed by softmax layer. 	2
492	14	A convolutional neural network consits of convolutional layers.  A convolutional layer applies one or multiple kernels (matrix) to an input vector/matrix (typically image) instead of connecting all single inputs of the input vector to the next layer with seperate weights.  Instead in training only the kernel is updated.   After a convolutional layer there is typicall a pooling layer.  Given a window size it reduce the dimensional size of the output of the convolutional layer by using e.g. max or avg pooling.  Afterwards the activation layer applies an activation function to the output of the pooling layer.  In the end of a cnn there are typically some fully connected regular layers resulting in a softmax activation function, which assigns the probabilities to the classes output.	2
493	14	An Convalutional neuron network assumes the input is an image. Because of that it has a achitecture, so that there are (abwechselnt) covalution and subsampling layers. After the last subsampling layer there is a normal FF NN which classifys the input.	1
494	14	A CNN conists of one or more convolution layers as well as subsampling or pooling layers followed by a fully connected standard FFN. In the convolutution layer kernels are used to create feature maps. A kernel is smaller matrix that is apllied to all possible positions on the input matrix. In the pooling stage the dimension of the rfeature map is reduced. for example by max pooling. 	1
495	14	Convolutional Neural Network    it has often images or video sequences as input. the input is computed by convolution (with different kernels) and downsampling in many steps to smaller but many more input matrices. In last step the matrices are connected to a classical FF NN.  	1
496	14	A basic CNN can be structured into the three layers convolution, detector and pooling.    In the first layer the convolution operation is performed on the inputs.    In the second layer the the activation function, mostly ReLU, is applied to the result of the convolution.    The last layer can be used to reduce the size of the resulting convoluted images, e.g. by max pooling. 	2
497	14	CNN uses convolutional layers to extract primitive information from pattern.     First data is convolved with the first layer to extract some features.    Output of this layer is passed through RELU function to rectify it.     Then is downsampled by pulling layer. It basicaly chooses only relevant outputs of convolution layer for further processing.     RELU is chosen instead of sigmoid because it doesnt allow gradient to vanish in backpropogation.    	2
498	14	The structure is as follows:  -Convolution: In this layer convolution takes place instead of matrix multiplication  -Deconvolution: In this layer deconvolution takes place , by matrix multiplication  -Average weight layer: This is a max pooling layer 	1
499	14	CNN comprises of multile layers of neurons which perform specific tasks. The initia layer is the convolution layer which performs convolution of the input with the elements of a given kernel. Simpler tasks such as edge detectoon are performed. Detector layer forms a seconf layer here the output of convolution layer if fed through an activation function such as ReLU. Further, the data is pooled in the pooling layers where downsamping is done to reduce dimensionaity. These layers are repeated to perform more complex feature extraction operations.	1
500	14	A CNN network consists of:      - Input layer      - conolution layer      - Detection layer      - Pooling layer      - Next layer(because CNN consists of many layers this will be another block of layers similar to what described)	1
501	14	A CNN consists of several stacked Convolutional layers which can be separated by other layers such as Pooling, Activation, Zero-padding and Dropout which is a form of Regularization. The output layer is generally dependent on the task but could be a Softmax Activation from a Fully connected (also called Densely connected) layer. The number of outputs is usually the number of classes.	2
502	14	A CNN is a neural network that replaces matrix multiplication with a mathematical operation called convolution in one or more layers. The main idea behind the structure of a CNN is to replace the activation of neuron with a flipped filter (Convolution layer) and then apply another function called pooling to adust the output further.	1
503	14	YOUR ANSWER HERE	0
504	15	YOUR ANSWER HERE:  - RBFs are only dependent on the radial distance i.e., distance from the center to the input	0
505	15	data varaince    Features    RBF uses suport vector machine which is classifier. it uses different kernels , it doesnot have feedback cycle.   it also classifies non linear classification problem. it mainly works with 2 classes C1 ,C2.    other NN is can also reggression and there can be feedback (RNN) 	0
506	15	The difference of RBF to other NNS are  1. RBF has only one hidden layer wheras their is no hard limitation on number of hidden layers on other NNs  2. The activation function used in RBF is non linear.	1
507	15	YOUR ANSWER HERE	0
508	15	1. Weigths in the network  2. the center of the clusters  3. variation of the cluster ($\sigma$)    Difference:     RBF always have only three layers    RBF can also trained in an unspervised method    RBF can also approximate any continuous function	1
509	15	- The centroids of the radial basis functions  - The weights of the neurons  - The amount of needed neurons    A difference to other neural networks is that the centroids of the radial basis functions need to be there.	1
510	15	Three items to be learned,  * origin  * center    Pros:  * It can transform data from n dimension to infinity dimension.  * It can solve non linear problems easily.    Cons:  * It may overfit.  * Learning is slow.	1
511	15	Center of the hidden neurons, synaptic weights connecting the neurons and    RBFs have only 1 hidden layer. There is a non0linear tranformation between the inputs and the hidden space and a linear tranformation between the hidden space and the output space.     Pros: It can be used for non-linearly separable data. 	1
512	15	The three items that needs to be learnt are the centers, widths and depth. Compared to other NN they have a standard 3 layer structure. They can have just one hidden layer. 	1
513	15	In RBF first inputs are transformed to higer dimension using non linear transformation. This is based on unsupervised learning. Inputs are then learned using least square estimation which is an supervised learning. RBF is based on Covers theorem which states that there is higher probability that data will be linearly separable in higher dimension. 	0
514	15	1. non-linear transformation function from input space to feature space  2. centers of input data that is used for each hidden neuron  3. synaptic weights connecting hidden layer and output layer	1
515	15	- Use distance to center as argument for computation of local fields.  - Use radial basis functions as activations  - RFBs are only global approximators,   - splitted learning instead of global learning	1
516	15	+ Kernels  + Only neighbourhoods are computed based on distances.  + Radius of neighbourhoods    Pros  + RBF are simple and easy to compute.     Cons  + They remember the data points	1
517	15	Differences are:  * RBFN has a single hidden layer. Nonlinear hidden layer.  * Linear output layer.  * Argument of hidden units: Euclidean norm.   * Universal approximation property. Local approximators.   * Splitted Learning. 	1
518	15	The mean of the k clusters, the 	0
519	15	This question is really unspecific: Difference to other NNs...    - Centers  - Widths  - Weights    The main diffrence is that the RBF uses localized activation functions and it has only one hidden layer.  It applys a non-linear transformation from the input space to the hidden space and a linear transformation from the hidden space into output space.     It is important to use regularization for RBF  RBF work well for interpolation, so it should work good for regression	2
520	15	A Radial Basis Function Network has the following structure:   - An input layer   - A hidden layer, where a non-linear dimensional transformation will be used.   - Each neuron of the hidden layer will have a defined center (extracted in previous steps).   - A linear transformation will be used to the hidden data space, and the output will be calculated.     So, the three items that must be learning in the RBF networks are:   - The centers of each hidden neuron (using for example k-means neighbours algorithm).   - The radial function that will be used for the non-linear transformation.   - The weights applied into the output layer.    	2
521	15	The centers of the clusters, the widhts of the clusters and the weights.   In contrast to other NNs the output only depends on the radial distance to the center of the clusters.	1
522	15		0
523	15	In RBF, we learn the centers of the radial basis functions using unsupervised clustering methods, the weights of the last output layer, and the width of our radial basis functions. As opposed to Multi layer NN, we dont need expensive backpropagation as we only need to train the last layer, while the unsupervised training algorithm does the work the RBF centers. A possbile Con would be that if the RBF centers dont represent the training data point distribution well, some data points may be hard to model.	2
524	15	1. weights  2. centres (or means) of clusters  3. $\sigma$ which is the width of the clusters    Difference: Uses functions which are radially invariant.    Pros:  - Easy to learn  - Non-linearity  - Only dependent on the radial distance    Cons:  - Data required is more  - OVerfitting   	2
525	15	In RBF network, we need to learn **centre** and **width** of gaussian function. We also learn **output weights**    Difference between RBF and NNs:    i. In RBF, there is only one hidden layer, while in NNs, there can be more than one hidden layer    ii. In RBF, activation function of hidden layer is Gaussian so parameters are in euclidean norm. While, in NNs, parameters for activation function are product of weights and inputs.     iii. Parameter computation is different in RBF as compute to other NNs. Like, we compute centre of cluster in RBF with the help of K-means clustering.	2
526	15	YOUR ANSWER HERE RBF network need to learn center of activation function. Differenec to other NN is that there are as many activation functions as data points. One con of Radial Basis Funtion is that due to many activation function RBF networks have a huge computational effort. 	1
527	15	Three items to learn in a RBFN:    1. Centroids of the input clusters.  2. Widths of the clusters.  3. Weights of the synapses connecting the hidden layer and the output layer.    The centroids and widths are learned in an unsupervised fashion while the weights in a supervised fashion. So an RBFN combines unsupervised and supervised learning while a regular NN is completely supervised or completely unsupervised.    Learning is fast and is not so sensitive to the unsupervised part.	2
528	15	An RBF network relies on a clustering algorithm. This can be e.g. k-means clustering.  The three items to be learned:  1. Cluster center  2. Cluster size  3. weights connecting the hidden nodes to the output layer    Difference to other NNs:  - only three layers: input, hidden and output  - each node in the hidden layer uses a different activation function depended on the cluster assigned to it  - only output weights are trained	2
529	15	If a RBF network used a gauss function as the activation fnction these thinks have to be learned:    - centroide $c_i$ (unsupervised)  - sigma (unsupervised)  - weights of the output layer (supervised)    The RBF network is easy learning and not so sesitive to the unsupervised learning part. 	2
530	15	Centroids, width, and parameter of function    The learning of an rbfn is splitted in an unsupervised and a supervised part.     Only one layer, no vanishing gradient.    Pros: easy learning    the unsupervised part is not very sensitive.    Cons:     Difficult to approximate constants  	2
531	15	Centers of the radial basis functions  best model (rbf)  distance of each input pair     pros:  non-linear functions application  ease to compute   using covers theorem     cons:  high-dimensional 	1
532	15	The clusters, the width of the basis function and the weights.    The clusters and the width are learned in an unsupervised fashion. While the weights are learning by a standard supervised steepest descent method.    Pros  RBFs can be very easily trained.  RBFs can achieve better results with less complexity.    Cons  Not as easy to understand	2
533	15	1. Input layer connecting RBF to environment.     2. Hidden layer: nonolinear tranformation of input space to hidden space     3. Output layer: linear tranformation of hidden space to output space.     It is different than other NNs because for learning patterns, it nonlinearly tranforms the input space to higher dimmensional space. Other NNs do not tranform input.     As it tranforms input patterns to high dimmensional nonlinear space, patterns which are not separable in lower dimmensions have greater chance to be separated.     But if we select basis functions equal to datapoints, problem is ill-formulated.    Processing is computationallly heavy.     Regualation becomes problem specific.    Hence, unsupervied learning is employed to clusters data initially. 	1
534	15	In rbf the main advantage is that it follows cover's theorem and the complex pattern classification problem can be solved .	0
535	15	The three parametrs to be learnedin Generalized RBF are 1) cluster centers of the basis functions 2) spread or the width of the basis functions $\sigma$ , and 3) weights of connecting the input and the hidden layers.    RBF are differenent from NNs in different ways.    1) The kernels are localized functions where as NNs are gobablized    2) They use euclidean distance in their activation functions where as NNs use inner products    3) They have a single hidden layer and output is a linear combinaation but NNs compulsarily are not the same.	1
536	15	The three items that must be learned in RBFs are:      - The center of the kernel      - The size(standard deviation) of the kernel      	0
537	15	The three open parameters of an RBF network are:  1. The centers $c_i$  2. The widths $\sigma_i$ and  3. The weights $w_i$    The number of centers $k$ has to be determined by trial and error.	1
538	15	YOUR ANSWER HERE	0
539	15	The weights, the interpolation matrix have to be learned. The RBF maps the input space into a higher dimensional feature space nonlinearly. The feature space is mapped into the output space linearly. The output space is much smaller than the feature space.    Pros: local learning    Cons: feature space can be really large, curse of dimensionality	1
540	16	K-nearest neighbors basically works as follows    1) the they define randomly the cluster points .    2) clacluate the mean of the equlidian distance between the data points. here the points from the previous step acts as centrioids.    3) check the variance of the clusters.     4) repeat 1-2-3 till you get the proper clusters. 	0
541	16	1. Slect random number of neghbourhood initially  2. Find out the input which is nearest to the weight vector using competitive learning  3. Change only the input which wins  4. decrease the size of neighbourhood  5. Repeat	0
542	16	YOUR ANSWER HERE    for x in input_points      neighbours = find_nearest_k_points(x)        for n in neighbours          v = get_vote_of(n)            update_votes_count_for(x,v)          max = get_max_vote_for(x)    	2
543	16	Let L be set of labeled data in memory, L ={x1,x2....x_n}, while x_prime is nearest point to the x_test point, in term of euclidean distance. Let class_of be funtion that return class type if certain data point x. And let K be constant number of neighboring points consired in algorithm search.     Initialize x_prime = {}, L_0 = L, list_of_classes = {}    for j= 1; j<=K; j++ do:           L_j = L_(j-1)/x_prime          x_prime = nearest neighbor to X_test form L_j data          c = class_of(x_prime)          list_of_classes.append(c)       end    c(x_test):=  most frequent class in list_of_classes	2
544	16	Step1 : Randomly select the k centers     Step2 : Cluster the datapoints based on the centers    Step3 : the centroid of the cluster becomes the new mean    Step4 : repeat step 2 and 3 until there is no more evidential cahnge in the network	0
545	16	    input: labeled data set, one unlabeled data point, number k        find the k labeled points, which are closest to the given unlabeled point      from these points, find the label which occurs most often      assign this label to the unlabeled data point	2
546	16	* Choose a value for k  * K represents the number of neighbors  * Get a sample from the input space  * Find the class based on the majority of votes received from the neighbors.   * For example, if the value of k is 3, then let say there are 2 neighbors from class one and 1 neighbor from class two, then the new input sample belongs to class one.	2
547	16	Step1: We randomly place the n neurons.    Step2: For each data point whichever neuron is closer to it, the datapoint is assigned to that neuron.    Step3: Once all the datapoints are assigned, the mean of the datapoints attached to each neuron is calculated and the neuron is shifted to the mean value.    Step4: Step 2 and 3 are done until there is no more shift in the neurons position.    In this way the neurons are adjusted.	0
548	16	L1 - Data set (x1,x2,x3,x_n)  L2 - Storing the dataset based on the number of neighbors.   x_test - Test data set.     So we basically have the k value to be an odd number, so that we can select a majority value.     for i based on the number of l:      x' = xtest - distance from the neighboring neuron i.       L2 = smallest x' in this based on the number of K     x_test = max(L2)    We select the neurons from the neighborhood by calculating the euclidean distance based on weights.   Then if K is 3, we have 3 neurons. So from that we select the label which is fixed maximum on the dataset given in the K-fields.  	2
549	16	define criterial for finding k nearest neighbours <br>  find k nearest neigbours of test input in training dataset <br>  find the class to which most of the neghbours belong <br>  assign that class to the test input <br>	2
550	16	1. identify k classified patterns that lie nearest to the test vector  2. assign the test vector to the class that is most frequently presented to the k nearest neighbors	2
551	16	Initialize k_neighbors = {},     for every neuron     find the nearest neighbor and add it to k_neighbors     Return nearest k_neighbors    	1
552	16	** Pseudo Code **  1. Initiate weights randomly  2. Assign labels to k-inputs that are map neuron is closest to.  3. append all inputs to map neurons using 2.  4. Find centroid of the cluster and move the map neuron to the centroid.  5. Do 2, and 4 until some convergence criteria is reached, e.g. maximum iterations is reached or no updates are performed or net distance is below some specified distance.	0
553	16	Given: L; X_TEST not element of L; k = number of neighbors that will taken into consideration; function class_of()    Set x'={}, L_0=L, Classf={};    for j=1,...k do:        L_{j-1} \ x'; //exclude all the data points which have been identified as nearest neighbors already            x'=find the closest neighbor of X_TEST in L_j; //e.g. compute eucldea distance            c = class_of(x');            Classf=push(c)        set c(x_TEST)= most frequently value in Classf;	2
554	16	Train the knn by storing the data labeled points.    Present a test point.  > Compute the distance between the test point and all the training data points.    > Sort the distance, and choose the k datapoints with smallest distance.    > Determine the class of the test point by majority vote.	2
555	16	Define K centroids, random intialised  assign each data point a class label  while the is no change anymore      for each k           calculate the centroid of the datapoint beloging to that label          for each datapoint               determine the nearest centroid              assign a new class label which belongs to the centroid.	0
556	16	K-nearest neighbors can be seen as an unsupervised learning method, where for a defined number of groups k, the nearest neighbors will be calculated.    1: For a given input data    2: Define value k    3: Get the k points that are closer to the given points.  	0
557	16	1. Get the nearest neighbor of the current x'  2. Remove it from L   3. Get the class of the current x   4. Classify x' as the class that occurs the most often in the neighbors    for 1 to K:        L_i = L/x'            x_nn = min(|x-x'|)            c = getclassof(x_nn)            AmountofClasses.add(c)        setclassof(x') = Max(AmountofClasses)	2
558	16	Firstly identify nearest neighbouring weights  then choose k amount of neighbors and adapt their weights 	0
559	16	YOUR ANSWER HERE	0
560	16	1. get the input  2. find the k- nearest neighbours by finding the distance (euclidean) from the input to all the nodes and selecting the k closest ones  3. Class of the input is the most frequent class in the k-neighbnours found (as such, k needs to be odd number)	2
561	16	i. First we initialize the random points, those points are considered as centroids of clusters    ii. Then, for each new points, we compute euclidean distance, and points closest to centrodis are assigned their respective clusters    iii. We again re-calculate the centroids of clusters    iv. Repeat 2 and 3 untill convergence is achieved, by making sure, no centroids are moving and cost function is minmized 	0
562	16	YOUR ANSWER HERE k-nearest neighbor wants to determine encoder $\C which assigns N inputs to K clusters based on a rule to be defined.	0
563	16	K-nearest neighbors:    1. Take the input data to be classified.  2. Find the first nearest neighbour in terms of euclidean distance.  3. Push the class of this nearest neighbour into a list of labels.  4. Repeat step 2 and 3 for each K which needs to be odd.  5. After all K nearest labels are collected in the list, count the labels in each class.  6. Assign to the input data, the class which as maximum count (majority vote).	2
564	16	$N$ number of clusters.    Given sample data select $N$ different cluster centers by random.    Assign all sample points to the closest cluster    repeat until no further change:   - recalucate the cluster centers   - Assign all sample points to the closest cluster	0
565	16	given a fixed $k$    given a point to classify $new$    given an empty $class$    given a List of all points $L$    from 1 to k do        find nearest point $x$ to $new$ in $L$      add class of neares point $x$ in list $class$      new list L = L without nearest neighboor $x$        class of new = most class in $class$   	2
566	16	training_set := training data    define #clusters    select #clusters datapoints as centroids randomly      for datapoint in training_set:         calculate distance to centroid      lable dataPoint according to closest centroid  end for    iterate over clusters:        calculate centroid	0
567	16	for a given input       compute distances to other input points      pick k nearest neighboors      look at labeling of neighboors      decide labeling (classification) by highest number of neighboors in one class (german: Mehrheitsentscheid)	2
568	16	This learning is based on the memory introduced into the dataset. For each data point the nearest neighbours are found via a distance function.    for each datapoint d        neighbours = get_k_nearest_neighbours_of(d)        d.class = get_most_represented_class(neighbours) 	2
569	16	(K-nearest neighbours is memory based learning.)    take input x     calculate calculate distance of x from each training point.     Select K training points with minimum distce from the data.     Fetch classes of selected K nearest points.     Calculate number points per class in k nearest points.     determine the class C having maximum points in k nearest pioints     The class of the input point is C.	2
570	16	$ L = {x1,x2...x_n} $    $L = L_0$    $x' = {}$    for the input (x,d) :  do {  x_test     }	0
571	16	  Parameters: k -number of clusters, x datapoints , c classes    1) Initialize randomly k centroid of the custers     2) select a data point and compute the set of nearest neighbours of the point using euclidean distances.    3) Find the class that maximum number of neighbours belong to and assign the class to the datapoint.    4) Once the class is assigned, compute the centroid of each cluster or class, considering all the class members.    5) Iterate over all the datapoints and repeat over all points (from step 2) until no update in centroids is required.	0
572	16	    1- randomly define a predefined number of cluster centers(CC)      2- calculate the distance of each datapoint from each CC      3- Each data point belongs to the cluster that has the least distance from its CC      4- Calculate a new CC by getting the average of all the points inside a cluster      5- Go to 2 and repeat this process untill we reach the termination condition	0
573	16	1. Given: Classified data $X$  2. For a new sample $x$:        Determine the $k$ nearest neighbours in X        Output $y$ := majority vote of the class of nearest neighbours	2
574	16	YOUR ANSWER HERE	0
575	16	For the input data x the distance to every other data point is calculated using a distance measure.    Take the k data points, which have the minimum distance to x. These are the k-nearest neighbours.    The most frequent class from the neighbours is assigned as the class of the input data.	2
576	17	YOUR ANSWER HERE	0
577	17	Bias is an proides an affine transformation. and it is treated  a extra inputs. which noramll taken as +1	0
578	17	High bias and variance is desirable in input. Bias Variance Dilemma is the property of input data where if the bias is increased the variance decreases and vice versa. It is difficult to find a tradeoff between them. 	1
579	17	YOUR ANSWER HERE    Bias: Bias means how much the prediction differs from the true value    Variance: Variance means how much the prediction varies for different datasets    The Dilemma is that both generally can not be reduced simultaneously. A learning machine can reduce one at the cost of other.	1
580	17	YOUR ANSWER HERE	0
581	17	Bias : differnce between the estmated output and the actual output    Variance: The range of output of a network for different training set.     Bias and Variance can't be decreased at the same time for many networks. ONly one at a time can be decreased   	1
582	17	NO ANSWER HERE	0
583	17	Bias is used to affine transform of $u$.    It helps to shift the classifier line.    $$v=u+b$$	0
584	17	Bias: How close the estimate is to the true value.    variance: How much does the estimate vary for different training sets.    we always have either hugh variance low bias or low variance high bias.	1
585	17	High value of bias means netowrk is unable to learn the data whereas higher variance means its difficult to learn the training data successfully.	0
586	17	YOUR ANSWER HERE	0
587	17	It is refers to the problem of tryning to mantain a balance between two causes of errors in learning algorithms such that the network is able to generalize data beyong that used for training. Namely the bias error and the variance error. Having a high bias error may cause the network to miss important features in the training data, which leads to underfitting. High variance will make the network to memorize noise present in the training data rather than learning features, which lead to overfitting. 	2
588	17	+ One cannot optimize simultaneously the learning algorithm both for learning maximum variance in the data and learning localization which can be termed as bias.	0
589	17		0
590	17	The Bias Variance Dilemma tells us that the bias (the difference between the actual and desired output) and the variance (output difference between each trial) cannot be decreased at the same time. A complex model results in small variance and larger variance.	1
591	17	You have to to a tradeoff between high bias or high variance. You cannot have both. High vaiance means the model is overfitting the data and therefore the variance on input can be quit hight. High bias means the model is generalization is to unspecific. 	1
592	17	The Bias is defined as the grade of correctness that a learning algorithm will use. The Variance is defined as the grade of flexibility that the algorithm have given a model to learn. When having the Bias high, but the Variance low, the algorithm will not be flexible into data and will discard any data is not exactly the data that fits into the model. On the other hand, when having the variance high but the bias low, the algorithm will be very flexible into the data and will accept any error data as part of the model to learn.	2
593	17	When adapting the parameters of a network we can either have a small bias or a small variance. If we have a small bias the approximation of the network is close to the real one, but the variance between trials is very high. If we have a low variance, the bias can't be minimized and the network has a bigger error between the approximation and the real value. 	1
594	17	Bias Variance dilemma is coming from the fact that you can not have both at the same time. Your network can not be equally great at outputing with extremely high accuracy extremely hight amount of variables. Therefore you need to find balance between the two that suits needs of your neural network.	0
595	17	When training a model on a limited training data set, we must decide wether we accept a biased model which makes assumptions about the test data, but has a better performance on the train data, or a model with more variance which might model the entirety of the data better but be prone to data noise. Usually we have to decide on a trade off between the two, where we may select well balanced models based on VC dimensions or Cross validation results. 	2
596	17	Bias and variance are both undesirable to the learning. Bias defines how far the generated output differs from the true value. Variance defines how much the o/p change on changing the input dataset. However, in most cases, it is only possible to decrease one at the expesne of other. Thus, it is called  Bias Variance Dilemma.	1
597	17	Bias Variance dilemma is used to analyse the generalization error of the algorithm.   If the value of Bias is very high, then network does not learn relations between features and outputs correctly(overfitting)  If the value of variance is very high, then network may model the random noise, and it does not learn intended ouputs(underfitting)    We have to to tradeoff between Bias and Variance so that our model can generalize properly	1
598	17	YOUR ANSWER HERE	0
599	17	In machine learning, a choice always needs to be made for the tradeoff between bias and variance. Bias determines how close the result is to the true value and variance determines the sensitivity to flutuations in the training dataset. If bias is reduced variance increases and vice versa. So an optimum tradeoff needs to be chosen which presents a dilemma.	1
600	17	YOUR ANSWER HERE	0
601	17	The bias is the error we make in the assumption by creating the learning machine (how much we we are away from the actual truth)    the variance is how much the learning machine changes with different training data sets.    if we have a high bias we habe a low variance and if we habe a low variance we habe a high bias   	1
602	17	Usualy only one of Bias and Variance can be minimized. In an RBFN for example    few kernels with greater width leads to a high bias but a low variance. If you choose many kernels with smaller width the bias is low but the variance is high. Higher complexity models need more training data.	2
603	17	YOUR ANSWER HERE	0
604	17	Ideally bias and variance would be 0 after learning a machine. However, bias and variance counteract eachother: when bias decreases, variance rises and respectively in the other direction. This leads to the dilemma that either one of the values has to be present.	1
605	17	YOUR ANSWER HERE	0
606	17	Bias variance dilemma refers to the problem of minimizing the two sources of error bias errror and variance error simultaneously which creates probblem in generaliztion of the network.  Bias error: It is the error that occurs while setting the parameters of the network  variance error:It refers to how sensetive the network is to the fluctuations in the dataset.	2
607	17	Bias and variances are the estimation errors.    Bias corresponds to the inability of the learning machine to appropriately approximate the function to be learnt. Hence this induces a deviation from the actual function    Variance is the inadequacy of the training data to allow the a learning machine to succesfully learn the function.     The dilemma is that , to completely learn the actual function( to reduce variance-related error), the training data required should consist of infinite samples. However, this resuts in slower convergence, inturn, bias error increases.    Therefore trade of between both the errors need to be made.	2
608	17	- Bias: the bias is the differnce between the predicted value and the desired value in the generalization run  - Variance: is the inadequity in the produced value in the regression and the desired value that we expect from the network	0
609	17	Bias is the difference between the predicted and true value. Variance is the range of several predicted values of the same datapoint. It is desirable to have low bias and low variance to ensure the predicted value is consistently close to the true value. The Bias Variance dilemma is that to achieve low bias, the variance becomes high and vice versa. Hence, there is always a tradeoff between the two.	2
610	17	YOUR ANSWER HERE	0
611	17	YOUR ANSWER HERE	0
612	18	[--] this    it is unsupervised     winning neuron takes it all    uppdating the weights of neuron also effect the local neihbour hood of that neuron.    Its competitve nature    cooperation    adapatation.	2
613	18	YOUR ANSWER HERE      The main properties of SOM are,    * Compition: To find the winning neuron  * Cooperation: To find the neighbours of the winning neuron to make weight adjustments  * Adoptation: Change the distance for the neighbourhood function $\sigma$ and learning rate $\eta$    $\sigma_n = \sigma_0 * e^{\frac{-n}{T_1}}$    $\eta_n = \eta_0 * e^{\frac{-n}{T_2}}$    $T_1$ and $T_2$ are constants	2
614	18	- The input layer is fully connected to the map layer  - There is a distance function for all neurons in the map defined  - Within the map competitve learning is used to find the best neuron for each input  - A neighborhood size needs to be defined so that the weights of all neurons within the neighborhood of the winning neuron are changed towards the input  - The neighborhood size and the learning rate should be reduced gradually  - The goal is that similar input should have near winning neurons	2
615	18	1. it has two layers including input layer and map layer  2. based on competitive learning, only winning neuron and its neighbors weight vectors are updated  3. Weight vector of winning neuron is most similar to input data  4. adjacent neurons tend to have similar weights  5. provides a good approximation of input data	2
616	18	+ SOMs are unsupervised learning algorithms.  + Self-organization of weights  + learns topological mapping between the input neurons.  + Modelled after visual cortex of the brain.	1
617	18	SOM stands for Self Organizing Map. SOMs are a NN model for unsupervised learning, which combine competitive learning principles with a topological structuring of neurons such that adjacent neurons tend to have similar weight vectors.	2
618	18	A SOM learns a task in an unsupervised manner. The neurons of the inital network are arranged in a topological order	1
619	18	SOM use unsupervised learning for mostly pattern recognition problems. The structure of a SOM ist:  - An input layer  - A map layer that can be 1, 2 or multiple dimensional (although in general 2 or 3D map layers are used).  - Each input neuron is connected to each map layer connection    The learning goes as follows:  - The distance (euclidean) is used to calculate the winner neuron, where the winner will be the one that have the most similar weights with the input data.  - The winner and a defined neighborhood will be updated. The other neurons' weights remain the same as the previous iteration.    The strategy of the neighborhood size is to start with a large number of neurons and reducing the number through the iterations. The same strategy is applied to the learning rate (big value and reducing it through the iterations).	2
620	18	Self organising maps are able to produce classes for data on their own, without supervision. Use 3 main steps (competition, cooperation, adaptation). Structure of a hidden layer is fully connected (grid style for 2d, lettuce style in 3d). Works great for travelling saleseman problem.	1
621	18	SOMs are unsupervised learning methods, expressing properties of input patterns in their spatial location and their strenght in activation in that point. Just like the human brain, different areas of the SOM map are thus specialized for different input features, which then can be visualzed in a map.	1
622	18	- Unsupervised  - Competitive learning  - Topological structure of the hidden nodes  - There are three layers:      - Input layer      - Hidden layer (which may be 1-D, 2-D or 3-D map)      - Output layer  - Weights in the same neighbourhood maps to similar features. For example in case of the travelling salesman problem, nearby nodes may refer to being nearby in euclidean distance.  - Three stages of learning:      - Competion      - cooperation      - weight adaptation 	2
623	18	[---]    SOMs are neural network that uses unsupervised learning by modelling the structure hidden in the data. There is no any additional information about labellings.     It is combination of competitive learning and topological neurons such that adjusted neurons have similar weight vectors.     There are three main processes:    i. Competitive: Neurons compete among themselves, each time only one neuron is activated. We determine this, by computing euclidean distance between input vector and weight vector of all neurons in map layer. Neuron that has minimum euclidean distance to the input vector is considered as winner.      ii. Cooperation: In this, we take the neighborhood neurons of the winning neuron    iii. Weight Updation: Here, we adjust the weights of neibhorhood neurons	2
624	18	YOUR ANSWER HERE A SOM consists of a grid within neurons are located. Another important propertie is neighborhood function. Neighborhood function decreases over time and when distance from winning neuron increases. So only weights of neurons close to winning neurons are strengthened. Moreover in the first step of SOM weights are randomly initialized.	1
625	18	- number of nodes  - neightbour function (what are the neightbours of node i), if gaussian $\sigma$  - distance function (distance between nodes)  - learning rate  - optinal: modifier for learning rate and neightbour function $\sigma$	0
626	18	A Self Organizing Map is an ANN for unsupervised learning, there we want to find patterns in the given dataset. We use competetive learning and adjacent neurons have simulare weights.   	1
627	18	SOMs have one hidden layer where the neurons are topologically connected to their neighbours in a 1/2/3D grid like fashion.    SOMs learn in an unsupervised fashion via winner takes all competitive learning.	2
628	18	1. Unsupervised learning algorithm    2. Clusterers data into natural group     3. Competitive learning rule    4. Three phases : Competition, cooperation, organization   	2
629	18	SOM is supervised learnign algorithm and finds spatial patterns in the data without external help.  It has two layers:  Input layer: which is connected to all the networks in the layer  Output layer: which is 1 dim, 2 dim or 3 dim map of neighbouhood relations.  It has three properties:  Competition:It takes between the neurons to find the best match  Co-operation: The winning neuron is in the center of the topological neighborhood of co-operating neurons  Adaptation:  ordering phase: $eta_0 = 0.1$    $h_ji(x) = high$  convergence phase: $eta_0 = 0.01$    $h_ji(x) = contains only few neighbours$	2
630	18	[--]    Properties:    1) input space should be comprised of continuous arbitrary vectors    2) The ouput space should be a topologicaly ordered discrte output features    3) The learning rate and width are time dependant    4) Neighbourhood function is be defined by euclidean disctance and is translationaly invariant 	1
631	18	- A fully connected input layer  - A 1 or 2 or 3 dimensional output layer  - A predefined method for defining the selected neurons neighborhood(such as a gaussian distribution)  - A method for learning( changing weights of the selected and neighbors weights)	2
632	18	1. It is an unsupervised learning algorithm  2. It is based on the concept of competitive learning  3. The network has a topological structure representative of the distribution of data in input space	2
633	18	[--]   1. A SOM is neural network model for unsupervised learning that uses a competetive learning principle to adust the weight vectors in such a way that the adjacent neurons have similar weights.   2. SOMs usually start with with a large neighbourhood that usually consists of almost all the neurons in the network, and shrinks gradually during the learing process.  3. The learning process causes a topological arrangement of neigbourhood neurons that resembles the input training examples.	2
634	19	[--] This    1) Error correction learning rule    2) Hebbian Learning rule    3) Widrow-Hoff learning rule    4) Cost minimization learning rule    5) Boltzman Learning rule   	2
635	19	Learning rules:  1. Memory based learning.  2. Error based learning  3. Hebbian Learning  4. Competitive Learning  5. Boltzman Learning	2
636	19	YOUR ANSWER HERE    The learning rules are,  * Error correction  * Hebbian learning  * Compititive learning  * Boltzman learning	1
637	19	1. Error based learning rule  2. Memory based learning rule  3. Hebbian learing   4. Bolzmann learning  5. Competitive learninng 	2
638	19	Single layer learning :    1. Error correction  2. Competitive learning  3. memory based learning  4. boltzmann model  5. Hebbs rule    Learning as optimization:    1. Gradient descent   2. Newton  3. Gauss newton  4. Least mean Square   	2
639	19	- Error correction rule  - Memory based learning  - Hebbian learning  - Boltzmann learning  - Competitive learning	2
640	19	* Error correction (Least mean square)  * Memory based learning (K nearest neighbors)  * Competitve learning  * Hebbian learning  * Boltzman machine learning	2
641	19	There are 5 learning rules:  1. Delta rule or Widdrow Hoff's rule  2. Hebbian rule  3. Competitive learning  4. Memory based learning  5. Boltzmann learning	2
642	19	1. Error correction learning.   2. Hebbian learning.   3. Memory based learning.   4. Competitve learning.   5. Boltzmann learning. 	2
643	19	* Error-correcrtion learning  * Memory based learning  * Hebbian learning  * Competitive learning  * Botltzmann learning	2
644	19	1.Error correction learning  2.Memory-based learning  3.Hebbian learning  4.Competitive learning  5.Boltzmann learning	2
645	19	- Error correction learning rule  - Memory based learning  - Hebbian learning  - Competitive learning  - Boltzman learning	2
646	19	* Error Correction  * Memory Based Learning  * Hebbian Learning  * Competitive Learning  * Boltzmann Learning	2
647	19	1. Error correction learning rule    2. Memory-based learing rule    3. Hebbian learning rule    4. Competition learing rule    5. Boltzmann learning rule	2
648	19	- memory based learning  - boltzmann learning  - error correction learning  - hebbian learing  - competitive learning	2
649	19	- Error-correction learning rule  - Hebbian rule  - Regression learning rule  - Memory based learning rule  - Supervised and unsupervised learnung rules	2
650	19	1. Error-correction learning  2. Memory based learning  3. Hebbian learning  4. Competitive learning  5. Boltzman learning	2
651	19	Boltzmann learning, competitive learning, error correction learning, supervised learning, unsupervised learning	1
652	19	Error correction learning, Hebbian learning, competetive learning, memory based learning	1
653	19	1. Error correction (or Widrow hoff or delta rule)  2. Memory based learning  3. Hebbian learning  4. Boltzmann learning  5. Competitive  learning	2
654	19	[---]    There are five learning rules:    i. Error correction learning rule    ii. Memory based learning    iii. Hebbian learning     iv. Competitive learning    v. Boltzman learning 	2
655	19	YOUR ANSWER HERE Hebb Learing rule, positive error rule, negative error rule, Widrow-Hoff learning rule	1
656	19	**Answer 1:**    Learning rules:    1. Error correction learning.  2. Memory based learning.  3. Hebbian learning.  4. Competetive learning.  5. Boltzmann learning.	2
657	19	- similar classes activate similar regions of neurons  - different classes activate different regions of neurons  - important classes activate a large number of neurons  - unimportant classes activate a small number of neurons	0
658	19	- error correction learning  - memory based learning  - hebbian learning  - competetive learning  - boltzman learning	2
659	19	Widrow-Hoff    Hebbian Learning    Memmory Based learning  (k-means for exapmle)    bolzmann learning 	1
660	19	1) similar inputs should result in similar outputs  2) different inputs should result in widely different outputs of the NN  3) many neurons should be involved by an important feature  4) prior information should be built in the architecture	0
661	19	- Error correction learning  - Memory based learning  - Competitive learning  - Hebbian learning  - Boltzmann learning	2
662	19	1. Supervised learning    2. Unsupervised learning    3. Reinforcement learning 	2
663	19	The learning rules are:  -Error correction learning    -Memory based learnig    -Hebbian learning    -Competitive learning    -Boltzmann learning 	2
664	19	[--]    Error correction rule    Memory based rule    Competitive learning    Boltzman learning    Hebbian learning	2
665	19	- Delta rule  - Hebbian rule  - Competetive rule  - Boltzman rule  - Memory based rule	2
666	19	1. Error correction (delta) learning  2. Memory based learning  3. Competitive learning  4. Boltzmann learning  5. Hebbian learning	2
667	19	[--]     1. Error- Correction learning rule  2. Memory based learing rule  3. Hebbian Learning rule  4. Competitive learning rule  5. Boltzmann Learing rule	2
668	19	Error-correction learning, competetive learning, hebbian learning, boltzman learning, statistical learning.	2
669	20	YOUR ANSWER HERE:    - Sigmoid functions are type of activation functions which are strictly increasing.  - Sigmoid function is given by     $\psi (x) = \frac{1}{1 + \exp (-x)}$    - Logistic function is an example for sigmoid function 	1
670	20	[--] this    Signmoid funtion is stricly positve and differentail. it is used as activion funtion . it given by    1/(1+exp(-av).    sigmiod funtion is used in Multiperecetron learning . 	1
671	20	$\phi(V) = \frac{1}{1-exp(-a*V)}$, where V is local field, and $a$ is constant parameter that define shape of sigmoid function. This function is used in for example in backpropagation method for MLP NNs. There, along original sigmoind function, derivatiive of this function is also used: $\theta(V)^{'} = \theta(V)*(1-a*\theta(v)$ 	1
672	20	$f(x) = \frac{1}{1 - e^{- a * x}}$    $a$ is a parameter of the sigmoid function. The output of the function will always be between 0 and 1. The parameter determines the steepness of the function.    Two examples:    $f(x) = \frac{1}{1 - e^{- 2 * x}}$    $f(x) = \frac{1}{1- e^{- 3 * x}}$	1
673	20	* Sigmoid function is a strictly increasing function that exhibits a graceful balance between linear and non-linear behaviour.  * Importantly it is diffentiable  * Two types,  * Logistic function  * Tangent function	2
674	20	Sigmoid function is a combination of linear and non linear behavior. they are strictly increasing functions.    $\phi(v) = \exp(\frac{1}{1+av})$    Logistic function is an example of sigmoid function.    $\phi(v) = log(\frac{\exp(v) + \exp(-v)}{\exp(v) - \exp(-v)})$       	2
675	20	Sigmoid function are an activation function which is non linear.   They are in the form ${{1}/{1+e^{-av}}$. They are continuously differentiable. They are used in back propogation as well as in radial basis function 	1
676	20	It is a strictly increasing function which has a balance between linear and non-linear behaviour. e.g. logistic function $h (v) = 1/(1+exp(-av)) $, another is tanh function.	2
677	20	the sigmoid function is an s shaped real value function that incorporates linear and nonlinear behavior. the two examples of sigmoid functions are the logistic function and the hyperbolic tangent.    $\phi(v) = \frac{1}{1 - exp(-av)}$    $phi(v) = tanh(v)$	2
678	20	Sigmoid function is given by:  $$ \sigma(x) = \frac{1}{1+e^{-x}}$$    + Sigmoid is strictly positive and infinitely differentiable. Thus is used as activation function in artificial neurons $$ \sigma(\sum_i w_i x_i) = \frac{1}{1+e^{-\sum_i w_i x_i}}$$    + Sigmoid maps input values to $[0,1]$ and thus is used to normalize the input such that they can be used as probability measures.	1
679	20	In general a sigmoid function is real valued, monotonic, and differentiable having a non-negative first derivative, which bell-shaped. A sigmoid function is constrained by a pair of horizontal asymptotes as x goes to either plus or minus infinity.	1
680	20	The sigmoid function is   $$ \phi(x) = \frac{1}{1 + e^{-ax}} $$   	1
681	20	sigmoid functions are non-linear functions which are s shaped.   - 1/1+exp^-av  - tanh()	1
682	20	Sigmoid functions are a type of functions used normally as activation functions in Neural Networks that have a non-linear behavior, making them more powerfull. Two examples can be:    $\phi = \frac{1}{1 - e^{x}}$      $\phi = tanh(x)$	2
683	20	Sigmoid functions are able to produce very well ballanced output in terms of linearity and non-linearity.    For example:  $\frac{1}{1+(exp(-av)}$, where v is local field, a constant    and tangent hyperbolic	1
684	20	Sigmoid functions (in NN) are activation functions which have an S shape, saturating towards their minimum and maximum values. Two examples are the sigmoid and the tanh activation functions. Considering that function values converge against 1 or 0 or -1 in the above examples, many activations & multiplications lead to the vanishing or exploding gradient problem in deep neural nets, for which the ReLu performs better.	1
685	20	Sigmoid functions are differntiable squashing functions with exponential of hyperbolic behaviour dependinf on the input.    Examples:  - $\phi(v)={\frac{1}{1-e^{-av}}}$  - $\phi(v)=tanh(v)=\frac{e^v-e^{-v}}{e^v+e^{-v}}$	2
686	20	[---]    Sigmoid function is strictly increasing function that exhibits the behavior of linear and non-linear.    Examples:     1 . Sigmoid = $ 1 / 1 + exp(-av)$   ,  v is local field and a is constant parameter       2 . Tangent hyperbolic = $$ e^{-av} - e^{av}/ e^{-av} + e^{av}   $$	1
687	20	YOUR ANSWER HERE $f(x) = exp(1/a*x)$. a denotes slope of f, if a is made infinitive large f take only value 1 and 0. One example of sigmoid function is Mc Culloch Pitch (slope is $\frac{1}{2}$ neuron operates only in linear region of activation function )	0
688	20	$sigmoid(v) = \frac{1}{1-e^{-av}}$    $sigmoid(10000) = 1$, a = 1    $sigmoid(-10000) = 0$, a = 1	0
689	20	A sigmoid function is a function that increases monoton and has a graceful balance between linear and nonlinear behavior.       example 1:  $f(x)=\frac{1}{1+ e^{-a*x}}$    example 2:  hyperbolic tanges $\tanh(x)$	2
690	20	Sigmoid functions are one type of activation functions. The range is between 0 and 1. The are smooth and continous differentiable. Examples are logistic function and tangent hyperbolic (range from -1 to 1).	2
691	20	Sigmoid functions are S-shaped functions that converge to a value on either side. In the middle the function jumps from the one value to the other with a slope based on additional parameters.    - hyperbolic tangent tanh(x)  - sigmoid function 1/(1-exp(x))	2
692	20	Sigmoid function is strictly increasing nonlinear function which shows a unique balance between linear and nonlinear properties.     e.g.    1. Logistic function     2. Hyperbolic tangent function  $\frac{e^{-ax} - e^{ax}}{e^{-ax} + e^{ax}}$	2
693	20	Sigmoid function is given by  $phi(v) = 1/1+exp(-a*v))$	1
694	20	Sigmoid function is the most common function being used as the activation function in the NN and one reason for this is it is derivable which is very important for the Back propagation method.  $$\frac{1}{1-e^{av}}$$    The examples are:  - Sigmoid function as mentioned   - $tan^{-1}$	2
695	20	Sigmoid functions are monotonically increasing, differentiable functions with a smooth transition between linear and saturated states. Examples are  1. Sigmoid $\varphi(v) = \frac{1}{e^{-av}}$  2. Hyperbolic tangent $\varphi(v) = tanh(v)$	2
696	20	[--]     A sigmoid is defined as strictly increasing function that describes a graceful balance between linar and non-linear behaviour and is given by : $\phi(v) = \frac{1}{1+exp(-av)}$    Another example of a non-linear activation function is the tangent hyperbolic function which is defined as : $\phi(v) = \frac{exp(v) - exp(-v)}{exp(v) + exp(-v)}$	2
697	20	$sign(x) = \frac{1}{1 + exp(-ax)}$    $tanh(x) = \frac{exp(-ax) - exp(x)}{exp(ax) - exp(x)}$	1
698	21	[--] this    Single layer feed foward : they consist single layer of output neurons. it very simple architecture    Multi- layer feed forward: it consist feedforward multple Hidden layer of neurons . it computionally little expensive than single layer.    Recurrent : it is multi-layer but it has the feedback which output of the neuron fed back as input. 	2
699	21	Architectures of NN:    1. Single Layer Perceptron Network  2. Multi Layer Perceptron Network  3. Recurrent Network	2
700	21	YOUR ANSWER HERE    * Single layer feed forward network  * Multi layer feed forward network  * Recurrent neural network	2
701	21	1. Single layer feedforward NN  2. Multy layer feedforward NN  3. Recurrent NN	2
702	21	1. Single layer - has only one hidden layer. eg: perceptron   2. Multiple layer - has multiple hidden layer . eg: convolutional networks, multi layer perceptron  3. recurrent - The nodes are connected to themselves with a unit function $Z^{-1}$	2
703	21	- Single layer feed forward  - Multi layer feed forward  - Recurrent netowrks	2
704	21	* Single layer feed forward architecture  * Multi layered feed forward architecture  * Recurrent architecture	2
705	21	There are three classes:  1. Single feedforward neural network  2. Multilayer feedforward neural network  3. Recurrent neural network	2
706	21	The three architectures:     1. Feed forward. Single layer  2. Feed back. Multi layer  3. Recurrent neural networks. 	2
707	21	* Single layer feedforward network  * Multilayer feedforward network  * Recurrent neural networks  * Convolutional neural networks	2
708	21	1. single layer feedforward  2. multilayer feedforward  3. Recurrent Neuron network	2
709	21	- Single layer network  - Multiple layer network  - Recurrent network	2
710	21	+ Single layer NN : input-output direct mapping  + Multilayer NN: 1 or more hidden layers  + Recurrent NN: at least one feedback connection	2
711	21	* Single-Layer Feedfoward Networks  * Multilayer Feedforward Networks  * Recurrent Networks	2
712	21	1. Single layer feedforward    2. Mult-layer feedforward    3. Recurrent	2
713	21	- Sigle layer feed forward NN  - Multi Layer feed forward NN  - Recurrent NN	2
714	21	- Single layer neural network  - Multiple-layer neural network  - Recursive neural network	2
715	21	1. Single layer feedforward  2. Multi layer feedforward  3. Recurrent Networks	2
716	21	Architecture of neural networks are:    1.Single layer networks (only input and output layers)    2.Multi-layer networks (have at least one hidden layers)    3.Recurrent (have feedbacks in their structure)	2
717	21	Feed forward NN, Recurrent NN	1
718	21	- Supervised  - Unsupervised	1
719	21	[---]    There are three different classes of architectures:    i. Single layer neural network    ii. Multiple layer neural network    iii. Recurrent neural network	2
720	21	**Answer 2:**    Three classes of NN architecture:    1. Single layer feedforward network.  2. Multi layer feedforward network.  3. Recurrent network.	2
721	21	- Feedforward NNs: Basically a directed acyclic graph. No backward connections between layers are allowed. Also means the network is stateless and has no memory    - Radial basis function networks    - Self ordering maps  - Recurrent NNs: all kinds of connections are allowed, has memory    - Echo state networks    - Convolutional neural networks    - Reduced Bolzmann machines	2
722	21	the architectures are:    -single layer feedforward    -multy layer feedforward    -recurrent neural networks 	2
723	21	single layer feedforward networks  multi layer feedforward networks  recurrent networks	2
724	21	- Single Layer Feedforward NNs      - Single layer perceptron      - Radial basis functions      - Support vector machines  - Multi Layer Feedforward NNs      - Multi layer perceptron  - Recurrent NNs      - ESN	2
725	21	1. Single layer feedforward NN    2. Multilayer feedforward NN    3. Recurrent NN 	2
726	21	The classes are:  Single layer feedforward nn,Multilayer feedforward and recurrent neural networks 	2
727	21	Two main classes:      - Feed forward network      - Recurrent network	1
728	21	1. Single layer Feed Forward (FF)  2. Multilayer FF  3. Recurrent networks	2
729	21	[--]    Architectures of NNs can fall into the following classes:  1. Single layer feed forward networks  2. Multilayer feed forward networks  3. Recurrent networks	2
730	21	Single layer feedforward networks, multilayer feedforward networks,recurrent neural networks.	2
731	22	[--]this    means that when some neuron weights are changed then wieghts of the neurons in the its local neibhorhood gets changed as well.	2
732	22	YOUR ANSWER HERE    In case of weight sharing all the neurons in a layer uses the same weight value.	1
733	22	All the neurons are connected with the same value of wieght to the next layer of neuron. By this the number learning parameters can drastically reduced. They are mostly used in convolutional neural networks. 	2
734	22	Weight sharing means that the same weight is used for some neurons. This saves computation time and memory and makes the network simpler.	2
735	22	Weight sharing is basically sharing of weight by all the neurons in a particular layer. If the weight changes in one layer, it should change in the other associated layers. So by this number of free parameters that are being used is reduced. 	2
736	22	Weight sharing is a way of putting constraints on weights adjustment. Here same weights are used across different inputs. This is a common technique used in CNN. Same kernel weights are used across all the inputs to generate feature maps.	2
737	22	The weight vector of each neuron is same .	1
738	22	It refers to the use of the same sat of weight for a neuron is a layer in order to reduce the number of free parameters.	2
739	22	If all hidden neurons share the same set of weights for their synaptic connections then we talk about weight sharing.	1
740	22	It is a way to constrain the structure of a network, such that adjacent neurons have the same weights. It can imporve the efficiency of the network learning.	2
741	22	Using the same weight for multiple neuron connections. Prominent example: Convolution of Kernels in CNN	1
742	22	Weight sharing is connection type, where the connections (number and value) between two layers will be the same for each neuron.	2
743	22	Usually every neuron will have different weights for the connection with the previous layer, weight sharing is a method, by which the same set of weights is used for every neuron to connect it to the previous layer. This reduces the number of network parameters that have to be adejusted.	2
744	22	Weight sharing is when neurons are getting same weights from neurons that are inputs for them.	1
745	22	As seen in CNNs, weight sharing is used to both reduce computational complexity, as well as introduce a kind of regularization. As weights are shared for example between spatial locations of a model of a retina, the algorithm is more robust to overfitting.	2
746	22	It referes to sharing of same weights between different neurons (in the same layer)	1
747	22	YOUR ANSWER HERE Weight sharing means that several inputs share a weight. So informations stored in a weight can be shared among inputs.	1
748	22	If the weights connecting two layers are not seperate for each node combination but instead are used for multiple edges. This reduces number of weights to be trained and thus can reduce time needed for training.	1
749	22	Weight sharing is used for example in CNNs. Some interneuron-connection-groups share the same weights and if one is changed(trained) all change in the same why. This simplifys the network and the training.	2
750	22	Weight sharing means that diffrent neurons share some or all of there weights with other neurons. In CNNs weight sharing is used in the convolution layer by using a kernel.	1
751	22	Weight sharing is used in convolutional NNs. Different convolutional kernels are applied to the input data first. After that we have to compute the weights for each resulting Layer. To reduce computational complexity, the weights are computed and trained once and shared to all layers. 	2
752	22	[--]    Weight sharing is sharing of weight parameters among different layers of NN. Shared weights correspond to similarity in the network. They can be used to incorporate prior information or increase the ability to differenctiate between the classes using the evident variances.    Having similar weight patterns will reduce the number of training samples required, will improve the convergence of the algorithm and hence, improve the performance.	2
753	22	Weight sharing is a technique to include prior knowledge in the network architecture. In this, the neurons in a layer share the same weights. It has the additional advantageous side effect of decreasing the number of parameters.	2
754	22	When multiple neurons share the same weight it is called weight sharing. When the shared weight is updated it is updated for every neuron, which uses the weight.	2
755	23	[--]this    Hebbs rule says.     if the two neurons are excited simultansouly then the sysnaptics weights between those neuron gets stronger.    if the two neuron not get excited togther or if the excited saperate then the sysnaptic weight between those two neuron tends to get weeker (reduced).	2
756	23	Hebb's rule;  1. If the neurons on both side of a synapse are activated simultaniously (synchronously) the synaptic strength is strenghtend  2. If neurons on both sides of a synapse are activated asynchronously then the synaptic strength is weakend.  The basic idea of Hebb's rule is neurons that wire together fire together	2
757	23	YOUR ANSWER HERE      If two neurons connected by a synapse is activated synchronously then the wight of the synapse is selectively increased.    If two neurons connected by a synapse is activated asynchronously then the wight of the synapse is selectively decreased.	2
758	23	In simple words, Hebbian rule defines that neurons that fire together fire together. In more details: when two neuros that are connected with synapse, are active at the same time (sychronosly), their sinaptic weight is becoming stronger. On another side, then two neurons, that are connected with synapse, are not active at the same time (asychronosly), their sinaptic weight is becoming weaker.	2
759	23	  THe neurons that wire together fire together.    Two neurons on either side of the synaptic weight get activated synchronously, then the strength of the synpatic weight is increased selectively.    Similarly when they get activate asynchronously, then the weight is selectively decreased or not considered.	2
760	23	"""Neurons which fire together wire together""    The weight between two neurons is increased if both show the same behavior for a given input."	2
761	23	In the network, neurons fire together wire together.    * If two neurons are activated simultaneously, then the synaptic links between those neurons are strengthend.  * If two neurons are activated asynchronously, then the synaptic links between those neurons are weakend or eliminated.	2
762	23	Hebb's rule:    1. If 2 Neurons on the either side of the connection are activated synchronously then the strength of the synapse is increased.  2. If 2 neurons on the either side of the connection are activated asynchronously then the synapse is weakend or vanished.	2
763	23	Hebbs rule states that.     1. It 2 neurons on the either sides of the synaptic links are activated simultenously, then the stength of the synapses increases.    2. It 2 neurons on the either sides of the synaptic links are activated asynchronously, then the stength of the synapses decreases.	2
764	23	Hebbs rule states that if 2 neuron are fired together then synaptic connections between the 2 should be strengthened, wheras if 2 neurins are fired asynchronously then synaptic ocnnections between the 2 should be weakened. <br>    $\nabla W_{ij}$ = $n F(y_{i}, x_{j})$	2
765	23	Two neurons on either side of a connection are activated simultaneously then the strenths of the connection is increased. Two neurons on either side of a connection are not activated simultaneously then the strenths of the connection is weakened or eliminated. so weight update is based on $w_{kj} (n+1) = w_{kj} (n) + \eta y_k(n) x_j(n)$, $\eta$ is learning rate and $y_k(n)$ is output signal at time step n, $x_j(n)$ is input signal at time step n.	2
766	23	when axon of cell A is close to fire axon of cell B, and consistently participates in firing it, a special mechanism should take place such that, the weights of A as the neuron that fires B, should be increased. In shot, neurons that fire together, wire together.	1
767	23	+ In short, neurons that fire together wire together.  + If firing of neuron A is causally related to the firing of neuron B, the the synaptic strength between A and B is increased.  + In ML, we use symmetric version of Hebb's rule and reduce synaptic strength of neurons that are not causally related to each other.	2
768	23	The Hebbs Rule describes how couplin between neurons (synaptic value) influences and is influenced by the operation performed by a group of neurons. The Hebb Rule is often summarized with the phrase: What wires together, fires togehter.	1
769	23	The Hebb's learning rule is defined as follows   - When the connection between to neurons is synchronously (if both neurons are active at the same time step), the weight of this connection will be increased.   - When the connection between to neurons is asynchronously (one is active, the other is not), the weight of this connection will be decreased.	2
770	23	Cells that wire together, fire together. Means that the connection between neurons that are activated synchronously is strengthend. There is also the symmetric Hebb's rule, which overcomes the saturation problems with Hebb's rule, by decreasing the connection strength between asynchronously activated neurons.	2
771	23	According to Hebb's rule:  - Neurons that wire together fire together    That means, weight is adapted depending on whether or not the neurons that are connected are fired (or not fired) at the same time.	1
772	23	[---]    Neurons that wire together, fire together    If two neurons are activated synchronously, then strength between two neurons is increased    If two neurons are activated asynchronously, then strength between two neurons is weakend	2
773	23	YOUR ANSWER HERE Neurons which fire together wire together. When neurons are activated synchronously their synase are strengthened when neuron are acivated asynchronously their synapses are weakened.	2
774	23	**Answer 3:**    Hebb's rule states that if two connected neurons activate synchronously, then the strength of the synaptic weights between them is increased. This eventually could lead to saturation. Thus, the rule was modified and a new condition was added which states that if two connected neurons activate asynchronously, then the strength of the synaptic weights connecting them is diminished or eliminated.    Simple form: $\Delta w = \eta \cdot y_k(n)\cdot x_j(n) $    Covariance form: $\Delta w = \eta \cdot (y_k(n) - y_m) \cdot (x_j(n) - x_m)$    where $y_m$ and $x_m$ are mean activations over time.	2
775	23	That fires together , wires together      for example $w_{ij}= learningrate *(x_i*y_j)$    if two neurons of each end of one synapse fire synchonly every time the synapse weight between them is increased. 	2
776	23	w+1 = w + n * x * y, where x is the input and y the output, n learning rate.  The idea of hebbs rule is that neurons that fire together wire to gether. If the input and the output are both positive or negativ the weight for that neron is increased. If they have diffrent signs the weight is decreased.	2
777	23	"it followes the rule of ""what fires together, wire together""  if 2 neurons are reacting similar on a stimulus, then increase their synaptic weigth.   If 2 neurons are reactiong different, than eigther keep the weight or decrese "	1
778	23	When two connected neurons are activated simultaneously their synapse strength is increased. When they are fired asynchronously the connection is weakened.    This means that connections that are used together more often are strenghtened, simulating the function of the brain.	2
779	23	If cell A is near enough to excite cell b and repeatedly takes part in firing by B, then some growth or metabollic change takes place in one or both cells such that A's efficiency in firing B is increased.     This rule can be interpreted as neurons those fire together wire together.     Synaptic connection betwwen the neurons which are fired together are strngthened because they are possibly representing the same feature in the data.	1
780	23	When the axon of cell A is near enough to excite cell B  and repeatatively takes part in firing it , then some metabolic change takes place in one or both the neurons such that the efficiency of A firing B is increased.	1
781	23	[--]    Hebb's rule describes the biological learning process of influence of the closeness of two axons on the strength of the synapse. It states that if the two axons have less proximity, then the tendency to fire together increases, and hence the efficacy of one axon to fire the other. Neurons that fire together , wire together.     It also states that is the neurons do not fire simultaneously, then the strength of the synapse decreases.    This way, the weights of the connections between neurons can be influenced by the surrounding neurons	2
782	23	Neurons that fire together, wire together      This means that the synaptic weight between two neurons defines the importance of that connection, if the connection has a big weight both neurons react to the input and they represent a common feature.	1
783	23	The original Hebb's rule states that is two connected neurons activate at the same time, their synaptic weight should be selectively increased. However, this may lead to saturation leading to the introduction of an additional rule. The synaptic weights of neurons that fire asynchronously are selectively reduced or eliminated. This can be summarized by 'neurons that fire together, wire together'	2
784	23	[--]    Hebb's rule is a two part rule that is stated as follows:  1. If two neurons on the opposite side of a synpase are activated synchronously, the synapse in question is successively strengthened.  2. If two neurons on the opposite side of a synpase are activated asynchronously, the synapse in question is successively weakened.    Such as synapse is termed as hebbian synapse. In simplest form, the Hebb's rule can be stated as fllows in matematical terms: $\Delta w_{kj} = \eta x_j(n)y_k(n)$ where $x_j(n)$ and $y_k(n)$ are the presynaptic and postsynaptic signals to neuron k, and $\Delta w_{kj}$ is the weight adjustment applied to the neuron k. Thus a positive correlation between the presynaptic and postsynaptic signals strengthen the synapse, while a negative correlation between the presynaptic and postsynaptic signals weakens the synapse.	2
785	23	Hebbs rule says that neurons which fire together wire together. This means that when neuron i is activated and neuron j, which is connected to neuron i, is also activated, the connection between those two neuron has to be increased, which means the weight has to be updated. If they are activated asynchronously the weight is decreased. Depending on how the rule is implemented, the strenght can be increases or decreased when both neurons are not fired simultaneously. $w_{new} = w_{old} - learning\_rate(x - y)$, where x and y are the values of the two neurons. It is also possible to use the mean of the variance $w_{new} = w_{old} - learning\_rate((x - x_{mean})(y - y_{mean}))$	2
786	25	YOUR ANSWER HERE   - Pattern Association - Given a set of labelled patterns, a distorted or incomplete pattern should be associated with the correct pattern.  - Pattern Recognition: It is the problem of labelling a a test pattern into one of the classes from the labelled data.  - Function approximation: It is problem of finding a approximate function based on the given input-output samples.  - Control - Controlling a system in an optimal way  - Filter - It is the problem of extracting the particular region of interest especially when there is noise in the data.  - Beam forming: It is a problem of spatial filtering where the interest region is to be extracted in the presence of noisy data as well as it's own background noise.	0
787	25	[--] this    Machine learning proplems     1) the high dimensionality of the data. if the dimensionaly of input space is increased the computioinal cost gets hight exponentionally.    2) if the learning algorithm is trained very much then it ends up with the memorising the data. that often termed as overfiting problem .  	1
788	25	1. The similar inputs from similar class should produce similar representation inside the network    2. The different inputs should produce widely different representation inside the network    3. More number of neurons should be allocated for imporatant features of the input data	0
789	25	* Overfitting.  * Slow convergence rate.  * Oscilations.  * Noise in input data.  * Bad choise of hyper parameters.  * Hard to find how many number of training samples required.  * Which network structure and learning algorithm should be used. 	2
790	25	Some of the principle problems in machine learning are curse of dimensionality and overfitting.     1. Curse of dimensionlity: When the number of features increases, it is very hard to find a pattern and the performance becomes poorer.     2. Overfitting: When the neural network learns excess of input output sample, it ends up memorsing the training sample. 	1
791	25	main problem is generalization from training data to test data avoiding overfitting. Overfitting means the error on the training set is driven to a small value, but when new test data is presented to learning machine, the error is large. That means The network only memorized the training examples, not learned to generalize to new situations.	1
792	25	Generalization: when the model is not able to correctly label new data .  Overfitting: When the model has memorized the training data and is not able to generalize.  Model complexity	1
793	25	* Computational Costs	0
794	25	Curse of dimensionality  Vanishing gradient problem 	1
795	25	Overfitting: The Network memoroizes the data and is not able to generalize    Curse of Dimensionality: The needed Training data increases with the number of dimensions of the data    Bias-variance dilemma: explained in Q. 17    Enough data for training: related to the curse of dimensionality	2
796	25	- Overfitting on limited training data, where for exampel a classifier performs worse on test data because it made assumptions on the training data.    - Closely related to the first, sparsity of (LABELED) training data in high dimensional space (Images, videos etc.). Obviously it is hard to cover the whole problem space with a limited amount of data, thus underlining the importance of a unbiased trainign set.    - Curse of dimensionality, also related to the first two.    - Growing hardware+parallel computation requiremetns when working with image and video data, especially made more accessible the introduction of CUDA based libraries.	2
797	25	- Overfitting  - Generalization  - Number of layers/neurons to use  - Vanishing gradient	2
798	25	YOUR ANSWER HERE One principle problem is noisy data ( data which have more information than needed , these extra information have to be removed ), overfiting of neural network ( overfitting = neural network learns training data and it is not able anymore to calculate a similar input - output mapping for similar input ( generalize ) )	1
799	25	**Answer 5:**    Some problems in machine learning:    1. Bias-variance dilemma- choosing a tradeoff between bias and variance as increasing one reduces the other and vice verse.  2. Overfitting - learner memorizing the training data and failing to generalize.  3. Curse of dimensionality: With increase in dimension of the input data, the amount of data grows exponentially.	2
800	25	- Data gathering: in order to train a network you need a dataset. The larger the network the larger the dataset.  - Time needed to train. Large networks require a large amount of time to train.  - Topology: Which topology to choose? Which performs the best?  - Blackbox: The networks are basically a blackbox. We don't really know whats going on in there. Even a small change in input can lead to a large change in output.	2
801	25	Training data might be hard to come by. For supervised learning labled data is required.    Complex data might require higher complexity models for the learning machine. This means computation becomes more expensive, also more complex models require more training data.   	1
802	25	pattern association. Detect feature on input data and do for example classification    regression. Find a function, that fits the data well, but does not have to fit each point (not interpolation)    controling. Use the sensory data to produce controling signals (e.g. in robotics)    filtering. Filter the input data 	0
803	25	The principle problems are overfitting and curse of dimensionality.Overfitting is the production of an analysis that corresponds too closely to the training data, so it cannot add new data and fails to make prediction of future.    Curse of dimensionality refers to the various phenomena that comes into picture while organizing and analyzing data in higher dimensions.More features reduce the performance.	2
804	25	- Face recognition  - OCR(Optical Character Recognition)  - Time series prediction  - Dynamic system modeling	0
805	25	[--]  Pattern recognition: The input data contains patterns, like a picture with specific features on it. The neural network needs to learn the patterns and recognize it in new input data.    Classification: The input data is seperated into classes. The network learns the input data and has to classify new input data, where the class labels are missing, to the specified classes.    Function approximation: The network learns to approximate a function. When unknown data is presented to the network it has to predict the output, which the function would produce.	0
806	26	[--] this    overfitting can slplved by     3-way splits method     hold out method    cross- validation    k-random sampling     k-fold sampling     leave one out method.	2
807	26	YOUR ANSWER HERE    Over fitting can be handled by finding the best learning machin using one of the approaches    * Cross validation  * AIC  * BIC  * SRMVC	2
808	26	One way to remedy over-fitting is to do structural risk minimization. Namely, to perform tradeof between compexity(size) of NN and number of training examples. The idea is to reduce size of NN(number of neurons) as long as the error in training remains minimal. With this idea, this problem is having to complex model of NN for certain number of experiments is minimized.  	2
809	26	1. By removing the complexity of the problem    2. By increasing the number of weights    3. By doing cross validation during the training phase 	1
810	26	* Cross validation  * Nomalization of input data (Whitening)  * Generalization of data  * Carefully choose the values of hyperparameters  * Keep the learning rate small.	2
811	26	Over fitting can be removed by reducing the training samples. The training data should be in such a way that it can show all the features.	0
812	26	1. Remedy for overfitting would be regularisation   2. In CNN using pooling we can reduce overfitting. 	1
813	26	1. add noise to input data  2. simplify structure of network	1
814	26	- Early stopping  - Regularization	1
815	26	Over-fitting can be remedied by several ways:  + Dropout: is known to make NN robust against overfitting. It works by randomly dropping some weights so that NN learns better generalization.  + Separate data in three parts: training, validation and test. Compare diffrent trained model by evaluating them on validation data. Select best model to test on test data.	1
816	26	A network that is overfitted does not generaliye well. One method for improving network generalization is to use a network that is just large enought to provide an adequate fit. The larger network you use, the more comlex the functions the network can create.    If the number of parameters in the network is much smaler than the total number of points in the training set, then there is little or no chance of overfitting. If you can easily collect more data and increase the size of the training set, then there is not need for further techniques to prevent overfitting. If this is not the case then techniques called regularization (modification of the performance function) and early stopping (available data is divided into three subsets: trainig, validation and test set; stopping runs at error minimums) can be applied.	1
817	26	We can use more examples to traing the network, or reduce the dimensionality of the model.	0
818	26	Over-fitting can be remedied by reducing amount of training data, as it causes the network to learn the noise.	0
819	26	Using a holdout validation set to stop training when overfitting, i.e. early stopping. Oversampling underrepresetned classes. Using regularization techniques such as weight sharing, dropconnect, dropout.	2
820	26	- Introduce a momentum term or an added bias to refrain the network from learning the data along with it's noise.	1
821	26	[---]    Over-fitting is the problem due to which network cannot generalize properly.   In order to remedy this problem, **regularization** is used, which reduces the complexity of the network	1
822	26	YOUR ANSWER HERE	0
823	26	**Answer 6:**    To reduce overfitting:    1. Reduce the complexity of the learner by simplyfying its structure.  2. Use cross validation to determine whether the learner generalizes.  3. Provide additional diverse inputs.	2
824	26	- Split data into train and test dataset  - Add noise to training data  - Stop training when error change gets very low and network starts to learn the actual training dataset	1
825	26	Overfitting cannot happen if the learning machine has a the right amount of power (Structural risk minimisation/ cross validation) and the training phase isen't so long, that the learing machine learns the training set.	0
826	26	Regularization: Modified Error function that punishes high complexity therefor the learned function is more smooth.	1
827	26	By decrease the number of training samples or decrease the complexity (VC-dim).  Also use better training samples:      1) Use training samples, which result in a higher training error      2) User training samples, that a widely different from all previous	1
828	26	Overfitting to the dataset can be counteracted by reducing the complexity of the machine. Less complex machines can not fit the data as accurately and may lead to better generalization.     By increasing the number of training samples a more general model of the underlying data can be learned.	1
829	26	Overfitting causes because NN learns memorizes each point in training datasets. So it cannot generalize the testing points especially those near the boundry.     This mainly happens due to use of NN with high expressive power on for training dataset with low number of data.     Remedy is to decrease the parameters in the NN to reduce its power. Or use high number of training datapoints rich enough to sufficiently express undelying principle. 	2
830	26	We can remedy overfitting by cross-validation.If we divide the training data into training data  for training the algorithm, validation data for fine tuning of the network and test data for generalization.	1
831	26	[--]    1) Cross validation  2) Bias invariance , reducing errors in training samples  3) Decreasing the number of training samples  4) Taking precautions of the complexity of the task and the architecture of the network.	1
832	26	Overfitting can be avoided by splitting the input data into training data, validation data and test data. This way the network only learns from the training data so it has no chance of memorizing the data, because a big part of the data is unknown to the network.	0
833	27	[--]this    the main idea of the  covers theorm says that    if the dimensionality of the input space higher than the the dimensionality of the output space . then the probablilty of the linearly saperating is closer to one.	2
834	27	Cover's therom states that the classifiaction problem present in certain dimension can be countered by projecting the the bianry input in higher dimension. It is very useful in RBF and SVM	1
835	27	Covers theorem states that for a pattern classification problem, given data casting in a higher dimension feature space is more likely to be linealry seperable than casting in a lower dimension feature space	1
836	27	Covers theorem states that, if a set of input data is not lineraly separable, then one can **apply non linear transformation** to the data to project the data in **higher dimensional space** where it can be separable by a linear classifier.    It is used in **Support vector machines** and using the **kernal function** SVM transforms data in to higher dimensional space and classify them using linear classifer with widest margin. 	2
837	27	Cover's theorem states that if data is not linearly separable in dimension C then it might be separable if we increase the dimension.     This is be used in RBFs. 	1
838	27	A complex pattern classification task, cast in higher dimensional space non linearly, can be classified very easily compared to that of a lower dimensional space.     One place where they use is RBFN. They perform non linear transformation of the data from the input space which is non seperable into a space where it can be linearly seperable. 	2
839	27	Covers theorem states that there is higher propability that data will lineary separated in higher dimension compared to that in lower dimension. It is used in tecniques like SVM to first transform the data in higher dimension using non-linear transformation and then learn the data.	2
840	27	Cover's theorem: a complex pattern classification problem cast in a high dimensional space nonlinearly is more likely to be linearly separable than in a low dimensional space.    it is used in RBF network for pattern classification problem, like XOR problem. Transforming nonlinearly from input data to a high dimensional hidden space using radial-basis function, then it is separable linearly in the hidden space.	2
841	27	The covers theorem tell us that it is more likely to separate nonlinear data in higher dimensional space than lower dimensional space. It is commonly used in SVMs or RBFs to classify lower dimensional data by elevating it to higher dimensional space where the inherent characteristics of the data is not changed. 	2
842	27	Suppose data sets C_1 and C_2 are linearly seperable. The perceptron convergence algorithm convergeses after n_0 iterations, with n_0 < n_max on training set C_1 U C_2.	0
843	27	Cover's theorem tells us that a complex classification problem is more likely to be solved if the data points are cast nonlinearly into a high dimensional feature space than in a lower dimensional space. It is used in RBF networks and SVM to classify nonlinear separable data points by transforming the data into a higher dimensional space.	2
844	27	Covers theorem is about the phi seperability of patterns. It makes the statement that when the dimension of our family of kernel functions is grater than the number of patterns in the input the probability of seperability is closer to 1. We used it to motivate why RBF networks exists and whay the work. Powell then formulated     $$ F(x)= \sum w*phi(||xi-x||)$$	2
845	27	The Cover's theorem declares that if a non-linear multi-dimensional transformation is applied into the input space, the probability that the resulted space is separable is close to 1. The cover's theorem is used for example in the RBFN, where the input space is non-linearly transformed into a hidden space by the radial basis functions. Also it's used in SVMs when using a kernel.	2
846	27	Cover's theorem is used in SVMs in the form of Kernel functions. It says that a problem casted into a higher dimensional feature space non-linearly is more probable to be linearly separable than in lower dimension. 	2
847	27	Covers theorem tells us that, when applying a nonlinear transformation on a dataset which is not linearly seperable, and thus transforming it into a higher dimensional representation, the chance of the dataset being linearly seperable is higher. This is used in RBF, where we transform the input patterns to higher dimensions using the radial basis functions and then learn the linear combination of them in the last layer.	2
848	27	According to covers theorem, if we have a feature space of m1, and if the dimension of data, m0 is less than m1, the probability of classification tends to 1.     That is, if the feature space (or number of nodes in the hidden neuron) is more than or equal to the dimension of the dataset, it is more likely to suceed.    It is used to estimate the number of neurons that needs to be present in the hidden layer	2
849	27	[---]    We mainly use for non-linear separability. This is normally used in radial basis function networks    Given a family of non-linear functions($m_{0}$), if the dimensions of hidden space(or number of features) are greater than number of training examples, then probability of separability is closer to 1.    It casts complex features in higher dimension non-linearity are linearly separable than lower dimension.  	2
850	27	YOUR ANSWER HERE: A non-linear input is more probably to be linear separable in a higher dimensional space. To solve XOR Problem Cover's idea is used.	1
851	27	**Answer 7:**    Cover's theorem states that if data is cast on to a higher dimension non linearly, the probability of it being linearly seperable is higher than that in the lower dimension. When cast on the higher dimension non linearly, probability of being linearly seperable is more close to 1.    It is used in Radial Basis Function Network (RBFN) where the hidden layer produces many feature spaces from the input data. The number of feature spaces is as many as the number of hidden neurons.	2
852	27	Covers theroem says if data is linearly transformed into a highdimensional space there are more likly saperatable then befor. We use that information if we want so saperates data that is not linearly saperatable in low dimensinal space. (radial basis functions) 	1
853	27	Covers theorem tells us, that if non linarly seperable data is cast via a nonlinear function to a high-dimensional space (called feature space), it is more likely to be linearly seperable in the highdimensional space.     it is used in SVMs, when using radial basis functions. Via the kernel of radial basis functions the data is transformed nonlinearly to a higher dimensional space 	2
854	27	By casting a non linearly seperable problem into a higher dimensional space the probability of linear seperability increases. It is used in RBF networks to seperate non linearly seperable data in a higher dimension.	2
855	27	Covers theorem states that chances of linearly separating data which is linearly non-seaparable in low dimmensions are higher if  nonlinearly transformed to higher dimmensions .      This concept is used in clssifiaction using RBF by transforming data into nonlinear high dimmensions and then linearly separate them. 	2
856	27	Cover's theorem states that complex pattern classificaton problem cast in high dimension non linearly is more likely to be linearly separable  than in lower dimension.It is used to make sure that the data is converted to higher dimension to make it linearly separable in RBF and SVM.	2
857	27	[--]    The main idea of Cover's theoem is that , as the dimensionality of the data increases, thhe data tends to become linearly separable. Thefore, it can be separated in finite number of iterations in higher dimensiona space.    This is used when the data is not linearly separable in lower dimension. Hence, the data is mapped to hifgher dimensions using kernel functions such as RBF	2
858	27	If the dimension of the hidden layer is bigger than the dimension of the input data the probability that the hidden layer can find a model that fits the data is close to 1. This means that for solving a problem we need a network that has more dimensions than our problems data.	1
859	27	Cover's theorem states that the probability of a data set being linearly separable is greater if it is projected to higher dimensions. It is used as the basis for RBFs where the basis functions are used to enhance the dimension of the data.   $$P(N, m_1) = {\frac{1}{2]}^N \sum_{m_1}^{N-1} (N-1 m_1)$$	2
860	27	[--]  The main idea of Cover's theorem in conceptual terms is that a complex pattern classification problem cast into a high dimensional space is more likely to be linearly separable compared to a low dimensional space, provided the input space is not densely occupied. Given N data points $\{x_1,x_2...x_n\}$, and dichotomy $C^1, C^2$ of the dataset, then from Cover's theorem, the given data points are $\phi$-separable with a probability of 1, if the dimenion of the input space $m_0$ $<$ dimensionality of the hidden space $m_1$	1
861	27	Cover's theorem says that the propability of a complex task to be linearly separable is higher in a high dimensional space than in a low dimensional space. This is used in RBF networks, where the input data is cast into a high dimensional feature space to make the problem linearly separable.	2
862	28	YOUR ANSWER HERE: Kernel functions are the functions that are used to transform the the feature space into a new space with higher dimension where the probability of the data to be linearly separable is high. Radial basis functions are one of the kernel functions that can be used in SVMs.	1
863	28	Kernel functions are used in SVMs to put the input into a higher dimension. One dimensional data may not be linearily separatable as it is, but maybe if we project it with a parabola it becomes linearily separatable.	1
864	28	Kernal functions are used in SVMs to project the non linear data points to high dimensional space where the data can be classified using linear decision boundaries.	1
865	28	SVM tries to maximise the margin of separation between the 2 classes. Kernel functions are used to trasform the non-lineary separable data into higher dimension using non-linear transformation. This is based on Cover's theorem which states that there is higher probability that data will linearly separable in higher dimension. Some common kernels used in SVM are rbf, polynomial etc.	2
866	28	Kernel function is inner product of weight vector and input vector, Transforming nonlinearly from input space to the hidden space (which has a number of support vectors) using polynomial function or radial-basis function, then it is separable linearly in the hidden space and optimal desicion surface is likely to be found.	2
867	28	One key innovation associated with SVMs is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. It allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge efficiently. Besides, the kernel function k often admits an implementation that is significantly more computatinal efficient than naively constructing two vectors and explicetly taking their dot product.	1
868	28	The Kernel functions used in SVMs is a non-linear transformation into the input space dimensionality by increasing the dimension of the input space. As the Cover's theorem mentiones, when applying this transformation into a non separable input data, the resulting input data space can be separable. An example of a kernel function in SVMs can be $k = (x, x^i)$	2
869	28	Kernel functions in svm transform a problem in a lower dimension into a higher dimension in order to give new view points on the data, that may enable a linear partioning of the data.	1
870	28	Kernel functions in support vector machines are used to increase classification abilities of the network. For example to make a network to be able to classify non-linear problems, a non-linear kernel function is required.	0
871	28	Kernel functions in SVMs transpose the input patterns into higher dimensions, making them more likely to be linearly seperable. Commonly used kernel functins are RBFs, or polynomials.	1
872	28	YOUR ANSWER HERE A kernel maps an input space to a higher dimesional feature space. The concept is that a non separable input becomes linear separable in a higher dimensional feature space.	1
873	28	**Answer 8:**    A kernel function is used to cast the data into a higher dimension or to provide a different viewpoint of the data. This enables the SVM to then separate the data linearly. For instance, 1D data could be cast into two dimensions by a polynomial kernel where it could become linearly seperable. Other kernels are RBF, sigmoid.	2
874	28	The kernel functions in SVMs are used to represent data in a different way. There are multiple options:  - squared inputs or even larger exponents  - different linear combinations of inputs  - different activation functions	1
875	28	Kernels help us to work with high diensional data into a 2 dimension and then we can use a SVM for doing the classification	1
876	28	Kernel functions are used in SVMs to allow them to classify non-linearly separable data. They project the data in higher dimensions where it becomes linearly separable. Some kernel functions are Polynomial, Radial etc.	2
877	29	Perceptron Learning only works in case of linearly separable inputs- Perceptrons forms the basic unit of the neural network model. They are divided into layers connected by synaptic weights.	1
878	29	The theorem tells us that perceptron learning converges if the given data is linearly separatable.	1
879	29	* Move the hyperplane towards separation in each iteration.  * If the error is positive, then update the weight by adding the error with the input.  * If the error is negative, then update the weight by subtracting the error with the input.  * If there is no error, then dont updtae the weight.	2
880	29	Perceptron Learning theorem tells us that if data is linearly separable then algorithm will converge in $n_{0}$ iterations such that $ n_{0} < n_{max}$	2
881	29	The perceptron leaning algorithm terminates if and only if it is linearly separable. it can not be used for nonlinearly separable problem.	2
882	29	+ Perceptron learning theorem states that if the data is perfectly linearly separable, a linear perceptron will always converge in finite number of iterations.	0
883	29	+ Perceptron learning theorem states that if the data is perfectly linearly separable, a linear perceptron will always converge in finite number of iterations.	1
884	29	It tells us that a classification problem with linearly separable dataset can be solved using error correction.	2
885	29	If the error from desired output and actual output is equal to 0 then learning is finished.	0
886	29	The theorem tells us that if a dataset is lineary seperable, the perceptron learning algorithm will converge in a finite number of steps. If it is not, the perceptron learning never converges.	2
887	29	[---]    It tells us that if the datasets or training examples are linearly separable then perceptron learning theorem converges	2
888	29	**Answer 9:**    The perceptron learning theorem tells us that if the data to be learned is linearly seperable, then the convergence is definetly attained. If the data is not linearly seperable then there will be no convergence as there is no linear boundary to seperate the data.	2
889	29	The theroem tells us that the learning of a perceptron is finite, by giving us a upper bould for the training iterations.	1
890	29	[--]    Perceptron learning theorem tells us that given a set of linearly separable data, a perceptron can fit a linear model in a finite number of iterations. It means that the perceptron convergence algorithm(PCA) will definitely converge after a finite iterations. Given two classes m_0 and m_1, the PCA will converge in n_0 iterations which is always less that n_{max} iterations required for the dataset of union of classes m_0 and m_1.	2
891	29	It tells us that given two linearly separable sets C1 and C2, the perceptron learning algorithm converges in $n_0$ iterations which is bound by the finite value $n_{max}$	2
892	29	[--]   The perceptron learning algorithm tells that given a set of training examples $\{x_i,d_i\}$ that are linearly separable, the algorithm converges by finding a decision surface in the form of a hyperplane that separates the given patterns into the respective regions. The algorithm however cannot learn examples that are not linearly separable	2
893	30	[--]this    Three-way data split is     Training : you use training data to train the prameters of the network.      Validation : In this process you do the validation by some validation data sets. here you tune the parameter of the NN.    Testing : this is final test data sets. which used to test the performance of the Neural Networks.    to use with NN you devide the Data into 3 sets which mentioned above.   and train initially then do some turning with validation set this will be final tuning/ modification.  then test with data.	2
894	30	The three way data split in NN training is dividing the data into training set, validation set and test set.  Training set is used to learn the parameters of the network and determine the network structure. Validation set validates the learnt model and optimizes it. The test set evaluates the performance of the learnt model No parameters are adjusted after the validation state.	2
895	30	YOUR ANSWER HERE      Three way data split is splitting the data into three groups, training set, validation set and test set.    * The training set is used for adjusting the weights of the network  * The validation set is used to adjust parameters like stopping criteria, numbers of layers, number of neurons in the layers etc.  * The test set is used to find generalization error.	1
896	30	The data is split into three groups:    1. training data:  They are used train the network    2. validation data: They are a small part of the training data. They are used validate the trained data and compare with the other networks.     3. testing data: They selected network is validated with this dataset	1
897	30	Data is split into training, validation and test data.    First, different networks are trained using the training data. After that, all trained networks are tested using the validation data. The best network is choosen and then trained again with training and validation data. Using the test data the error of this final network is computed.	2
898	30	The data is split into training data, evaluation data and test data.    First the NN is trained with training data and using the evaluation data the weights are adjusted. This is repeated until there is no change in any of the weights. Now the test data is used to test and know the accuracy of the NN.	0
899	30	Three way data split.   1. Training stage: In this stage we set the parameter for classifier.   2. Generalisation stage: In this stage we adjust the parameter for classifier, such that it will be the last stage before testing. So the classifier in this stage is the best one.   3. Testing stage: Here we check the parmaeters, how the learner performs.     After testing we dont adjust anything.	2
900	30	In NN training data is splitted into training dataset, validation dataset and test dataset. Initially the models are trained using training dataset and tested with validation dataset to test the models. Then the model with minimum error is chosen and again trained with training plus validation dataset. Newly trained model is finally tested on testing dataset.	2
901	30	+ Training set: for learning NN models  + Validation set: for evaluating various NN models for performance  + Test set: Evaluating performance of NN that performed best on validation set.	1
902	30	Three-way data split separates the given data points into training set and test set, and the training set is divided into two groups: one for model training and the other for model selection. The test set is used to validate the model.	1
903	30	In a three-way data split, the data is spit into three sets, the training set, the validation set and the test set. If different archictectures have to be evaluated, the three-way split helps to choos the architecture of the network. For this purpose all architectures are trained on the training set. The validation set is then used to determine a training error. From the results the best performing architecture is choosen and trained on the training and validation set. The test set is the used to accquire the final error of the choosen architecture.	2
904	30	Three-way data split is splitting the dataset what you have in following categories, so that data from one of them is not part of other two:    1.Training data (network is trained using this dataset)    2.Verification data (verify your training results)    3.Test data (test your network on this dataset)	1
905	30	Training dataset is used for tuning of weight parameters of the model. The Validation set is used to determine if the model is improving on unknown data or overfit on the training data. The test set is used to report the final classifier or regression error on a new dataset, and should be only used a single time, for eyample in reporting classificatin results in a publication.	2
906	30	"- The input data is split three ways:      - Training set      - Validation set      - Test set  - The training set is used to train several modes (say, M_1 to M_n)  - The validation set is used to ""test"" the results of the different models to get errors (E_1 to E_N)  - Then the model with minimum error is selected.  - This model is trainied again with the training and validation sets combined.  - Test set is used to get the error of this model"	2
907	30	[---]    Three-way data split is :    i. Training set    ii. Validation set    iii. Test set    First we train the algorithm only on training set and we adjust parameters based on training set to get optimized model. We adjust parameters based on error between current output and desired output. This can be done using Back-propagation algorithm    After that, we evaluate that trained model from training set on validation set. Again, we optimized the parameters of model based on validation set.     There are methods using those, we can generate validation set. Such as, Random subsampling, k-fold cross validation and so on. This is generated from training set. Like, from every batch of training, we can take one set for validation and save it for later testing of model      Now in Test set, we do not train any parameters. We only test and compute the accuracy of the model.   	2
908	30	**Answer 10:**    A three-way data split splits the labeled data available into training set, validation set and test set. Different NN structures could be trained on the training set and the performance of each of the structures could be measured on the validation set. The structure which performs best is selected and then trained on both training and validation set to produce the final model. The final model is then tested on the test set to get the performance and determine the generalization ability of the final model (as the test set is unseen).	2
909	30	A dataset is split into train, test and verification data.  It the train dataset is used to train different network topologies. These are than evaluated and compared using the test dataset. Once the best performing topology is found is is newly training using train and test data. Finally it's performance is determined using the verification dataset.	2
910	30	The three data-split is to split the given data in three groups:    - training data -> given to train models   - validation data -> given to optimize the parameters of the final model  - test data -> given to test the final model    if we have enouth data we can find the best NN-Model to use by saperating the data in thee groups. 	2
911	30	Data is splitted into training data and two types of test data (a and b). Training data is used to train diffrent models of a NN. Then test data a is used to test the trained models. Choose model with lowest test error. Train the selected model with training data and test data a. Then test with test data b.  	2
912	30	Spilt the given training data into   1) training data set  2) validation data set  3) test data set  First many differnet arcitectures are trained with the same training data. Then the best model (lowest error) is picked by using the validation set and compute the error for each model. At last the best model architecture is picked and trained with traning and validation data, to ensure more traning data and a better generalization. At last the error of the trained NN is determined by using the test data set.   It is related to the curse of dimensionality 	2
913	30	Three way split data split is a training and testing method for different NN models.  The three splits are training, testing and validation.  The training split is used to train the different models and verify them with the testing data. The gathered knowledge is used to generate a combined model which is then tested against the validation split.	2
914	30	Three way data split states that the training data into training data  for training the algorithm, validation data for fine tuning of the network and test data for testing the performance of the network.	2
915	30	[--]    Data is split into training, testing and validation subsets.    Training data is used to fin dthe weight parameters of NN that model the desired function with the help of the labels.    Testing data is used to test the accuracy of the trained   model.    Validation data is used to validate the model for different conditions and parameters.	2
916	30	The three-way data split is   1. Training data  2. Validation data  3. Test data  The training and validation data is used for model selection (for e.g. Structural Risk Minimization). Once the best model is determined, it is trained usinf both training and validation data. This trained model is then evaluated using the test data which should be completely new to the network.	2
917	30	[--]    The three-way data split refers to dividing the input data into 3 parts : namely the training set, validation set and the test set. The training set is first used with different models to learn the parameters of the network. The validation set is then applied to further reduce the error during training. The training and validation set are then combined and used together to learn the parameters of the network to further minimze the error. The learned parameters are then used with the test set to test the results.	2
918	30	Three-way data split splits the input data into training data, validation data and test data. The NN is trained only on the training data. After this the NN is used on the validation data to check how good the network performs on unknown data. At the end the test data is used to further test the network. This way of training helps to prevent overfitting of the network.	2
919	31	If we add more trainning smaples their is a risk of overfitting. The training examples are learnt very well, which results in decrease of training error, but can increase test error due to lack of generalization	2
920	31	YOUR ANSWER HERE    TESTERROR <= TRAINERROR + VC-Conf    VC-Conf is inversely proportional to the number of training data points. So if we increate training samples then the VC-Conf will decrease and the upper bound of the TESTERROR will decrease too.    So adding more and more traning sample will reduce the test error.	2
921	31	As we increase the training samples the train error and test error might decrease but after a specific point the train error goes to zero and the test error increases. This is due to overfitting. The machine memorizes the train data and not be able to estimate the output for new data properly.	2
922	31	If we add more and more training samples then training error will decrease as per statistical learning theory. Further statistical learning theory states that test error bound is given by training error and VC confidence which depoends on VC dimension the machine. Thus with decrease in training error, test error bound will also decrease.	2
923	31	The Neural network learns better generalization after seeing more and more data. Therefore if we increase the number of training samples, the test error should either reduce or stay the same if the new samples are not \emph{good} representatives of the underlying distribution.	1
924	31	Adding more and more training examples can lead to the problem of overfitting. In this case the training error becomes smaller and smaller with additional training samples, but the test error becomes larger and larger. This is due to fact, that the NN have learned features of less importance and became too specific. 	2
925	31	The test error is bounded by the training error and a term dependent on the VC dimension of the given task. Both errors change with the number of training samples: if we add more training samples, the training error will become smaller while the test error will increase.	2
926	31	When adding more training samples, the training error will be reduced. But, if we add too much training samples we will have what is call generalisation error. The generalisation error occurs when over-fitting the neural network with too much training samples, and the neural networks become unflexible and remember just the training samples that where given.	2
927	31	We are not able to calculate the test error of a network since we would need all possible test cases for this. So we can instead approximate the test error by computing a training error on a given training set that contains a subset of all examples. So the more samples we add, the closer the training set gets to containig all examples and thus the trainig error approximates the test error closer.	2
928	31	Train error is what you get when training your network, as we add more samples it will be reducing. But if we will be adding to many training examples test error will increase, as in the end we will learn the noise from training data)	2
929	31	- If we add more training  samples, then the training error will decrease. In other words, the Bias will decrease  - But, this will result in higher test error when we check with previously unseen data; i.e., variance will increase.  - This generally means that the system is overfitting to the training data.	2
930	31	YOUR ANSWER HERE If we add more and more training samples the network tends to overfit (neural network learns training data), so testerror increases.	1
931	31	Because the network is only trained with the training samples, the train error will go down. If we decrease the train error too far, the network might learn the actual training samples and their noise. This results in an increasing test error.	2
932	31	If we add many many more training examplex the training error approximates the test error. If we use all possible data in the world the errors are the same, because were isn't a difference between training set and test set anymore. 	0
933	31	The trainig error will decrease with more training examples. The test error will get closer to the Trainig error. The test error can not be better than training error.	2
934	31	The train error is steady decresed by adding more training samples. But the more samples, the less the error is decreased (it stagnates (german: stagniert)). The test error is first decresed by adding more training samples until it reaches a minimum. When adding even more traning samples, the test error is increased because of overfitting. 	2
935	31	Training error always generally decreases with increasing training samples.     Testing error reduces upto some point with the training samples. But after a point VC confidence starts increasing with the training samples hence testing error is also increased. 	2
936	31	By adding more and more training samples there is a possibilities that the network becomes overfit and learns everything in the training data set such as noise and this means we will have a very small training error but when we start to test our network with samples that it hasn'e seen before we will have a very big error.	2
937	31	As we add more and more training samples, the test error decreases while the training error increases as the network begind to overfit the data by memoring the mappings instead of generalizing.	2
938	31	The train error will become smaller when adding more and more training samples. The test error will become bigger, because the network overfits and memorized the training samples. New unkown test data will produce errors because it can be different from the trained data.	2
939	32	The term local gradient in back propagation is  when the output error is fed back into the hidden layer , for each synapse the error is calculated locally by steppest descent and the weights are adjusted. This is done by calculating the gradient locally for that connection only.	1
940	32	* Local gradient tells how much the weight should be updated for a neuron.  * The synaptic weight updated is directly proportional to **partial derivatives**  * Local gradient is calculated at ouput neurons and hidden neurons.  * Local gradient at output neurons are calculated using the observed error.  * But the error function is missing in the hidden neurons, so the local gradient of hidden neuron j is calculated recursively from the local gradients of all neurons which are connected directly to the hidden neuron j. 	2
941	32	Local gradient is the jacobian of the cost function with respect to weights. It is used in steepest descent method in backpropogation. The weights are adjusted in the opposite direction of the local gradient to reduce the error. 	1
942	32	An hidden layer is not directly connected with feedback layer, so it cannot directly contribute to it. In order ro contribute to that we find local gradients. Local gradients are basically nothing but variation of cost function with respect to the weight. This is particular to a single hidden neuron.	1
943	32	$$\delta_j(n) =  \phi^{'}_j(v_j(n)) e_j(n) \phi^{'}_j(v_j(n)) \sum_{k}{} \delta_k(n) w_{kj}(n) $$	1
944	32	The local gradient is the proportion of the activation corresponding to a neuron j when the error is being back propagated into hidden layer neurons. Since we we need compute error $e = d - y$, and $y$ does not exists yet in the hidden layers, we use the local gradient, which is a calculated using the derivative of the activation function. 	1
945	32	Local gradient is the partial derivative of the cost function to the induced local field of each neuron. It is required in back-propagation to adjust the weights of each neurons.	1
946	32	The term local gradient will be calculated in each neuron of the hidden layers in the back-propagation algorithm. These gradients values will be used in the backward phase, where the weights of the hidden neurons will be adjusted.	1
947	32	A local gradient is the measure of how much the neuron that the gradient belongs to contributes to the final error. So it can be used to adjust the weights of a neuron in a multilayer perceptron. It depends on the neurons position how this gradient is calculated. 	1
948	32	Local gradient for a neuron is partial derivative of the output function. There are two cases, first is when we are dealin output neuron, then we need to take partial derivative of the error of the output (desired output minus actual output). Second case is for neurons that are located further away from output. Then local gradient will consist from partial derivatives of all the previous neurons that we computed local gradient for.	1
949	32	The local gradient is the sum of weighted deltas in backpropagation of the error in a hidden layer from previous layers (either other hidden layers or the output layer). This is due to the derivation from the chain rule for derivation, since we only have the error signal in the output layer and thus must backpropagate the error.	2
950	32	- Local gradient is the gradient that is calculated at each ndoe (including the hidden nodes)  - This is used in back propagation to pass the error from the output layer to the inner layers.	1
951	32	[---]    Local gradient is partial derivative of error with respect to weight.    In order to minimize the error of the model and adjust weight parameters. We compute the gradient of error with respect to weight. However, in case of output neuron it is easier to compute.     For hidden layer neurons, we use chain rule and compute gradient of each recursively. 	2
952	32	YOUR ANSWER HERE The local gradient in back-propagation is the derivation of error function which is given by:  $ \frac{1}{2}\sum e^{2} $ The errror is given by $ e = d - y $. We calculate the derivation of error function as we want to determine the minimum of error surface. To do so we calculate the local gradient which points into direction of highest increase, by multiplying gradient with -1 we get direction in which error surface decreases most.	1
953	32	The local gradient used in back-propagation determines how the weights of a neuron changes. It is different for general and output neurons:  - For node i in an output layer: $\delta_i(v_i) = \varphi^\prime(v_i)(d_i - y_i)$  - For node i in other layers: $\delta_i(v_i) = \varphi^\prime(v_i)\sum_{j\in C} w_ji \delta_j(v_j)$, where $C$ are all the nodes that use node i output as an input	2
954	32	The local gradient is a vector, that showes in direction of steepest ascend of a function at one point (local). In back-propagation the negative gradient is used to get the direction of steepest descent. It is used to adjust the weights of each neuron while follow in direction of steepest descent of the error function	0
955	32	Local gradient is the derivative of local error of a neuron with respect to the local field of that neuron.     $d_{i} = \frac{de_{i}}{dv_{i}}$ 	0
956	32	Local gradient is the gradient of the all the output error of the neuron(from all the connections to the next layer) divided by the gradient of the activation function and the inputs multiplied by their respective weights.    This gives us the amount of error each input is responsible for for the input of the each neuron	0
957	32	The local gradient is the partial derivative of the cost function with respect to the induced field.  $$\delta_j(n) = - \frac{d \xi(n)}{d v_j (n)}$$   d is partial derivative.     The value is different for output and hidden neurons. For output neuron, $\delta_j(n) = \varphi'_j(n) y_i (n)$. For hidden neuron $\delta_j(n) = \varphi'_j(n) \sum_{k in C} \delta_k(n) y_{i} (n)$	2
958	32	[--]  The local gradient in back-propagation is defined as :  $\phi^{'}_{j} (d(n)-y(n))$ if j is a output neuron	1
959	32	The local gradient $\delta = \sum w\delta x$ is the summation of the error of the neurons, which are connected to the current neuron multiplied by the weights and the input data. The weights of the current neuron are updated using the local gradient because the error of all following neurons influence the weight update. The error is backpropagated through the network with the local gradient.	1
960	34	YOUR ANSWER HERE:    In Newton's method, the quadratic approximation of the error signal around a weight vector at time step n is used. It can be utilized for NNS if  - The hessian matrix is positive definite  - The error function is twice differentiable.    Newton's method is smoooth and no zigzag behavior is seen.	1
961	34	[--] this    Newtons is quadratic approximation method.     2nd order differential equation.  it can be used for NN to reduce the learning time and it converges faster than other algorithm.	1
962	34	Newtons method in genral uses a Hessian matrix of the error function to adjust the weights of the network. Only problem is the Hessian matrix has to be positive definite. It removes the zigzaging nature of trajectory of weight adjustment caused by high learning rate in steepest descent and also converges faster. 	1
963	34	YOUR ANSWER HERE    Newton's method is an iterative approach of finding the root of a function.    * In neural network it is used to minimize the error  * It converges quickly  * It does not give zigzag effect  * It requires the error function to be twice differentiable	1
964	34	Newton method is an approxiamtion of second order derivative.    They construct a slope for the function to be predicted using the know current step values.   Based on the x intersect of the slope and the other known values the next slope is estimated.     They are used as a learning technique in the Neural networks. 	1
965	34	Newtons method is a method for finding minima or maxima in a function using the hessian matrix. It is more robust against local minima and maxima than the method of steepest descent. It can be used in neural nets to change the weights in the right direction, because the error should be minimized.	1
966	34	* Newton method minimizes the quadratic transformation of the error cost function $E$ around the vector (w).  * It uses Second order taylor series    How can it be utilized for NNs?     * Steepest decent methond finds only the direction to reach global minima to reduce the error.  * But Newton method also finds the velocity (steps) of moving the weight vector to reduce the error.  * Newton method doesn't work always, because it requires the hessian matrix (H) should be positive definite for all values.	2
967	34	In newton method we try to minimize the cost function. It is used in backpropogation for adjusting the weights.	0
968	34	Newton method minimize the quadratic approximation of cost function E(w) around the current point w(n). It is basically used to reduce the value of the cost function. The hessian matrix produc by the second order of the taylor series must be positive definite.	1
969	34	Newtons method is second order version of a steepest descent method. It minimises the quadratic approximation of the error function. In neural network it is used as an iterative unconstrained optimization learning technique. It is used to adjust the weights such that second order approximation of the error function is minimises. Newtons method converges only if Hessian matrix is postive definite.	2
970	34	The newton method is an unconstrained optimization method that uses the squared error as cost function to minimize. Unlike the steepest descent, it requires a second derivative of the cost function and knowledge about the mxm Hessian matrix dependent on the input.	1
971	34	+ Newton mathod learning rule: $w(n+1) = w(n) - H^{-1} e(n)$ $H$ is hessian of $e(n)$.  + The convergence criteria is met only if the hessian of error vector is positive definite.  + In general, we cannot be sure of the positive definiteness of the hessian.	1
972	34	The Newton's method is an improvement of the steepest descent method, where the minimization of the error will be calculated by using the Hessian Matrix. For the Newtons method to work properly, the Hessian matrix must be a positive defined matrix. This approach will then converge faster and also the zig-zagging behavior (from the steepest-descent function) will be removed. This method can be utilized when adjusting the weights of the NN and reducing then its error.	1
973	34	Newtons method is used to optimize a function by using the second order tylor expansion on a point, then getting the minimum for that expansion and take it as the next point. In NNs it is used to optimize the weights of the network by finding the weight vector that minimizes the error between output and desired output.	1
974	34	Newton method is a method of weight adaptation that uses second order derivatives, therefore is converges a lot faster and non in zig-zagging manner as stepest descent. But in order for it to work Hessiam patrix must be positive and fully defined, which is not always the case. Weights of neural networks can be addapted using this method.	1
975	34	[---]    It is an unconstrained optimization technique. Where we minimize the error of the network by adjusting the weights    Newtons method approximates the quadratic function of the cost with respect to weight. It converges faster than steepest descent.     This method works:      . Hessian is positive definite matrix      . In real problem, this is not always possible.	2
976	34	Newtons method find the root of an differantal function. We can optimize the NN by using Newtons method if the Hessian matrix is invertable. If not we can use gauss-netwon. The newtons method minimzises the quadratric approximation of the cost function around the current point w(n).	1
977	34	Newtons method makes quadratic approximation of an underlying function. But it requires a positive definit matrix. In NN it is used to reach the minimum of the cost function by quadratic approximation of the current error	0
978	34	Newtons method is a gradient descent method deriving from steepest descent.  Instead of using the gradient directly a Taylor series approximation is used to converge to the minimum. This avoids the oscillation problem of steepest descent.    It can be used to train NNs instead of the steepest descent method, however it only works when the Hessian is positive definite.	2
979	34	Newton's method is a zero finding method which tends to minimze the cost function . It uses second order taylors' series for this purpose , hence the hessian matrix needs to be positive definite for newton's  method to work.It is utilized in NN's to mind the local or glolbal minima or to minimize the cost funcion.	2
980	34	[--]    Newton's method utilized the second order approximation of the estimated average cost function. This is to overcome the possibility of divergence due to steepest descept method. The weight si updated using second order Taylor's approxmation. This involves computation of Hessian matrix and requires it to be non-singular.	2
981	34	Newtons method is a stepwise approximation to the zero point of a function. It uses the first taylor polinomial to approximate the function and to get closer to the zero point in every step. It converges in quadratic time. This can be used in NNs to minimize the error function. This is done by using the Hebbian of the error function. The Hebbian has to be positive definite for newtons method to work. This converges faster than the Steepest Descent.	1
982	35	In case of steepest descent method of weight adjustment for learning, high learning rate($\eta$) results in oscillatory (zigzaging) nature in weight adjustment process. This can be countered by adding a momentum term ($\alpha$) in the learning equation.  $W(n) = W(n-1)-\alpha\eta\Delta W$	1
983	35	The momentum term is added in delta learning rule, such that is enables(keeps) smooth path, a path without occilations, while searching for min of error function, in negative gradient direction. IN other words, it accelerates seach in direction of steepest descent, such that search stays in that direction-does not ocilates.    It has similar effects on seach for min error in NN, like effects in mechanics, when momentum(a moment  of inertia)  keeps certain body moving in its already defined direction, until some stronger force changes its direction.	2
984	35	$ delta w(n) = \alpha delta w(n-1) + \eta e_k . x_j$    $ \alpha $ - momentum.     They decide how fast the network learns from the training data    THey are used to solve the problem of over damping and underdamping state caused by learining rate	2
985	35	The momentum is a value which tells how strong the weight change of the last iteration is taken into account for the current weight change. It therefore is a value for the momentum of the weight change. The momentum is used to avoid oscillating behavior.	2
986	35	Momentum is used to increase the convergence rate. It basically varies for the various values of the learning rate. 	2
987	35	The momentum accelerate the gradient in direction to the steepest descent. It also stabilizes the function in the directions where it oscilates.	2
988	35	Momentum term stabilizes the gradient descent learning in the direction of the descent.	2
989	35	Momentum constant is a term to improve the training of a network. It can increase the learning process while mitigate the undesirable oscillation of the learning path. It is used in the generalized delta rule.	2
990	35	momentum is an additional term which can be applied to the widrow hoff delta rule to increase the convergence. It is a variable factor which incoporated the slope of change of the weights. So when we are in a straight down hill direction we can accelerate convergence.	2
991	35	Momentum is a parameter used in learning machines that will stabilize the effect of the learning rate into the progression of learning. If learning rate is large, then we have an unstable learning progression, and if learning rate is smalle we have an slow learning progression. The momentum helps to find an stable learning state, where both effects (learn fast and stable) can be obtained.	1
992	35	[---]    As we know, when learning rate is lower there is slower rate of convergence.  when learning rate is higher, then algorithm become unstable    In order to overcome this problem, we can use momentum.    Momentum avoids the fluctuations or it smoothes the behavior, when gradient is oscillatory    It accelerates the steepest descent to the direction of steady state   	2
993	35	**Answer 11:**    A momentum is a term used to speed up the descent of weights and to attain faster convergence. The term is based on the previous weight update and a factor $\alpha$. The term encourages movement along the steepest downhill faster. It has a stabilizing effect on weight updates which oscillates with time.	2
994	35	The momentum in learning basically uses the old $\Delta w(n-1)$ from previous iteration to determine the new $\Delta w(n)$.    By doing so we prevent oscillation in weight changes and can improve learning speed.	2
995	35	Momentum can be used to prevend oscillation during learning. A part of the weight change from the last iteration is added to the next iteration.	2
996	35	The momentum is a parameter used steepest descent method in back-propagation learning. It increases the learning speed in steady downhill directions to increase speed. In other case it slowes down the steepest descent to increase stability and convergance	2
997	35	Momentum can be used to adjust the learning speed of a machine. By increasing the speed of learning during stable convergence and stabilizing the learning in phases of oscillation.	2
998	35	Momentum is added turn in the adjusting weights to keep the learing rate fast in the begining and then stabilize it.    $\del w[n] = \alpha * \del w[n - 1] + \eta * g(n) * x(n)$    $\alpha$ is momentum constant.	2
999	35	The momentum is a term added alon with learning rate to avoid oscillations while finding the transient response of the algorithm.	2
1000	35	[--]    Momentum is used to maintain stability in the learning process by reducing the oscillations that the network without stability is prone to.    Momentum can also pass on the required information that needs to be retained in the network's memory.	2
1001	35	Momentum helps the learning process in the steepest descent methode. As mentioned before when we have a big learning rate we move very fast toward the global minima but we will oscilate around it because of the step size in each jump but the momentum will handel this situation and while it lets the methode to move fast toward the global minima at the beginning but while we get closer to the global minima it controls the scilation and we can reach the global minima.	2
1002	35	Momentum is an additional term used in the weight update rule. It reduces the impact of the learning rate by -  1. Steadying the descent in directions where the algorithm oscillates  2. Faster convergence n directions where the gradient is smooth	2
1003	35	The momentum is a additional parameter, which can be added during learning. When updating the weights the formula is adapted using the momentum $w_{new} = \alpha w_{old} + learning\_rate \cdot x \cdot e$. Similar to the learning rate the momentum changes how fast the network learns. It smoothes the descent of the learning. The momentum is adapted with time.	2
1004	36	[--] this    Normlization of inputs consist of scaling the inputs to the same scale so that it has least variance .     Normalizing is also reduce the data between 0-1. for complexity redcution.      PCA is one of the form. 	1
1005	36	YOUR ANSWER HERE    Normalization of input is a way of making the data independent of units. For example in a given data of total working hours and salary for a goup of people, the range of the values will be different for salary and total working hours. Normalization is a process to convert these values within a similar range (say 0 to 1) to make computation easier.	1
1006	36	Normalization is a process of preparing input data, such that it is independent of its units, and enables easer learning.    First step is remove the mean from the data, to center the data.  Next step is to make data uncorelated, by performing PCA on it.  Next step is to reshape decorealated data,  such that to make its covariances equal. 	2
1007	36	When features of different range has to be used by the network, then is advised to do normalise the input.    the normalisation involves three steps:    1. Mean removal  2. Un correlate the data (PCA)  3. Scale to approximate the covarianvce of the data    By this all the weights in the network change uniformly. 	2
1008	36	Before input data is passed to a network it should be normalized. Normalizing means mean subtraction, correlation normalization and covariance normalization. Using this the network will not learn any biases but only the structure of the data.	2
1009	36	* Normalization is a scaling process that makes the inputs to have **zero mean and variance of 1.**  * The set of input variables are uncorrelated using PCA.  * Then the decorrelated values are scaled, shifted/rotated such that all values should have **approximately equal covariance.**	2
1010	36	For better training the inputs variables should be uncorrelated. The decorrelated variables are then manipulated so that they have equal covariance. This whole process is called normalization of inputs of whitening the data.	1
1011	36	Before the learning process the data may be higly correlated, so we need to    1. Remove the mean.   2. Decorrelate using the PCA and   3. And try to make the data set have high covariance.     This is basically used to reduce the computation complexity and in order to consider the measuring units. 	2
1012	36	It refers to rescaling the inputs such that each input has equal influence on the output of the network.  	0
1013	36	make data independent of measurement units and has a variance of 1.	1
1014	36	+ The inputs of neural network could take arbitrary values and the distribution cannot be learnt well.  + Normalization of the input is mapping the input to a probability space $[0,1]$ by dividing it by the total sum of the inputs.	1
1015	36	The input variables contained in the training set should be uncorrelated. In order to prevent this PCA is a technique that can solve the issue. Further the decorrelated input values should be scaled so that their covariances are approximetely equal, thereby ensuring that the differnt synaptic weights in the network learn at approximetey the same speed.	1
1016	36	Normalization of inputs is to make inputs uncorrelated and with equal covariances among each other. It is used to remove the dependence of inputs on measurement units and make the weights of a network learn at similar rate. 	1
1017	36	normalization or whiting is transforming the input data so that it is easier to compute and the network is working independent of the measuarment units.     Techniques:  - mean removeal  - decorrelation  - covariance equalization  - changing the codomain to ie [0,1]	2
1018	36	Normalization of inputs is basically making inputs invariant in scaling, so that inputs go through covariance matrix and getting normalize, as if they dont some weights will be trained faster then other ones, which is undesirable.	0
1019	36	Normalization of inputs is used to transform input patterns into an uniform range, making them usable for sigmoidal functions or other bounded activation functions. Often, this is either done through min max normaliyation between 0 and 1, or subtracting the mean and dividing by the standard deviation.	2
1020	36	Normalization of inputs refers to processing the inputs as per below:  - Mean removal  - De-correlation  - Covariance removal    This is also called whitening and is used to ensure that the weights are learned in the same pace and are not scaled different magnitudes of data due to differebt units.	2
1021	36	[---]    Normalization of inputs means making the data independent of the measurement units. We also make sure that data is properly distributed, so we centered the inputs or training exmaples. This can make the computations faster and easier. 	2
1022	36	YOUR ANSWER HERE Normalization determines method to map inputs into a normalized distribution. A normalized distribution helps network to perform better.	0
1023	36	**Answer 12:**    In normalization, mean is removed from the input data and decorrelalized with the help of PCA. The data is then put through covariance equilization. This ensures that the data has similar effects on all the wieghts.    Normalization is required to rid the data from its dependency to a measurement unit.	2
1024	36	Also called whitening of data:  - Mean removal, center data for each dimension  - Decorrelation, typically principal component analyses.  - variance equalization	2
1025	36	The input should be normalized, so that the NN weights learn approxamtly the same speed. We want to decorrelate the date (PCA) and make the covariance the same. In the beginning we make mean removal.	1
1026	36	In normalization the variance of the data is brought to 1.	0
1027	36	By normalizing or whitening data the unwanted correlations between the data points, as well as scaling or rotation of the data set should be removed. With a covariance matrix equal to the identity matrix there is no correlation and a variance of 0, which leads to better learning results.	2
1028	36	Normalization is also refered to as whitening of data. First the data is made uncorrelated using PCA. Then the decorelated data is scaled so that the covariances are same , so that each parameter takes equal amount of time while training.	1
1029	36	[--]    Normalization of inputs is the scaling of data. This removes biases and possible effects of noise on the data. It can be done by subtractionng the mean of the data and dividing it by the range,. It makes it easy for data comparison.	1
1030	36	It means that by excution of some mathematical oparators on the inputs we eliminate the effect of non relevant information such as units and this will simplifies working with the data.	0
1031	36	Normalization of inputs includes following steps -  1. Mean removal (centering)  2. Decorrelation (using PCA)  3. Scaling of covariances  It is done so as to allow the weights to learn at the same rate.	2
1032	36	The normalization of inputs removes the mean of the input data, decorrelates the input data using PCA and equalizes the covariance of the input data. This is done to normalize the input so that every input data has a similar influence on network as other input data.	2
1033	37	[--] this    It depends on the application.  There are two methods for determining    1) Start with more number of layers and check the performace . start reducing the layers and stop till you get the satisfacory performance.    2) start with less number of layers and check the performance . keep adding layers till you get satisfactory performance.	2
1034	37	YOUR ANSWER HERE    * We can start with a simple network and keep adding layer untill the expected performance is achieved  * We can start with a complex network and keep removing layers while performance is within expected level.	2
1035	37	Trial and error:     1. Design the network with more number of layers and reduce them gradually. compare the output for all configurations    2. Design the network with more least of layers and increase them gradually. compare the output for all configurations    the First method is the prefered method	2
1036	37	The correct number of layers can be found by trial and error. For boolean function we know that no hidden layer is needed, for any continuos function we know that one hidden layer is sufficient, but for any other unknown problem we try to increase the amount of layers and check how the error behaves.	2
1037	37	* Number of layers/neurons is completely depend on specific tasks.  * This problem is solved by **trial and error** method.  * There are two ways to do this,  * Start with a larger network and try to remove some layers/neurons until there is a degradation in performance.  * Start with a smaller network, and try to add layers/neurons until the performance criteria satisfies. 	2
1038	37	The number of layers are determined by trail and error approach.  1. We can start with more number of layers and reduce the layers until the performace is good.  2. We can start with less number of layers and increase the layers until the performace starts to go bad.	2
1039	37	There are 2 ways of determining correct number of layers in feed forward networks:  * Add large number of layers. Then train the network until the performance is satisfactory. Then start removing 1-layer at a time until it keeps producing reasonable output.  * Start with few layers and keep adding more layers until network achieves satisfactory performance.	2
1040	37	The number of layers and of neurons depends on the specific task. In practice the issue is solved by trial and error. However there exist two types of adaptive algorithms that can be used to approach the problem:    * Start from a large network and successively remove some neurons and links until the network performance degrades.  * Begin with a small network and introduce new neurons until performance is satisfactory.	2
1041	37	The number of layers depends on the given task. One way is to start with a large network, and reduce the layers until the performance of the network decreases. Another way is to start with a small network, and increase the layers until the performance of the network cannot be improved.	2
1042	37	There are two ways:  1. Start with a small network and increase the number of neurons/layers until the error is sufficent for the problem  2. Start with a big network and reduce the number of neurons/layer until the error significatly changes    Also there are model selections techniques like 10-CV, SRMVC, AIC, BIC that can be used to choose promising models	2
1043	37	In order to determine the number of layers, different numbers of layers have to be implemented and tested. The one with the best performance is then chosen. 	1
1044	37	This is more or less trail and error, but there are two ways:    1.We can start with many hidden layers and start reducing them until the desired error rate is not below our threshold.    2.We start with very small amount of layers and gradually increase them until desired error rate is reached.	2
1045	37	[---]      It depends upon the task for which we are designing the feed-forward NN. This is mostly done by trail-and-error    We can also apply this:        i. We start from the large number of layers  and neurons, and each time we reduce the amount of layers untill we achieve good performance.      ii. or We can start from very small of layers, and then every time we add one layer and see when we achieve good performance. 	2
1046	37	**Answer 13:**    The correct number of layers can be indentified by using three-split of labeled data into training, validation and set. NNs with different layers are trained on the training set. Best generalizing NN on the validation set could be selected as the final model.    Other ways include starting from a large number of layers, obtaining the generalization, reducing the number of layers till reaching a good number of layers. It is also possible to start from one layer and go up until the desired generalization is reached.	2
1047	37	If we want two know the korrekt number of layers we have to know how complex the problem is and how much power a NN has. We can use Structural Risk minimisation or cross validation to find out which model fits best.	2
1048	37	There are two ways to do this:  1) start with high number of layers and continously reduce number of layser still error is acceptable  2) start with less layers and increase layers until generalization is sufficient 	2
1049	37	There is no solid rule on number of layers in feedforward NNs.    Generally a NN with large enough number of layers is chosen and trained.  Then number of layers are decreased until performance starts degrading.     OR    Start with a small NN and then keep on adding layers until satisfactory performance is achieved. 	2
1050	37	The number of layers can be determined using two approaches, one approach would be to increase the number of neurons until the generalization performance is optimum , other approach would be to start with a large number of layers and reduce it until the performance degrades. Number of layers depend upon the application.	2
1051	37	[--]    This is done by trial and error.    Two methods are possible:    A network with large numbe rof layers can be initialized and later reduced by checking errors.  or a smal network can be initialized and addition of nodes and connection can be done until satifactory performance is obtained.	2
1052	37	The number of layers is usually dependent on the nature of the problem and is determined by trial and error. There are two possible ways:  1. Start with a large network (with more layers) and successively trim layers until performance degrades.  2. Start with a small network and add layers until the performance is satisfactory.    The former is considered more popular.	2
1053	38	[--]this     This depends on the numbe of features. it not fixed because it vary from application to application    Normally it should be 5 -10 times the numeber of weights.     it decide the NN performance.	1
1054	38	Number of training examples for training a neural network follows a rule of thumb, it is usully 5-10 times the number of weights.  There is also another rule which can be useful:  it is equal to $\frac{|W|}{1-\alpha}$ where W is the number of weights and $\alpha$ the expected accuracy of the learning process.	2
1055	38	YOUR ANSWER HERE      * The number of training example is 5 to 10 times of the numer of weights in the network  * The number can also be expressed as $\frac{|W|}{1-a}$, a is the accuracy of the training data	2
1056	38	The number of training examples should be 5 to 10 times the number of weights in the network.    $$ nunmber of samples = \beta |\omega^2|/ \alpha^2$$	2
1057	38	We can use VC dimension to know the number of training examples need for training an NN.    Number of examples = $ \frac{1}{\epsilon} [4 \log_{2}(\frac{2}{\delta}) + 8 VCdim(H) \log_{2}(\frac{13}{\epsilon})]$	0
1058	38	The number of training examples needed for the NN should be 5 times the number of weights     or we can calculate using     N > |W|/1-a     Here the numerator denotes the number of weights and a denotes the accuracy of the test data set	2
1059	38	We can use statistical learning theory to calculate the minimum number of training examples based on accuracy we want and VC dimension of the machine. Another way of determining approximate number of training examples required is using the $\frac{W}{1-a}$ where W no. of adjustable weights and a is the amount of accuracy we want.	2
1060	38	It depends, on the application, but the rule of thumb is to use at least 5 to 10 times the amount of weigths in the network. You can also calculate it with the following formula:    $N = \frac{W}{1-\alpha}$    where N is the number of training samples, W the number of weigths, and $\alpha$ the desired degree of accuracy of the model. 	2
1061	38	As a general rule of thumb, we require 5-10 times the number of samples to the number of weights we want to train in the network.	1
1062	38	Two ways to determine that:    1) Rule of Thumb: The number of training examples should be at least 5-10 times the number of weights of the network.    2) Other Rule: N>|W|/(1-a); W=number of weights; a=expected accuracy on test set.	2
1063	38	A rule of thumb is that the traning examples should be at least five times the number of the synaptic weights in the network.	1
1064	38	According to the rule of thumb we need 5-10 times training examples than the amount of features that we have	1
1065	38	Generally dependent on the task, but, an estimate can be taken as $\frac{|W|}{\epsilon}$ where W is the weight vector and the $\epsilon$ is the desired accuracy	1
1066	38	[---]    The number of training examples should be 5 to 10 times the number of weights in the network    Rule of Thumb:     $$ N > w / |1- a  $$      where w is number of weights in the network, a is accuracy	2
1067	38	Rule of thumb: 5 to 10 times the number of nodes, BUT:  It obviously depends on the application and input data. E.g. if we want to classify images into 1000 different classes we will need much more sample data.  Also the share between train and test dataset varies.	1
1068	38	German:  Eine Faustregel lautet:   5 bis 10 mal mehr als Gewichte im Netz vorhanden sind 	1
1069	38	As per the rule of thumb the number of training examples should be 5-10 times the number of neurons in the layer.	1
1070	38	[--]    Number of training sampples,m is determined by the formula    m should be greater than or equal to $\frac{4\ln(2/\delta) + 8*VC(H)*\ln(13/\epsilon)}{\epsilon}$ where \delta and \epsilon are the errors and VC (H) is the vc dimension. $\ln$ is logarithmic to the base 2.	2
1071	38	The number of training examples depend on the problem given. For a MLP the training samples should be 5-10 times as many weights in the network. Using a probability and desired error of the network it can also be calculated using this formula $m \leq \frac{4\cdot log(2/probability) - log(13 / error)}{error}$.	1
1072	39	* Guassian   * Quadratic	1
1073	39	Mutliquadrics, Gaussiona function, Polynomial functions	1
1074	39	1. Multi quadraics  2. Inverse multi quadraics  3. Gaussian function. 	2
1075	39	1. polynomial functions  2. Gaussian functions 	1
1076	39	- Gaussian functions  - Mutiquadratic  - Inverse multiquadratic	2
1077	39	* Gaussian  * Multiquadratic  * Inverse Multiquadratic  * Polyharmonic Spline	2
1078	39	Gaussian, single layer perceptron	0
1079	39	1. Gaussian function  2. Multi Quadrics  3. Inverse Multiquadrics	2
1080	39	**Answer 14:**    1. Multiquadrics.  2. Inverse multiquadrics.  3. Gaussian functions.	2
1081	39	gaussan function  multiquadrics  inverse multiquadrics    (all covered by michellis theroem)	2
1082	39	multi quadric functions    inverse multi quadric functions    Gaussian functions  	2
1083	39	- Gaussian functions  - Quadric functions  - Multiquadric functions	2
1084	39	1. Exponetial     2. Gaussian     3. Polynomial    	2
1085	39	[--]    1) multivariate functions     4) inverse quadratic functions     3) gaussian fzunctions	2
1086	39	1. Non-localized: $\sqrt{{||x - c_i||}^2 + l^2}, l > 0$    2. Localized: $\frac{1}{\sqrt{{||x - c_i||}^2 + l^2}}$, $exp(- \frac{{||x - c_i||}^2}{2 \sigma^2})$ 	2
1087	47	YOUR ANSWER HERE    In k-fold cross validation, for each experiment k-1 fold of the data is used training and the remaining data is used for testing.    Large number of folds makes the process more computionally complex	1
1088	47	Step 1: The training set is divided into k folds.     Step 2: Among them one fold is left for validation while the others are used to train the network    Step 3: After training each of them are the K-fold dataset are shuffled    Step 4: Step 2 and 3 are repeated until a least error is achieved 	1
1089	47	k- fold cross validation means that we take k data points from the given set and don't use it for training but for testing of the net. We do this several times, each time take other k random points and use them only for testing and not for training.	2
1090	47	During cross validation some part of the training data is taken for validation and this is called as a fold. This is done k times and then the error is averaged.	2
1091	47	Given a dataset. K-fold cross validiation have a sequence of training data. It will be of a k-1 folds. So that all the data included for training. At the end the average test error from the test data(which was basically splitted from the training data) gives the global test error	2
1092	47	+ In k-fold cross validation, we separate the dataset in set of k sequentially separated datasets and we perform cross-validation of these data-sets.  + This is done to prevent the NN from learning the data too well and gain better generalization capabilities.	1
1093	47	It divides data into k groups. Each group is used once as the test set while other as the training set. The training is carried out k times.	1
1094	47	It is a method for model selection. The idea to seperate the input data in k partitions and check the the training error for the model. This gives an upper bound for the testerror.	1
1095	47	In K-fold-cross validation the training set is split into K equal sized parts. While one of the parts is used for testing the others are used for training. This method is used to overcome the lack of training data. 	1
1096	47	We repeat the train and validation procedure on splits of datasets, where we use randomly selected parts for training and validation. The extreme case is leave one out CV, where we validate on one single patter, and is used when data is very sparse.	1
1097	47	- The leraning is splitted into k- folds.  - In each fold, n-k part of the input dataset is used to train the network and the remaining to test it.  - This ensures that we always use the entire dataset for both testing and training at the end of k-folds.	1
1098	47	**Answer 16:**    In k-fold cross validation, the training data is split into k different sets. In each experiment, the network is trained by all sets except one. All experiments leave out a different set. The errors in each experiement is averaged to get the final error.	2
1099	47	The k-fold cross validation is then we split the given data in k -folds and use each fold one time to test the NN. The rest is used to train the NN and we compute the error each time, so we can use the whole set to train the NN to find out how good it is. For each fold the traing has to start from the beginning.	1
1100	47	The data is splitted into k-folds. The network is trained k-times each time a diffrent fold is left out and used for training. after k traings all folds have been used for traing ones.	1
1101	47	K-fold cross validation is a training method where the given dataset is split into training and testing subsets for k folds. For each fold a different training and testing dataset is used and the whole dataset is used to its fullest. In the end the average of the training is used as the real trained machine.	1
1102	47	"""k-fold cross validation"" means that we divide our whole data into ""k"" equal sets and train we run the network for k times and each time we keep one set as out test and train the network with the other sets"	1
1103	47	In k fold cross validation, the test data is separated into k groups and in each training, one is left out to be used as the validation set. The error is the combination of errors obtained in each run.	2
1104	49	Traveling salesman problem of fingin optimal path along all consireded cities in the problem. Perform this without desired signal.	1
1105	49	The traveling salesman problem can be solved using a self organized map. Points on a circle in the center of all cities are the starting weights, then the cities are given as an input to the network so the circle will spread to the cities. This will give a good but not necessarily optimal soultion to the problem.    A line can be fit to a space using a SOM to generate an optimal grid. This can be used to find a short way which covers a field with a certain minimum distance.	2
1106	49	* Travelling sales man problem can be solved using SOMs.  * Randomly initialize weights for neurons.  * Select inputs which are the actual locations of cities in the weight space.  * Take inputs iteratively and find the winning neuron (closest neuron to the current input)  * Find the neighbors of winning neuron.  * Update the weights of winning neuron and it neighbors.  * Repeat the process until thers is no noticable changes in the map space.  * Stop the learning and sort the neurons in a topological order to get the list of cities need to be visited in order.	0
1107	49	Travelling sales man problem can be solved using SOMs. Say the number of cities is n, initially we place n or more than n neurons randomly. Then we sample the data and adjust the winner neuron. This repeats until atleast one neuron is paired with one city. Once we connect all the neurons we get the path for TSP.  	1
1108	49	SOM can be used for solving Travelling SalesMen Problem. As its based on competitive learning if we adjust the initial weights in circular fashion then SOM will try to assign one city to each output neuron. Since transvering cities in circular fashion can lead to shortest distasnce, SOM can give us optimal or very close to optimal solution. 	1
1109	49	The traveling salesman problem  - The problem must be one dimensional.  - Each neuron of the network corresponds to one city.  - If the weight of a neuron is similar or equal to the coordinated of a city, then that neuron is selected to represent the city.	1
1110	49	+ Travelling Salesman Problem as we did in the exercises.  + K-means clustering problems.  + Facial recognition can be learnt by learning the correspondance maps for faces (Philipp Wolfram, C. von der Malsburg, et. al. FIAS, 2008, I think).	2
1111	49	The TSP can be solved appoximatly using som. For each Node introduce a neuron. The neurons are connected 1d. The neurons are initialised in the mean of the points. If we now run tsp and go along the connections between the neurons in the lattice we get an approximate tour	1
1112	49	SOMs will be able to solve mostly pattern recognition patterns by calculating the most similar neurons with these patterns. Multiple patterns can be trained, and the result will be that the different regions of the SOM will be able to identify the multiple patterns.	0
1113	49	Travelling salesman problem can be solved using self organising maps, firstly we need to have more neurons than cities that are to be visited. Then these neurons must be connected to form a circle. After SOM algorithm finishes neurons will represent the path for a salesman to visit.  Given unlabled dataset and passing it through a SOMs algorithm, it will be able to create classes for data and classify it accordingly.	1
1114	49	YOUR ANSWER HERE One sample problem solved using SOM is traveling salesman agent problem.	2
1115	49	**Answer 17:**    SOMs can be used to solve the travelling salesman problem. The neurons in the lattice can be arranged in the form of a circle in the space consisting the cities. During learning, neurons move towards the cities in the ordering phase and fine tune the map in the convergence phase. The final map provides a path close to the shortest path.    SOMs can also be used to cluster words in a document for document analysis. The words would be clustered according to similarity.	2
1116	49	- Traveling salesman problem: number of nodes atleast number of cities to visit, the nodes neighborhood relation is a cirlce-> each node has two neighbours. Apply SOM. A good but typically not optimal solution is represented by the order of the nodes after SOM.  - Fit a one dimensional line to a multidimensional space.	2
1117	49	One example problem that can be solved with SOMs is the traveling salesman problem.  By initializing the SOM with at least as many neurons as cities in an indexed circle and then training it, all of the neurons should cover a city. Now the indices can be used to get a route for the TSP.    Also similarities between objects/animals can be extracted, by using a semantic SOM to order the animals.	2
1118	49	Travelling saleman problem.   Initialize 2D SOM in circular mesh.   Feed 2D locations of city.   After training, order of vising is the order in which neurons are adjusted.       Clustering.    Initialize SOM vectors with same dimensions as data.  feed input data. After trainig clusters in data are same as the clusters of neurons.    	2
1119	49	[--]  Traveling salesmen pronblem by considering every neuron corrwsponding to a city. Using 1 dimensional  arrangement of lattice. Computing the distance and optimising the arrangement    2) They can also be used for identifying patterns in the unput	2
1120	49	The traveling salesman problem can be solved using SOMs. For this, neurons which are connected in a circular way are created. There should be as many neurons as cities in the problem. The cities are fed into the network and the neuron with the weight closest to the city is selected as winner. Then the weights are updated and the neuron moves closer to the city. Here it is only necessery to update the winning neuron, but using a neighbourhood function which also changes neighbours does also work. In the end the neurons form a circle trough all the cities.	1
1121	51	YOUR ANSWER HERE    Max polling: Max polling a filter which gives the maximum value within the filter frame. In CNN it helps in extracting the high level filters and making the learning transformation invarience.    Relu: ReLU is rectified linear unit. In CNN it helps in solving the vaninshing gradient problem.    In case of ReLU $f(x)= max(0, x)$ 	2
1122	51	This function is defined as ReLu:= max(0, v), where v is local field of  neuron	1
1123	51	* Max pooing selects the maximum value in the specified sliding window and forms a new layer of values.  * Rectified linear unit selects the maximum value between 0 and induced field (v).	2
1124	51	Max Pooling: In Max pooling for reducing the dimension fo feature matrix, we select the value of the highest cell to be carried into the new low dimension matrix.   ReLU: Rectifiec linear unit is used to remove the negative terms so that we have some non-linearity.	2
1125	51	Max-pooling is used for extracting only certain information from the feature maps of the CNN. It is one of the factor behind translation invariance observed in CNN. ReLU is an activation function used to introduce non-linearity in the system. It is computationally efficient compared to other activation functions such as sigmoidal function. Its given by max(v,0).	2
1126	51	Max-pooling is a process of coarse-graining the matrix/image by taking the maximum value of an element among a selected sub-matrix/image area (e.g. 2x2 pixel area).  A = $\begin{pmatrix} 2,3\\  4,5 \end{pmatrix} \Rightarrow maxpool(A)= 5$.     Rectified linear Unit (ReLU) is a linear rectifier that responds linearly to the input.	2
1127	51	max pooling is one technique to realise the pooling layer.   relu is an activation function often used for cnn	1
1128	51	Pooling is a method used in CNNs where, for a given convolutional step, a single value will be extracted from the result of a convolution. In the max pooling, the maximal value of the convolution's result will be used. ReLu is a type of activation function used in CNNs. ReLu is a better activation function than the sigmoid, since the ReLu works for value between 0 and 1 (negative values not used in images), and also when using sigmoid functions the resulting values will be very large making it computationally difficult.	2
1129	51	Max pooling refers to a type of layer where our input volume (e.g. RGB images) are spatially reduced in size, reducing both computational need and condensing information through picking the maximum value in a given window. Historically, other pooling methods such as mean pooling have been used, but where later replaced by max pooling or even all convolutional layers with an increased stride to reduce the volumes spatially. As with FFNN, we introduce nonlinearity through our activation function ReLU, being a partwise linear function its less computatinally expensiye in both activation and derivation, and less so has the problem of vanishing gradients in deep CNN.	2
1130	51	[---]    In max-pooling, we take maximum of all values inside the kernal. For example, If kernal size is 2 by 2, we overlay this kernal over the image, we just take maximum value among all 4 values(as 2 by 2 will be over 4 values)    Max-pooling is used for dimensionality reduction as well translation invariance      ReLu is activation function is that is used in CNNs.     It is defined as: max(0,x) where x is input. Whenever, any input has negative value, it results will be 0 else it will return same value.     The reason we are using this because it does not have vanishing gradient problem. Furthermore, computations become easier and faster.        	2
1131	51	- Max pooling refers to a pooling layer with a sliding window where the maximum value in the window is used to represent the windows value.  - ReLU is an activation function used: $\varphi(v) = max(0, v)$. It used because its compulational less complex than sigmoid or tanh but still makes the network behave nonlinear.	2
1132	51	max pooling is subsampling by finding the maximum in each square. The relu (rectivied linear unit) function is the activation function they use in CNN given by $phi(x)= max{0,x}$, which is differantial (threshold function).	2
1133	51	Max-pooling is used to do subsampling in CNNs it takes the maximum of a predefined  number of elements. 	1
1134	51	Max pooling is used in the process of subsampling. Out of windows the maximum value is used in the downscaled variant.    ReLU is an activation function used in CNNs to counteract the vanishing gradient problem. Its derivative does not scale down with lowering values and resembles the step function.	2
1135	51	[--]  Max poling is done using a kernal of finite size passed over the data and computing maximum of data elememts coered by kernel. This is also called downsampling    ReLU are the rectilinear activation function used as activation functions in NN. The overcome the vanishing gradient problem and are computationaally less expensive	2
1136	51	In convolution networks we have a convolution(such as a median or gaussian filter) that we move it over a fixed size window and the result of this oparation will be a window with the same size but new values. In the max-pooling method we choose the maximum value from all the values in this window.	1
