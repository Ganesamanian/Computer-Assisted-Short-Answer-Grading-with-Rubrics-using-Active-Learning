	id	question	solution
0	1	"Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!"	 Artificial neural network is a massively distributed parallel processor which is composed of simple processing units called neurons, which have the natural propensity for storing experiential information and making it available for use. It resembles the human brain in the following aspects.  - Knowledge is acquired by the network from its environment through a learning process.  - Synaptic links are used to store the acquired knowledge.
1	2	Define the mathematical model of a neuron, use the appropriate technical terms!	 The neuronal model consists of the following:  - Synaptic links characterized by their weights which connects the network to the environment it is embedded in.  - An adder function which sums up the weighted inputs and outputs the induced local field of the neuron.  -  An activation function which takes the induced local field of the neuron as it's input and limits the output of the neuron.
2	3	Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!	 Perceptron learning algorithm:  - Initialize the network by assigning random weights to the synaptic links.  - Calculate error as the difference of the desired output with the actual output.  - If the input is misclassified with positive error, $w_(new)$ = $w_(current) + input$.  - If the input is misclassified with negative error, $w_(new)$ = $w_(current) - input$.  - If the input is correctly classified, no changes are made in the weights.  - Repeat from step 2 as long as the error is under some defined threshold value.
3	4	Explain classification and regression; what is the difference?	Classification: Each datapoint is assigned with a class    Regression: Each datapoint is assigned with a value    In classification we assign classes or labels to datapoints. The error signal here can be only true or false. In regression we try to learn a function, the error for each prediction can be a number.
4	5	Write down the SOM learning in pseudo code.	 SOM learning   - Initialization with random small weights.   - Sampling: Picking a input pattern with certain probability.   - Similarity matching: Finding the most matching neuron i.e., the winning neuron.   - Synaptic updation: Updating the weights of the neuron and also the neurons in it's neighbourhood.   - Continuation: Repeat steps 2 to 4 till there is no considerable change in the map.
5	6	Give the basic idea of an SVM using the correct terminology!	The idea behing SVM is to find a hyperplane which separate data into classes. First it is required to find a data point which are clossest to hyperplane, and these data points are called support vectors. Next task is to find a maximum possible width of the hyperplain such that support vectors are on the edge of that hyperplane.    This problem is formulated as min-max constrained optimization problem. In order to find a optimum width of hyperplane, (optimimum of a funtction) the idea is to use method of Langrange multiplier. Additionally, when I data is not linearly separable, than an approach is to project data in higher dimension and then to find a hyperplane that separates data in that dimension.
6	7	What role does the method of steepest decent have when learning a network?	 In steepest descent method, the network moves towards the direction of the maximum gradient. The learning with steepest descent method can be slow to converge and can exhibit zigzag behavior.
7	8	Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$	and there exist a linear saperater which saperates positve examples from the negavtive examples correctly. then we say that A can be shatter at h.
8	9	Write down and explain the Widrow-Hoff learning rule!	Widrow-Hoff rule states that the weight adjustment is proportional to the product of input and the error in the output. It is also called the delta rule.  $$\Delta w_{ji}  = \eta x_ie_{ji}$$  $\eta$ is the proportional constant also called as learning constant  $$W(i)=W(i-1)+\Delta W_{ji}$$
9	10	Explain back propagation, use the correct technical terms!	 Backpropagation is used for Multilayer perceptron network. It consists of two passes.  - Forward pass: The outputs are calculated at every computational node and passed till the output node where the error is calculated by difference of desired output and the actual output. In this pass, the weights of the synaptic links are not changed.  - Backward pass: The error generated at the output neuron is passed in the backward direction i.e., against the direction of the synapses and the local gradient of the error is calculated at every neuron.
10	11	When learning using steepest descent, explain the role of the learning rate? What is a danger?	  - When the learning rate is small, the learning is very slow.  - When the learning rate is large, the learning is unstable and can exhibit zigzag behavior.  - When the learning rate is too large, the learning never converges.
11	12	How does a Reduced Boltzman Machine work (main idea)?	Reduced Boltzman Machine is a biparted (two parts) Reccurent NN, that has two layers visible and hidden layers. In Reduced Boltzman Machine neurons can have two states, namely, + or - 1, depening on current time step. At each time step, the states of neurons are flipped. Here the visible layer represent interface for connection between evironment and hidden layer, and it operates in clamped mode (limited values by environment). WHile hidden layer, operates in free mode.
12	13	Define: Echo State Network (ESN), how are they different to FF NNs?	Echo State Network is a type of neural network which has a recurrent network of 100 to 1000 neurons called dynamic reservior, as the hidden layer. The weights are choosen randomly. The synaptic weights from the resorvoir to the output layers are only adjusted during the learning process.      They are different from the FF NNs in the following regards:  1. ESN have atleat one loop wheras the FF NNs dont.  2. Only the output weights are adjusted in ESN , in FF NNs both the input and output weights are adjusted.  3. ESN s have a memory, FF NNs dont.
13	14	Describe: the structure on an CNN.	    CNN has three components,  * Input  * Convolution stage  * Feed forward network    In CNN the input pass through one or more convolution stage befor it is feed into a feed forward network. The convolution stage uses a hierarchical set of filters, RELU and polling to extract low level as well as high level concepts from the input. The feed forward network along uses the output of the convolution stage and back propagation is used to make adjustment to the network weights as well the filters in the convolution stage.
14	15	What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?	    A RBF network learns,    * The radial function  * weight of the hidden to output neuron  * Centroid of a cluster    Difference:    * A RBF is composed of input layer, 1 hidden layer and the output layer. Other NN can generally use as many hidden layers as required.  * The transformation from input to hidden layer in RBF is non linear and hidden to output is linear. In most other NN both are non linear.    Pros/Cons:  * This is a very simple learner  * There are many variations of RBF available. 
15	16	Describe how learning based on k-nearest neighbors works, use pseudo code!	 Learning based on K-nearest neighbors:  - All the input-output samples from the training set are stored in the memory.  - For a test input, find the k-nearest neighbors.  - Assign the test vector with the class of the most of the neighbours in the neighborhood.
16	17	Explain the Bias Variance Dilemma!	So in machine learning problem, minimizing the two main source of error simultenously does not allow the networks to be generalised very easy.      If bias increase, variance decrease. And vice versa also holds.    1. Bias tells us how close we are to the true value!   2. Variance tells us how they vary for different data set.     So this is a standard problem in NN
17	18	Give the main properties of a SOM!	 Self organizing maps use unsupervised learning by combing the competitive learning principle with the topological structure of the network.    Properties of SOM:  - Competition: The neurons in the SOM compete against each other to be the winning neuron. The neuron with the maximum value for a user defined discriminant function is the winner neuron.  - Correlation: The neighborhood of the winner neuron is determined by the distance of the neighboring neurons from the winning neuron which shows the correlation among them  - Updation: The weights of the winning neuron and the neurons in the neighborhood of the winning neuron are updated
18	19	Enumerate all learning rules, which you know!	 Learning rules:  - Error correction learning  - Memory based learning  - Hebbian learning  - Competitive learning  - Boltzmann learning
19	20	Define sigmoid functions and give at least two examples.	Sigmoiod function is a type of activation function used in ANN which has charecterisctic which balances linear and non-linear models effectively. It can be written as:  $$\phi(v)= \frac{1}{1+exp(-av)}$$    In another form it can also be written as  $$phi(v) = \frac{v}{1+v^2}$$
20	21	Architectures of NNs fall into different classes, which?	 Architectures of Neural networks:  - Single layer feed forward neural networks  - Multilayer feed forward neural networks  - Recurrent neural networks
21	22	What do we understand by weight sharing?	  - Weight sharing means that the synaptic links are assigned the same weight i.e., there is only one weight vector that is shared.  - If there is a change in that weight vector, all the connected synaptic links will be effected.
22	23	Write down and explain Hebb's rule!	 Hebb's rule:  - When the neurons on either side of the synapse are excited synchronously, then the weight of the synapse is increased accordingly.  - When the neurons on either side of the synapse are excited asynchronously, then the weight of the synapse is decreased accordingly.
23	25	What are some principle problems in machine learning? Give explanations!	Overfitting: The network learns noise in the data    Too strong generalization: The network generalizes too strong so that the output does not tell anything.    No convergence: The learning machine may not converge and oscillate.    There may be too few labelled data available, training need lots of data.
24	26	How to remedy over-fitting?	 Overfitting can be avoided or decreased by the following methods:  - Adding prior information or invariances in the design of the network.  - Early stopping of the algorithm when the error is under some threshold value.  - Assigning a momentum factor to the error function which makes the error function descend slowly for variation in time.
25	27	Give the main idea of Cover's theorem and where / how is it used?	 Cover's theorem states that a complex pattern clasification problem cast in high dimension is more likely to be linearly separable than in a low dimension provided the following conditions.  - The transformation is non linear.  - The dimension is high enough.    It is used to classify non linearly separable patterns by casting them to high dimension.
26	28	Explain the concept of Kernel functions used in SVMs!	The kernel functions are used project the input in a higher dimension so that the data non - linearly separable data becomes linearly seperable.    few kernel functions are:  1. polynomial term  2. Radial basis function   3. gaussian function
27	29	What does the Perceptron Learning Theorem tell us?	 Perceptron learning theorem states that the learning converges if the data is linearly separable.
28	30	What is three-way data split and how to use if for NN training?	  - Three-way data split is the process of dividing the available dataset into three parts.  - The training dataset, validation dataset and the testing dataset.  - After the network is presented with the training dataset, validation dataset is used find the most appropriate method i.e., which produces the least error.  - Then the test dataset is used to find the generalization performance of the network.
29	31	Explain the behavior of train and test error, if we add more and more training samples	    If we add more more training samples then the when we train the network, it there is will less trainning error it will try to fit for the trainging data.     may end up memorising the  training data.     and  test data is from the traing set then there will very less or no error. if it is not from the training set then there might be relativeliy high error in test data.
30	32	Define and explain the term local gradient in back-propagation!	    The local gradient in back propagation is the local component of the error function that is to be calculated for every neuron in the direction against the synaptic flow.  - If a neuron is a output node, the local gradient is equal to the product of the derivative of the activation function and the error signal.  - If a neuron if a hidden node, the local gradient is equal to the product of the derivative of the activation function and the sum of the products of the local gradient and the ouput of the neuron in the next layer.
31	34	What is Newtons method in general? How can it be utilized for NNs?	The Newtons method in general, is a method used for optmizing certain non-linear funtion. This method uses a second order approximation of the cost function, based on second order Tailor expansion, for perdorming optimization procedure. In contrast to steepest descent method where the cost function is approximited with first order function-line, in this method cost function is approximated with quadratic function. Newton method does not exibit osicilatory path while leaning process, but it requires additionaly second derivative(gradient) of a cost function, and also this graddient must be always positive definite. THis method is hard to implement, because we do not have always second gradient available, and if we have, it is not guaratiened that this gradient will always stay positive definite.   Newton method can be used for learning in  NN , such that it can be applied for searching(finding) the minimum of a error funtion. The error funtion is defined as difference between desired signal and output signal from NN. The Newton method searchs for min of errror function, by changeing values of synaptiic weigths, and by then performing learning process of NN.
32	35	"What do we understand by ""momentum"" in context of learning?"	      $W_{n+1} = \alpha W_n + \eta e_n x_n$    In a learning machine $\alpha$ is the momentum, It    * Reduces the zigzag effect while making adjustment of the weights  * Speed up the convergence in a steady down hill
33	36	"What is ""normalization"" of inputs?"	Normalization of the input takes place in following steps:  1. Mean removal  2. Uncorrelation of data points.  3. Decrease the covariance.  PCA is a useful tool for data normalization or whitening of inputs
34	37	When designing a feed-forward NN how to determine the correct number of layers?	  - We can start with large number of layers and decrease them until the performance degrades.  - We can start with small number of layers and increase them until the performance is acceptable.
35	38	How many training examples are needed for training an NN?	  - The number of training examples needed for training an NN should be atleast 5 to 10 times the number of weights.  - The lower bound for the number of training examples, m, can be calculated when the VC dimesion is given as h and the error propabability is 1- $\sigma$    $m \leq \sqrt\frac{(4h\log(2/\sigma)) - 8 log(13 \sigma)}{h}$
36	39	Give some examples of families of RBF functions!	 Families of RBF functions:  - Multiquadric functions  - Inverse multiquadric functions  - Gaussian functions
37	47	"Explain the term ""k-fold cross validation"""	    K fold cross validation    its basically helps computing performance .    you divide the data set into traning and test data. into k folds .     then clacluate the error for each fold and find out average all errors. 
38	49	Give samples for problems with can be solved using SOMs and explain how.	    Travelling sales person problem, grouping a set of given RGB colors into groups such problems can be solved with SOM because,    * SOM gives topological structure of the neurons where adjacent neurons tends to have similar value.   * The network maps an input to a neuron closest to the input.  * The neuron is the centroid of the points in a cluster    So in case of travelling sales person problem the network will give structure of the locations to travel and the adjacent locations will be presented by adjacent neurons.    In case of grouping colors the network will group similar colors together and adjacent neurons will have similar values.
39	51	Define max-pooling and the term ReLU in context of CNNs.	Max pooling in CNN is further adjustment of the convoluted network to make it tarnslation invariant. That is, for small translation in input the output remains unaffected.     Rectified Linear Unit is a type of activation function used in CNN. It can be written as  $$\phi(x)= max(0,x)$$  It is a type of picewise activation function and removes the danger of vanishing gradient problem which is a charecteristic of sigmoid function
